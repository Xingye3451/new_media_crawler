# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚


import asyncio
import json
import re
from typing import Any, Callable, Dict, List, Optional, Union
from urllib.parse import urlencode

import httpx
from playwright.async_api import BrowserContext, Page
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_result, wait_exponential

import config
from base.base_crawler import AbstractApiClient
from tools import utils
from html import unescape

from .exception import DataFetchError, IPBlockError
from .field import SearchNoteType, SearchSortType
from .help import get_search_id, sign


class XiaoHongShuClient(AbstractApiClient):
    def __init__(
        self,
        timeout=10,
        proxies=None,
        *,
        headers: Dict[str, str],
        playwright_page: Page,
        cookie_dict: Dict[str, str],
    ):
        self.proxies = proxies
        self.timeout = timeout
        self.headers = headers
        self._host = "https://edith.xiaohongshu.com"
        self._domain = "https://www.xiaohongshu.com"
        self.IP_ERROR_STR = "ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®æˆ–é‡å¯è¯•è¯•"
        self.IP_ERROR_CODE = 300012
        self.NOTE_ABNORMAL_STR = "ç¬”è®°çŠ¶æ€å¼‚å¸¸ï¼Œè¯·ç¨åæŸ¥çœ‹"
        self.NOTE_ABNORMAL_CODE = -510001
        self.playwright_page = playwright_page
        self.cookie_dict = cookie_dict

    async def _pre_headers(self, url: str, data=None) -> Dict:
        """
        è¯·æ±‚å¤´å‚æ•°ç­¾å
        Args:
            url:
            data:

        Returns:

        """
        encrypt_params = await self.playwright_page.evaluate(
            "([url, data]) => window._webmsxyw(url,data)", [url, data]
        )
        local_storage = await self.playwright_page.evaluate("() => window.localStorage")
        signs = sign(
            a1=self.cookie_dict.get("a1", ""),
            b1=local_storage.get("b1", ""),
            x_s=encrypt_params.get("X-s", ""),
            x_t=str(encrypt_params.get("X-t", "")),
        )

        headers = {
            "X-S": signs["x-s"],
            "X-T": signs["x-t"],
            "x-S-Common": signs["x-s-common"],
            "X-B3-Traceid": signs["x-b3-traceid"],
        }
        self.headers.update(headers)
        return self.headers

    @retry(stop=stop_after_attempt(5), wait=wait_exponential(multiplier=1, min=2, max=10))
    async def request(self, method, url, **kwargs) -> Union[str, Any]:
        """
        å°è£…httpxçš„å…¬å…±è¯·æ±‚æ–¹æ³•ï¼Œå¯¹è¯·æ±‚å“åº”åšä¸€äº›å¤„ç†
        Args:
            method: è¯·æ±‚æ–¹æ³•
            url: è¯·æ±‚çš„URL
            **kwargs: å…¶ä»–è¯·æ±‚å‚æ•°ï¼Œä¾‹å¦‚è¯·æ±‚å¤´ã€è¯·æ±‚ä½“ç­‰

        Returns:

        """
        # return response.text
        return_response = kwargs.pop("return_response", False)

        async with httpx.AsyncClient(proxies=self.proxies) as client:
            response = await client.request(method, url, timeout=self.timeout, **kwargs)

        if response.status_code == 471 or response.status_code == 461:
            # someday someone maybe will bypass captcha
            verify_type = response.headers.get("Verifytype", "unknown")
            verify_uuid = response.headers.get("Verifyuuid", "unknown")
            utils.logger.error(f"[XiaoHongShuClient.request] å‡ºç°éªŒè¯ç ï¼Œè¯·æ±‚å¤±è´¥ï¼ŒVerifytype: {verify_type}ï¼ŒVerifyuuid: {verify_uuid}")
            raise Exception(
                f"å‡ºç°éªŒè¯ç ï¼Œè¯·æ±‚å¤±è´¥ï¼ŒVerifytype: {verify_type}ï¼ŒVerifyuuid: {verify_uuid}, Response: {response}"
            )

        if return_response:
            return response.text
        
        try:
            data: Dict = response.json()
        except Exception as e:
            utils.logger.error(f"[XiaoHongShuClient.request] JSONè§£æå¤±è´¥: {e}, Response: {response.text}")
            raise DataFetchError(f"å“åº”æ•°æ®æ ¼å¼é”™è¯¯: {e}")
        
        if data.get("success"):
            return data.get("data", data.get("success", {}))
        elif data.get("code") == self.IP_ERROR_CODE:
            utils.logger.error(f"[XiaoHongShuClient.request] IPè¢«é™åˆ¶: {data}")
            raise IPBlockError(self.IP_ERROR_STR)
        elif data.get("code") == -510000 and data.get("msg") == "ç¬”è®°ä¸å­˜åœ¨":
            # ğŸ†• ä¿®å¤ï¼šç¬”è®°ä¸å­˜åœ¨æ˜¯æ­£å¸¸ç°è±¡ï¼Œä¸æ˜¯é”™è¯¯
            utils.logger.debug(f"[XiaoHongShuClient.request] ç¬”è®°ä¸å­˜åœ¨ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡: {data}")
            return {}  # è¿”å›ç©ºå­—å…¸ï¼Œè®©è°ƒç”¨æ–¹å¤„ç†
        elif data.get("code") == -510001 and data.get("msg") == "ç¬”è®°çŠ¶æ€å¼‚å¸¸ï¼Œè¯·ç¨åæŸ¥çœ‹":
            # ğŸ†• ä¿®å¤ï¼šç¬”è®°çŠ¶æ€å¼‚å¸¸ä¹Ÿæ˜¯æ­£å¸¸ç°è±¡
            utils.logger.debug(f"[XiaoHongShuClient.request] ç¬”è®°çŠ¶æ€å¼‚å¸¸ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡: {data}")
            return {}  # è¿”å›ç©ºå­—å…¸ï¼Œè®©è°ƒç”¨æ–¹å¤„ç†
        else:
            error_msg = data.get("msg", f"æœªçŸ¥é”™è¯¯ï¼ŒçŠ¶æ€ç : {response.status_code}")
            utils.logger.error(f"[XiaoHongShuClient.request] è¯·æ±‚å¤±è´¥: {error_msg}, å®Œæ•´å“åº”: {data}")
            raise DataFetchError(error_msg)

    async def get(self, uri: str, params=None) -> Dict:
        """
        GETè¯·æ±‚ï¼Œå¯¹è¯·æ±‚å¤´ç­¾å
        Args:
            uri: è¯·æ±‚è·¯ç”±
            params: è¯·æ±‚å‚æ•°

        Returns:

        """
        final_uri = uri
        if isinstance(params, dict):
            final_uri = f"{uri}?" f"{urlencode(params)}"
        headers = await self._pre_headers(final_uri)
        return await self.request(
            method="GET", url=f"{self._host}{final_uri}", headers=headers
        )

    async def post(self, uri: str, data: dict, **kwargs) -> Dict:
        """
        POSTè¯·æ±‚ï¼Œå¯¹è¯·æ±‚å¤´ç­¾å
        Args:
            uri: è¯·æ±‚è·¯ç”±
            data: è¯·æ±‚ä½“å‚æ•°

        Returns:

        """
        headers = await self._pre_headers(uri, data)
        json_str = json.dumps(data, separators=(",", ":"), ensure_ascii=False)
        return await self.request(
            method="POST",
            url=f"{self._host}{uri}",
            data=json_str,
            headers=headers,
            **kwargs,
        )

    async def get_note_media(self, url: str) -> Union[bytes, None]:
        async with httpx.AsyncClient(proxies=self.proxies) as client:
            response = await client.request("GET", url, timeout=self.timeout)
            if not response.reason_phrase == "OK":
                utils.logger.error(
                    f"[XiaoHongShuClient.get_note_media] request {url} err, res:{response.text}"
                )
                return None
            else:
                return response.content

    async def pong(self) -> bool:
        """
        ç”¨äºæ£€æŸ¥ç™»å½•æ€æ˜¯å¦å¤±æ•ˆäº†
        Returns:

        """
        """get a note to check if login state is ok"""
        utils.logger.info("[XiaoHongShuClient.pong] Begin to pong xhs...")
        ping_flag = False
        
        # æ£€æŸ¥cookiesçŠ¶æ€
        cookie_count = len(self.cookie_dict) if self.cookie_dict else 0
        utils.logger.info(f"[XiaoHongShuClient.pong] Current cookies count: {cookie_count}")
        if cookie_count > 0:
            utils.logger.info(f"[XiaoHongShuClient.pong] Cookie keys: {list(self.cookie_dict.keys())}")
        
        try:
            note_card: Dict = await self.get_note_by_keyword(keyword="å°çº¢ä¹¦")
            if note_card.get("items"):
                ping_flag = True
                utils.logger.info("[XiaoHongShuClient.pong] Ping xhs success")
            else:
                utils.logger.warning("[XiaoHongShuClient.pong] Ping xhs failed: no items returned")
                utils.logger.debug(f"[XiaoHongShuClient.pong] Response: {note_card}")
        except DataFetchError as e:
            utils.logger.error(
                f"[XiaoHongShuClient.pong] Ping xhs failed with DataFetchError: {e}, and try to login again..."
            )
            ping_flag = False
        except IPBlockError as e:
            utils.logger.error(
                f"[XiaoHongShuClient.pong] Ping xhs failed with IPBlockError: {e}, IP may be blocked..."
            )
            ping_flag = False
        except Exception as e:
            utils.logger.error(
                f"[XiaoHongShuClient.pong] Ping xhs failed with unexpected error: {e}, and try to login again..."
            )
            ping_flag = False
        
        # å¦‚æœpingå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯
        if not ping_flag:
            utils.logger.info("[XiaoHongShuClient.pong] å°è¯•ä½¿ç”¨å¤‡ç”¨å…³é”®è¯è¿›è¡Œpingæµ‹è¯•...")
            try:
                note_card: Dict = await self.get_note_by_keyword(keyword="ç¾é£Ÿ")
                if note_card.get("items"):
                    ping_flag = True
                    utils.logger.info("[XiaoHongShuClient.pong] å¤‡ç”¨å…³é”®è¯pingæˆåŠŸ")
                else:
                    utils.logger.warning("[XiaoHongShuClient.pong] å¤‡ç”¨å…³é”®è¯pingä¹Ÿå¤±è´¥")
            except Exception as e:
                utils.logger.error(f"[XiaoHongShuClient.pong] å¤‡ç”¨å…³é”®è¯pingå¤±è´¥: {e}")
        
        return ping_flag

    async def update_cookies(self, browser_context: BrowserContext):
        """
        APIå®¢æˆ·ç«¯æä¾›çš„æ›´æ–°cookiesæ–¹æ³•ï¼Œä¸€èˆ¬æƒ…å†µä¸‹ç™»å½•æˆåŠŸåä¼šè°ƒç”¨æ­¤æ–¹æ³•
        Args:
            browser_context: æµè§ˆå™¨ä¸Šä¸‹æ–‡å¯¹è±¡

        Returns:

        """
        cookie_str, cookie_dict = utils.convert_cookies(await browser_context.cookies())
        self.headers["Cookie"] = cookie_str
        self.cookie_dict = cookie_dict

    async def set_cookies_from_string(self, cookie_str: str):
        """ä»å­—ç¬¦ä¸²è®¾ç½®cookies"""
        try:
            from tools import utils as crawler_utils
            cookie_dict = crawler_utils.convert_str_cookie_to_dict(cookie_str)
            
            # è®¾ç½®cookiesåˆ°æµè§ˆå™¨ä¸Šä¸‹æ–‡
            for key, value in cookie_dict.items():
                await self.playwright_page.context.add_cookies([{
                    'name': key,
                    'value': value,
                    'domain': '.xiaohongshu.com',
                    'path': '/'
                }])
            
            # æ›´æ–°å®¢æˆ·ç«¯cookies
            self.headers["Cookie"] = cookie_str
            self.cookie_dict = cookie_dict
            
            utils.logger.info(f"[XiaoHongShuClient] å·²è®¾ç½® {len(cookie_dict)} ä¸ªcookies")
            
        except Exception as e:
            utils.logger.error(f"[XiaoHongShuClient] è®¾ç½®cookieså¤±è´¥: {e}")
            raise

    async def clear_cookies(self):
        """æ¸…é™¤cookies"""
        try:
            # æ¸…é™¤æµè§ˆå™¨ä¸Šä¸‹æ–‡ä¸­çš„cookies
            await self.playwright_page.context.clear_cookies()
            
            # æ¸…é™¤å®¢æˆ·ç«¯cookies
            self.headers["Cookie"] = ""
            self.cookie_dict = {}
            
            utils.logger.info("[XiaoHongShuClient] å·²æ¸…é™¤æ‰€æœ‰cookies")
            
        except Exception as e:
            utils.logger.error(f"[XiaoHongShuClient] æ¸…é™¤cookieså¤±è´¥: {e}")
            raise

    async def get_note_by_keyword(
        self,
        keyword: str,
        search_id: str = get_search_id(),
        page: int = 1,
        page_size: int = 20,
        sort: SearchSortType = SearchSortType.GENERAL,
        note_type: SearchNoteType = SearchNoteType.ALL,
    ) -> Dict:
        """
        æ ¹æ®å…³é”®è¯æœç´¢ç¬”è®°
        Args:
            keyword: å…³é”®è¯å‚æ•°
            page: åˆ†é¡µç¬¬å‡ é¡µ
            page_size: åˆ†é¡µæ•°æ®é•¿åº¦
            sort: æœç´¢ç»“æœæ’åºæŒ‡å®š
            note_type: æœç´¢çš„ç¬”è®°ç±»å‹

        Returns:

        """
        uri = "/api/sns/web/v1/search/notes"
        data = {
            "keyword": keyword,
            "page": page,
            "page_size": page_size,
            "search_id": search_id,
            "sort": sort.value,
            "note_type": note_type.value,
        }
        return await self.post(uri, data)

    async def get_note_by_id(
        self, note_id: str, xsec_source: str, xsec_token: str
    ) -> Dict:
        """
        è·å–ç¬”è®°è¯¦æƒ…API
        Args:
            note_id:ç¬”è®°ID
            xsec_source: æ¸ é“æ¥æº
            xsec_token: æœç´¢å…³é”®å­—ä¹‹åè¿”å›çš„æ¯”è¾ƒåˆ—è¡¨ä¸­è¿”å›çš„token

        Returns:

        """
        if xsec_source == "":
            xsec_source = "pc_search"

        data = {
            "source_note_id": note_id,
            "image_formats": ["jpg", "webp", "avif"],
            "extra": {"need_body_topic": 1},
            "xsec_source": xsec_source,
            "xsec_token": xsec_token,
        }
        uri = "/api/sns/web/v1/feed"
        res = await self.post(uri, data)
        if res and res.get("items"):
            res_dict: Dict = res["items"][0]["note_card"]
            return res_dict
        # ğŸ†• ä¿®å¤ï¼šå°†é”™è¯¯æ—¥å¿—æ”¹ä¸ºè­¦å‘Šçº§åˆ«ï¼Œå› ä¸ºè¿™æ˜¯æ­£å¸¸ç°è±¡
        # çˆ¬å–é¢‘ç¹äº†å¯èƒ½ä¼šå‡ºç°æœ‰çš„ç¬”è®°èƒ½æœ‰ç»“æœæœ‰çš„æ²¡æœ‰ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡
        utils.logger.warning(
            f"[XiaoHongShuClient.get_note_by_id] ç¬”è®°è¯¦æƒ…è·å–å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬ä¿¡æ¯: note_id={note_id}"
        )
        return dict()

    async def get_note_comments(
        self, note_id: str, xsec_token: str, cursor: str = ""
    ) -> Dict:
        """
        è·å–ä¸€çº§è¯„è®ºçš„API
        Args:
            note_id: ç¬”è®°ID
            xsec_token: éªŒè¯token
            cursor: åˆ†é¡µæ¸¸æ ‡

        Returns:

        """
        uri = "/api/sns/web/v2/comment/page"
        params = {
            "note_id": note_id,
            "cursor": cursor,
            "top_comment_id": "",
            "image_formats": "jpg,webp,avif",
            "xsec_token": xsec_token,
        }
        return await self.get(uri, params)

    async def get_note_sub_comments(
        self,
        note_id: str,
        root_comment_id: str,
        xsec_token: str,
        num: int = 10,
        cursor: str = "",
    ):
        """
        è·å–æŒ‡å®šçˆ¶è¯„è®ºä¸‹çš„å­è¯„è®ºçš„API
        Args:
            note_id: å­è¯„è®ºçš„å¸–å­ID
            root_comment_id: æ ¹è¯„è®ºID
            xsec_token: éªŒè¯token
            num: åˆ†é¡µæ•°é‡
            cursor: åˆ†é¡µæ¸¸æ ‡

        Returns:

        """
        uri = "/api/sns/web/v2/comment/sub/page"
        params = {
            "note_id": note_id,
            "root_comment_id": root_comment_id,
            "num": num,
            "cursor": cursor,
            "image_formats": "jpg,webp,avif",
            "top_comment_id": "",
            "xsec_token": xsec_token,
        }
        return await self.get(uri, params)

    async def get_note_all_comments(
        self,
        note_id: str,
        xsec_token: str,
        crawl_interval: float = 1.0,
        callback: Optional[Callable] = None,
        max_count: int = 10,
    ) -> List[Dict]:
        """
        è·å–æŒ‡å®šç¬”è®°ä¸‹çš„æ‰€æœ‰ä¸€çº§è¯„è®ºï¼Œè¯¥æ–¹æ³•ä¼šä¸€ç›´æŸ¥æ‰¾ä¸€ä¸ªå¸–å­ä¸‹çš„æ‰€æœ‰è¯„è®ºä¿¡æ¯
        Args:
            note_id: ç¬”è®°ID
            xsec_token: éªŒè¯token
            crawl_interval: çˆ¬å–ä¸€æ¬¡ç¬”è®°çš„å»¶è¿Ÿå•ä½ï¼ˆç§’ï¼‰
            callback: ä¸€æ¬¡ç¬”è®°çˆ¬å–ç»“æŸå
            max_count: ä¸€æ¬¡ç¬”è®°çˆ¬å–çš„æœ€å¤§è¯„è®ºæ•°é‡
        Returns:

        """
        result = []
        comments_has_more = True
        comments_cursor = ""
        while comments_has_more and len(result) < max_count:
            comments_res = await self.get_note_comments(
                note_id=note_id, xsec_token=xsec_token, cursor=comments_cursor
            )
            comments_has_more = comments_res.get("has_more", False)
            comments_cursor = comments_res.get("cursor", "")
            if "comments" not in comments_res:
                utils.logger.info(
                    f"[XiaoHongShuClient.get_note_all_comments] No 'comments' key found in response: {comments_res}"
                )
                break
            comments = comments_res["comments"]
            if len(result) + len(comments) > max_count:
                comments = comments[: max_count - len(result)]
            if callback:
                await callback(note_id, comments)
            await asyncio.sleep(crawl_interval)
            result.extend(comments)
            sub_comments = await self.get_comments_all_sub_comments(
                comments=comments,
                xsec_token=xsec_token,
                crawl_interval=crawl_interval,
                callback=callback,
            )
            result.extend(sub_comments)
        return result

    async def get_comments_all_sub_comments(
        self,
        comments: List[Dict],
        xsec_token: str,
        crawl_interval: float = 1.0,
        callback: Optional[Callable] = None,
    ) -> List[Dict]:
        """
        è·å–æŒ‡å®šä¸€çº§è¯„è®ºä¸‹çš„æ‰€æœ‰äºŒçº§è¯„è®º, è¯¥æ–¹æ³•ä¼šä¸€ç›´æŸ¥æ‰¾ä¸€çº§è¯„è®ºä¸‹çš„æ‰€æœ‰äºŒçº§è¯„è®ºä¿¡æ¯
        Args:
            comments: è¯„è®ºåˆ—è¡¨
            xsec_token: éªŒè¯token
            crawl_interval: çˆ¬å–ä¸€æ¬¡è¯„è®ºçš„å»¶è¿Ÿå•ä½ï¼ˆç§’ï¼‰
            callback: ä¸€æ¬¡è¯„è®ºçˆ¬å–ç»“æŸå

        Returns:

        """
        if not config.ENABLE_GET_SUB_COMMENTS:
            utils.logger.info(
                f"[XiaoHongShuCrawler.get_comments_all_sub_comments] Crawling sub_comment mode is not enabled"
            )
            return []

        result = []
        for comment in comments:
            note_id = comment.get("note_id")
            sub_comments = comment.get("sub_comments")
            if sub_comments and callback:
                await callback(note_id, sub_comments)

            sub_comment_has_more = comment.get("sub_comment_has_more")
            if not sub_comment_has_more:
                continue

            root_comment_id = comment.get("id")
            sub_comment_cursor = comment.get("sub_comment_cursor")

            while sub_comment_has_more:
                comments_res = await self.get_note_sub_comments(
                    note_id=note_id,
                    root_comment_id=root_comment_id,
                    xsec_token=xsec_token,
                    num=10,
                    cursor=sub_comment_cursor,
                )
                
                if comments_res is None:
                    utils.logger.info(
                        f"[XiaoHongShuClient.get_comments_all_sub_comments] No response found for note_id: {note_id}"
                    )
                    continue
                sub_comment_has_more = comments_res.get("has_more", False)
                sub_comment_cursor = comments_res.get("cursor", "")
                if "comments" not in comments_res:
                    utils.logger.info(
                        f"[XiaoHongShuClient.get_comments_all_sub_comments] No 'comments' key found in response: {comments_res}"
                    )
                    break
                comments = comments_res["comments"]
                if callback:
                    await callback(note_id, comments)
                await asyncio.sleep(crawl_interval)
                result.extend(comments)
        return result

    async def get_creator_info(self, user_id: str) -> Dict:
        """
        é€šè¿‡è§£æç½‘é¡µç‰ˆçš„ç”¨æˆ·ä¸»é¡µHTMLï¼Œè·å–ç”¨æˆ·ä¸ªäººç®€è¦ä¿¡æ¯
        PCç«¯ç”¨æˆ·ä¸»é¡µçš„ç½‘é¡µå­˜åœ¨window.__INITIAL_STATE__è¿™ä¸ªå˜é‡ä¸Šçš„ï¼Œè§£æå®ƒå³å¯
        eg: https://www.xiaohongshu.com/user/profile/59d8cb33de5fb4696bf17217
        """
        uri = f"/user/profile/{user_id}"
        html_content = await self.request(
            "GET", self._domain + uri, return_response=True, headers=self.headers
        )
        match = re.search(
            r"<script>window.__INITIAL_STATE__=(.+)<\/script>", html_content, re.M
        )

        if match is None:
            return {}

        info = json.loads(match.group(1).replace(":undefined", ":null"), strict=False)
        if info is None:
            return {}
        return info.get("user").get("userPageData")

    async def get_notes_by_creator(
        self, creator: str, cursor: str, page_size: int = 30
    ) -> Dict:
        """
        è·å–åšä¸»çš„ç¬”è®°
        Args:
            creator: åšä¸»ID
            cursor: ä¸Šä¸€é¡µæœ€åä¸€æ¡ç¬”è®°çš„ID
            page_size: åˆ†é¡µæ•°æ®é•¿åº¦

        Returns:

        """
        uri = "/api/sns/web/v1/user_posted"
        data = {
            "user_id": creator,
            "cursor": cursor,
            "num": page_size,
            "image_formats": "jpg,webp,avif",
        }
        return await self.get(uri, data)

    async def get_all_notes_by_creator(
        self,
        user_id: str,
        crawl_interval: float = 1.0,
        callback: Optional[Callable] = None,
    ) -> List[Dict]:
        """
        è·å–æŒ‡å®šç”¨æˆ·ä¸‹çš„æ‰€æœ‰å‘è¿‡çš„å¸–å­ï¼Œè¯¥æ–¹æ³•ä¼šä¸€ç›´æŸ¥æ‰¾ä¸€ä¸ªç”¨æˆ·ä¸‹çš„æ‰€æœ‰å¸–å­ä¿¡æ¯
        Args:
            user_id: ç”¨æˆ·ID
            crawl_interval: çˆ¬å–ä¸€æ¬¡çš„å»¶è¿Ÿå•ä½ï¼ˆç§’ï¼‰
            callback: ä¸€æ¬¡åˆ†é¡µçˆ¬å–ç»“æŸåçš„æ›´æ–°å›è°ƒå‡½æ•°

        Returns:

        """
        result = []
        notes_has_more = True
        notes_cursor = ""
        while notes_has_more and len(result) < config.CRAWLER_MAX_NOTES_COUNT:
            notes_res = await self.get_notes_by_creator(user_id, notes_cursor)
            if not notes_res:
                utils.logger.error(
                    f"[XiaoHongShuClient.get_notes_by_creator] The current creator may have been banned by xhs, so they cannot access the data."
                )
                break

            notes_has_more = notes_res.get("has_more", False)
            notes_cursor = notes_res.get("cursor", "")
            if "notes" not in notes_res:
                utils.logger.info(
                    f"[XiaoHongShuClient.get_all_notes_by_creator] No 'notes' key found in response: {notes_res}"
                )
                break

            notes = notes_res["notes"]
            utils.logger.info(
                f"[XiaoHongShuClient.get_all_notes_by_creator] got user_id:{user_id} notes len : {len(notes)}"
            )

            remaining = config.CRAWLER_MAX_NOTES_COUNT - len(result)
            if remaining <= 0:
                break

            notes_to_add = notes[:remaining]
            if callback:
                await callback(notes_to_add)

            result.extend(notes_to_add)
            await asyncio.sleep(crawl_interval)

        utils.logger.info(
            f"[XiaoHongShuClient.get_all_notes_by_creator] Finished getting notes for user {user_id}, total: {len(result)}"
        )
        return result

    async def get_note_short_url(self, note_id: str) -> Dict:
        """
        è·å–ç¬”è®°çš„çŸ­é“¾æ¥
        Args:
            note_id: ç¬”è®°ID

        Returns:

        """
        uri = f"/api/sns/web/short_url"
        data = {"original_url": f"{self._domain}/discovery/item/{note_id}"}
        return await self.post(uri, data=data, return_response=True)

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(1))
    async def get_note_by_id_from_html(
        self,
        note_id: str,
        xsec_source: str,
        xsec_token: str,
        enable_cookie: bool = False,
    ) -> Optional[Dict]:
        """
        é€šè¿‡è§£æç½‘é¡µç‰ˆçš„ç¬”è®°è¯¦æƒ…é¡µHTMLï¼Œè·å–ç¬”è®°è¯¦æƒ…, è¯¥æ¥å£å¯èƒ½ä¼šå‡ºç°å¤±è´¥çš„æƒ…å†µï¼Œè¿™é‡Œå°è¯•é‡è¯•3æ¬¡
        copy from https://github.com/ReaJason/xhs/blob/eb1c5a0213f6fbb592f0a2897ee552847c69ea2d/xhs/core.py#L217-L259
        thanks for ReaJason
        Args:
            note_id:
            xsec_source:
            xsec_token:
            enable_cookie:

        Returns:

        """

        def camel_to_underscore(key):
            return re.sub(r"(?<!^)(?=[A-Z])", "_", key).lower()

        def transform_json_keys(json_data):
            data_dict = json.loads(json_data)
            dict_new = {}
            for key, value in data_dict.items():
                new_key = camel_to_underscore(key)
                if not value:
                    dict_new[new_key] = value
                elif isinstance(value, dict):
                    dict_new[new_key] = transform_json_keys(json.dumps(value))
                elif isinstance(value, list):
                    dict_new[new_key] = [
                        (
                            transform_json_keys(json.dumps(item))
                            if (item and isinstance(item, dict))
                            else item
                        )
                        for item in value
                    ]
                else:
                    dict_new[new_key] = value
            return dict_new

        url = (
            "https://www.xiaohongshu.com/explore/"
            + note_id
            + f"?xsec_token={xsec_token}&xsec_source={xsec_source}"
        )
        copy_headers = self.headers.copy()
        if not enable_cookie:
            del copy_headers["Cookie"]

        html = await self.request(
            method="GET", url=url, return_response=True, headers=copy_headers
        )

        def get_note_dict(html):
            state = re.findall(r"window.__INITIAL_STATE__=({.*})</script>", html)[
                0
            ].replace("undefined", '""')

            if state != "{}":
                note_dict = transform_json_keys(state)
                return note_dict["note"]["note_detail_map"][note_id]["note"]
            return {}

        try:
            return get_note_dict(html)
        except:
            return None

    async def search_user_notes(self, user_id: str, keywords: str, max_count: int = 50) -> List[Dict]:
        """
        æœç´¢æŒ‡å®šç”¨æˆ·çš„ç¬”è®°
        Args:
            user_id: ç”¨æˆ·ID
            keywords: æœç´¢å…³é”®è¯
            max_count: æœ€å¤§è·å–æ•°é‡
        Returns:
            List[Dict]: ç¬”è®°åˆ—è¡¨
        """
        try:
            utils.logger.info(f"[XiaoHongShuClient.search_user_notes] å¼€å§‹æœç´¢ç”¨æˆ· {user_id} çš„å…³é”®è¯ '{keywords}' ç¬”è®°")
            
            # ğŸ†• ä½¿ç”¨å°çº¢ä¹¦çš„åŸç”Ÿæœç´¢APIï¼Œç„¶åè¿‡æ»¤å‡ºæŒ‡å®šç”¨æˆ·çš„ç¬”è®°
            utils.logger.info(f"[XiaoHongShuClient.search_user_notes] ä½¿ç”¨åŸç”Ÿæœç´¢APIæœç´¢å…³é”®è¯: {keywords}")
            
            all_matching_notes = []
            page = 1
            max_search_pages = 10  # é™åˆ¶æœç´¢é¡µæ•°ï¼Œé¿å…è¿‡åº¦è¯·æ±‚
            
            while page <= max_search_pages and len(all_matching_notes) < max_count:
                utils.logger.info(f"[XiaoHongShuClient.search_user_notes] æœç´¢ç¬¬ {page} é¡µ")
                
                try:
                    # ä½¿ç”¨å…¨å±€æœç´¢API
                    search_result = await self.get_note_by_keyword(
                        keyword=keywords,
                        page=page,
                        page_size=20,
                        note_type=SearchNoteType.VIDEO  # é»˜è®¤æœç´¢è§†é¢‘å†…å®¹
                    )
                    
                    utils.logger.debug(f"[XiaoHongShuClient.search_user_notes] ç¬¬ {page} é¡µæœç´¢APIå“åº”: {search_result}")
                    
                    if not search_result or not search_result.get("items"):
                        utils.logger.info(f"[XiaoHongShuClient.search_user_notes] ç¬¬ {page} é¡µæ²¡æœ‰æ›´å¤šç»“æœ")
                        break
                    
                    items = search_result.get("items", [])
                    
                    # è¿‡æ»¤å‡ºæŒ‡å®šç”¨æˆ·çš„ç¬”è®°
                    for note in items:
                        try:
                            note_user_id = note.get("user", {}).get("user_id")
                            if note_user_id == user_id:
                                all_matching_notes.append(note)
                                utils.logger.info(f"[XiaoHongShuClient.search_user_notes] æ‰¾åˆ°åŒ¹é…ç”¨æˆ· {user_id} çš„ç¬”è®°")
                                
                                if len(all_matching_notes) >= max_count:
                                    utils.logger.info(f"[XiaoHongShuClient.search_user_notes] å·²è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶ {max_count}")
                                    break
                        except Exception as e:
                            utils.logger.warning(f"[XiaoHongShuClient.search_user_notes] å¤„ç†ç¬”è®°æ—¶å‡ºé”™: {e}")
                            continue
                    
                    # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç¬”è®°ï¼Œç»§ç»­æœç´¢ä¸‹ä¸€é¡µ
                    page += 1
                    
                    # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                    await asyncio.sleep(1)
                    
                except Exception as e:
                    utils.logger.error(f"[XiaoHongShuClient.search_user_notes] ç¬¬ {page} é¡µæœç´¢å¤±è´¥: {e}")
                    break
            
            utils.logger.info(f"[XiaoHongShuClient.search_user_notes] æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(all_matching_notes)} ä¸ªåŒ¹é…ç”¨æˆ· {user_id} çš„ç¬”è®°")
            return all_matching_notes
            
        except Exception as e:
            utils.logger.error(f"[XiaoHongShuClient.search_user_notes] æœç´¢ç”¨æˆ·ç¬”è®°å¤±è´¥: {e}")
            return []
