# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š  
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚  
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚  
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚  
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚   
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#   
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚  
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚  


# -*- coding: utf-8 -*-
# @Author  : relakkes@gmail.com
# @Time    : 2023/12/2 18:44
# @Desc    : Bç«™çˆ¬è™«

import asyncio
import os
import random
import json
from asyncio import Task
from typing import Dict, List, Optional, Tuple, Union
from datetime import datetime, timedelta
import pandas as pd
import time

from playwright.async_api import (BrowserContext, BrowserType, Page, async_playwright)

import config
from base.base_crawler import AbstractCrawler
from proxy.proxy_ip_pool import IpInfoModel, create_ip_pool
from store import bilibili as bilibili_store
from tools import utils
from var import crawler_type_var, source_keyword_var

from .client import BilibiliClient
from .exception import DataFetchError
from .field import SearchOrderType
from .login import BilibiliLogin
from utils.db_utils import get_cookies_from_database


class BilibiliCrawler(AbstractCrawler):
    context_page: Page
    bili_client: BilibiliClient
    browser_context: BrowserContext

    def __init__(self, task_id: str = None):
        self.index_url = "https://www.bilibili.com"
        self.user_agent = utils.get_user_agent()
        # ä½¿ç”¨å­˜å‚¨å·¥å‚åˆ›å»ºå­˜å‚¨å¯¹è±¡
        from store.bilibili import BilibiliStoreFactory
        self.bilibili_store = BilibiliStoreFactory.create_store()
        self.task_id = task_id

    async def start(self) -> None:
        """åˆå§‹åŒ–çˆ¬è™«ï¼Œåˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡å’Œå®¢æˆ·ç«¯"""
        playwright_proxy_format, httpx_proxy_format = None, None
        if config.ENABLE_IP_PROXY:
            ip_proxy_pool = await create_ip_pool(config.IP_PROXY_POOL_COUNT, enable_validate_ip=True)
            ip_proxy_info: IpInfoModel = await ip_proxy_pool.get_proxy()
            playwright_proxy_format, httpx_proxy_format = self.format_proxy_info(ip_proxy_info)

        # åˆ›å»ºplaywrightå®ä¾‹ï¼Œä½†ä¸ä½¿ç”¨async withï¼Œè®©å®ƒåœ¨æ•´ä¸ªçˆ¬å–è¿‡ç¨‹ä¸­ä¿æŒæ‰“å¼€
        self.playwright = await async_playwright().start()
        
        # Launch a browser context.
        chromium = self.playwright.chromium
        self.browser_context = await self.launch_browser(
            chromium,
            None,
            self.user_agent,
            headless=config.HEADLESS
        )
        # stealth.min.js is a js script to prevent the website from detecting the crawler.
        await self.browser_context.add_init_script(path="libs/stealth.min.js")
        self.context_page = await self.browser_context.new_page()
        await self.context_page.goto(self.index_url)

        # Create a client to interact with the bilibili website.
        self.bili_client = await self.create_bilibili_client(httpx_proxy_format)
        
        # ğŸ†• ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­çš„tokenï¼Œæ— éœ€å¤æ‚ç™»å½•æµç¨‹
        utils.logger.info("[BilibiliCrawler] å¼€å§‹ä½¿ç”¨æ•°æ®åº“ä¸­çš„ç™»å½•å‡­è¯...")
        
        # ä»ä¼ å…¥çš„å‚æ•°ä¸­è·å–account_id
        account_id = getattr(self, 'account_id', None)
        if account_id:
            utils.logger.info(f"[BilibiliCrawler] ä½¿ç”¨æŒ‡å®šè´¦å·: {account_id}")
        else:
            utils.logger.info(f"[BilibiliCrawler] ä½¿ç”¨é»˜è®¤è´¦å·ï¼ˆæœ€æ–°ç™»å½•ï¼‰")
        
        # ä»æ•°æ®åº“è·å–cookies
        cookie_str = await get_cookies_from_database("bili", account_id)
        
        if cookie_str:
            utils.logger.info("[BilibiliCrawler] å‘ç°æ•°æ®åº“ä¸­çš„cookiesï¼Œç›´æ¥ä½¿ç”¨...")
            try:
                # è®¾ç½®cookiesåˆ°æµè§ˆå™¨
                await self.bili_client.set_cookies_from_string(cookie_str)
                
                # éªŒè¯cookiesæ˜¯å¦æœ‰æ•ˆ
                if await self.bili_client.pong():
                    utils.logger.info("[BilibiliCrawler] âœ… æ•°æ®åº“ä¸­çš„cookiesæœ‰æ•ˆï¼Œå¼€å§‹çˆ¬å–")
                    # æ›´æ–°cookiesåˆ°å®¢æˆ·ç«¯
                    await self.bili_client.update_cookies(browser_context=self.browser_context)
                else:
                    utils.logger.error("[BilibiliCrawler] âŒ æ•°æ®åº“ä¸­çš„cookiesæ— æ•ˆï¼Œæ— æ³•ç»§ç»­")
                    raise Exception("æ•°æ®åº“ä¸­çš„ç™»å½•å‡­è¯æ— æ•ˆï¼Œè¯·é‡æ–°ç™»å½•")
            except Exception as e:
                utils.logger.error(f"[BilibiliCrawler] ä½¿ç”¨æ•°æ®åº“cookieså¤±è´¥: {e}")
                raise Exception(f"ä½¿ç”¨æ•°æ®åº“ç™»å½•å‡­è¯å¤±è´¥: {str(e)}")
        else:
            utils.logger.error("[BilibiliCrawler] âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç™»å½•å‡­è¯")
            raise Exception("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç™»å½•å‡­è¯ï¼Œè¯·å…ˆç™»å½•")
        
        utils.logger.info("[BilibiliCrawler.start] çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œæµè§ˆå™¨ä¸Šä¸‹æ–‡å·²åˆ›å»º")

    async def get_creators_and_notes(self) -> None:
        """Get creator's videos and retrieve their comment information."""
        utils.logger.info(
            "[BilibiliCrawler.get_creators_and_notes] Begin get bilibili creators"
        )
        for creator_id in config.BILI_CREATOR_ID_LIST:
            # get creator detail info
            creator_info: Dict = await self.bili_client.get_creator_info(creator_id=int(creator_id))
            if creator_info:
                await self.bilibili_store.store_creator(creator_info)
                utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes] creator info: {creator_info}")

            # Get all video information of the creator
            all_video_list = await self.get_creator_videos(creator_id=int(creator_id))
            if all_video_list:
                utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes] got creator_id:{creator_id} videos len : {len(all_video_list)}")
                
                # å¤„ç†æ¯ä¸ªè§†é¢‘ï¼Œè·å–è¯¦ç»†ä¿¡æ¯å’Œæ’­æ”¾åœ°å€
                for video_item in all_video_list:
                    try:
                        # è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
                        video_detail = await self.get_video_info_task(
                            aid=video_item.get("aid", 0), 
                            bvid=video_item.get("bvid", ""), 
                            semaphore=asyncio.Semaphore(5)
                        )
                        
                        if video_detail:
                            # è·å–æ’­æ”¾åœ°å€
                            video_aid = video_detail.get("View", {}).get("aid")
                            video_cid = video_detail.get("View", {}).get("cid")
                            
                            if video_aid and video_cid:
                                play_url_result = await self.get_video_play_url_task(
                                    video_aid, video_cid, asyncio.Semaphore(5)
                                )
                                if play_url_result:
                                    video_detail.update(play_url_result)
                            
                            # ä¿å­˜åˆ°æ•°æ®åº“
                            await self.bilibili_store.update_bilibili_video(video_detail, task_id=self.task_id)
                            await self.bilibili_store.update_up_info(video_detail)
                            await self.get_bilibili_video(video_detail, asyncio.Semaphore(5))
                        
                    except Exception as e:
                        utils.logger.error(f"[BilibiliCrawler.get_creators_and_notes] å¤„ç†è§†é¢‘å¤±è´¥: {e}")
                        continue
                
                # Get comments for all videos
                video_ids = [video_item.get("bvid") for video_item in all_video_list if video_item.get("bvid")]
                await self.batch_get_video_comments(video_ids)
            else:
                utils.logger.warning(f"[BilibiliCrawler.get_creators_and_notes] creator_id:{creator_id} not found")

    async def search(self):
        """
        search bilibili video with keywords
        :return:
        """
        utils.logger.info("[BilibiliCrawler.search] Begin search bilibli keywords")
        bili_limit_count = 20  # bilibili limit page fixed value
        if config.CRAWLER_MAX_NOTES_COUNT < bili_limit_count:
            config.CRAWLER_MAX_NOTES_COUNT = bili_limit_count
        start_page = config.START_PAGE  # start page number
        
        # æ·»åŠ èµ„æºç›‘æ§
        start_time = time.time()
        processed_count = 0
        
        # ğŸ†• ä¿®å¤ï¼šå®Œå…¨å¿½ç•¥é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®å­—ï¼Œä½¿ç”¨åŠ¨æ€ä¼ å…¥çš„å…³é”®å­—
        # ä»å®ä¾‹å˜é‡è·å–å…³é”®å­—ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ï¼ˆå‘åå…¼å®¹ï¼‰
        keywords_to_search = getattr(self, 'dynamic_keywords', None)
        if not keywords_to_search:
            utils.logger.warning("[BilibiliCrawler.search] æœªæ‰¾åˆ°åŠ¨æ€å…³é”®å­—ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®å­—ï¼ˆå‘åå…¼å®¹ï¼‰")
            keywords_to_search = config.KEYWORDS
        
        # ç¡®ä¿å…³é”®å­—ä¸ä¸ºç©º
        if not keywords_to_search or not keywords_to_search.strip():
            utils.logger.error("[BilibiliCrawler.search] æ²¡æœ‰æœ‰æ•ˆçš„å…³é”®å­—ï¼Œæ— æ³•è¿›è¡Œæœç´¢")
            return
        
        # å¤„ç†å¤šä¸ªå…³é”®å­—ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰
        keyword_list = [kw.strip() for kw in keywords_to_search.split(",") if kw.strip()]
        
        for keyword in keyword_list:
            source_keyword_var.set(keyword)
            utils.logger.info(f"[BilibiliCrawler.search] Current search keyword: {keyword}")
            # æ¯ä¸ªå…³é”®è¯æœ€å¤šè¿”å› 1000 æ¡æ•°æ®
            if not config.ALL_DAY:
                page = 1
                while (page - start_page + 1) * bili_limit_count <= config.CRAWLER_MAX_NOTES_COUNT:
                    if page < start_page:
                        utils.logger.info(f"[BilibiliCrawler.search] Skip page: {page}")
                        page += 1
                        continue

                    try:
                        utils.logger.info(f"[BilibiliCrawler.search] search bilibili keyword: {keyword}, page: {page}")
                        video_id_list: List[str] = []
                        videos_res = await self.bili_client.search_video_by_keyword(
                            keyword=keyword,
                            page=page,
                            page_size=bili_limit_count,
                            order=SearchOrderType.DEFAULT,
                            pubtime_begin_s=0,  # ä½œå“å‘å¸ƒæ—¥æœŸèµ·å§‹æ—¶é—´æˆ³
                            pubtime_end_s=0  # ä½œå“å‘å¸ƒæ—¥æœŸç»“æŸæ—¥æœŸæ—¶é—´æˆ³
                        )
                        video_list: List[Dict] = videos_res.get("result")

                        # é™åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…èµ„æºè€—å°½
                        max_concurrent = min(config.MAX_CONCURRENCY_NUM, len(video_list))
                        semaphore = asyncio.Semaphore(max_concurrent)
                        
                        # åˆ†æ‰¹å¤„ç†è§†é¢‘è¯¦æƒ…
                        batch_size = 5  # æ¯æ‰¹å¤„ç†5ä¸ªè§†é¢‘
                        video_items = []
                        
                        for i in range(0, len(video_list), batch_size):
                            batch_videos = video_list[i:i + batch_size]
                            utils.logger.info(f"[BilibiliCrawler.search] Processing video batch {i//batch_size + 1}, items: {len(batch_videos)}")
                            
                            task_list = []
                            try:
                                task_list = [self.get_video_info_task(aid=video_item.get("aid"), bvid="", semaphore=semaphore) for video_item in batch_videos]
                            except Exception as e:
                                utils.logger.warning(f"[BilibiliCrawler.search] error in the task list. The video for this page will not be included. {e}")
                                continue
                            
                            try:
                                # æ·»åŠ è¶…æ—¶æ§åˆ¶
                                batch_results = await asyncio.wait_for(
                                    asyncio.gather(*task_list, return_exceptions=True),
                                    timeout=60  # 60ç§’è¶…æ—¶
                                )
                                video_items.extend([r for r in batch_results if not isinstance(r, Exception)])
                            except asyncio.TimeoutError:
                                utils.logger.warning(f"[BilibiliCrawler.search] Video batch timeout, skipping remaining items")
                                break
                            except Exception as e:
                                utils.logger.error(f"[BilibiliCrawler.search] Video batch processing error: {e}")
                                continue
                            
                            # æ·»åŠ é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                            await asyncio.sleep(1)
                        
                        # å¤„ç†è§†é¢‘è¯¦æƒ…
                        for video_item in video_items:
                            if video_item:
                                try:
                                    import json
                                    utils.logger.info(f"[BilibiliCrawler] åŸå§‹è§†é¢‘æ•°æ®: {json.dumps(video_item, ensure_ascii=False)}")
                                    
                                    # è·å–è§†é¢‘æ’­æ”¾åœ°å€
                                    video_info = video_item.get("View", {})
                                    aid = video_info.get("aid")
                                    cid = video_info.get("cid")
                                    
                                    if aid and cid:
                                        # è·å–æ’­æ”¾åœ°å€
                                        utils.logger.info(f"[BilibiliCrawler] å¼€å§‹è·å–æ’­æ”¾åœ°å€ - aid: {aid}, cid: {cid}")
                                        play_url_result = await self.get_video_play_url_task(aid, cid, semaphore)
                                        if play_url_result:
                                            # å°†æ’­æ”¾åœ°å€æ•°æ®åˆå¹¶åˆ°è§†é¢‘ä¿¡æ¯ä¸­
                                            video_item.update(play_url_result)
                                            utils.logger.info(f"[BilibiliCrawler] è·å–åˆ°æ’­æ”¾åœ°å€æ•°æ®: {json.dumps(play_url_result, ensure_ascii=False)}")
                                        else:
                                            utils.logger.warning(f"[BilibiliCrawler] è·å–æ’­æ”¾åœ°å€å¤±è´¥ - aid: {aid}, cid: {cid}")
                                    else:
                                        utils.logger.warning(f"[BilibiliCrawler] ç¼ºå°‘aidæˆ–cid - aid: {aid}, cid: {cid}")
                                    
                                    video_id_list.append(aid)
                                    await self.bilibili_store.update_bilibili_video(video_item, task_id=self.task_id)
                                    await self.bilibili_store.update_up_info(video_item)
                                    await self.get_bilibili_video(video_item, semaphore)
                                    processed_count += 1
                                except Exception as e:
                                    utils.logger.error(f"[BilibiliCrawler.search] Failed to process video: {e}")
                                    continue
                        
                        # æ£€æŸ¥å¤„ç†æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´è¿è¡Œ
                        elapsed_time = time.time() - start_time
                        if elapsed_time > 300:  # 5åˆ†é’Ÿè¶…æ—¶
                            utils.logger.warning(f"[BilibiliCrawler.search] Processing time exceeded 5 minutes, stopping")
                            break
                        
                        # è·å–è¯„è®ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if config.ENABLE_GET_COMMENTS and video_id_list:
                            try:
                                await self.batch_get_video_comments(video_id_list)
                            except Exception as e:
                                utils.logger.error(f"[BilibiliCrawler.search] Failed to get comments: {e}")
                        
                        page += 1
                        
                    except Exception as e:
                        utils.logger.error(f"[BilibiliCrawler.search] Unexpected error during search: {e}")
                        page += 1
                        continue
                        
            # æŒ‰ç…§ START_DAY è‡³ END_DAY æŒ‰ç…§æ¯ä¸€å¤©è¿›è¡Œç­›é€‰ï¼Œè¿™æ ·èƒ½å¤Ÿçªç ´ 1000 æ¡è§†é¢‘çš„é™åˆ¶ï¼Œæœ€å¤§ç¨‹åº¦çˆ¬å–è¯¥å…³é”®è¯ä¸‹æ¯ä¸€å¤©çš„æ‰€æœ‰è§†é¢‘
            else:
                for day in pd.date_range(start=config.START_DAY, end=config.END_DAY, freq='D'):
                    # æŒ‰ç…§æ¯ä¸€å¤©è¿›è¡Œçˆ¬å–çš„æ—¶é—´æˆ³å‚æ•°
                    pubtime_begin_s, pubtime_end_s = await self.get_pubtime_datetime(start=day.strftime('%Y-%m-%d'), end=day.strftime('%Y-%m-%d'))
                    page = 1
                    #!è¯¥æ®µ while è¯­å¥åœ¨å‘ç”Ÿå¼‚å¸¸æ—¶ï¼ˆé€šå¸¸æƒ…å†µä¸‹ä¸ºå½“å¤©æ•°æ®ä¸ºç©ºæ—¶ï¼‰ä¼šè‡ªåŠ¨è·³è½¬åˆ°ä¸‹ä¸€å¤©ï¼Œä»¥å®ç°æœ€å¤§ç¨‹åº¦çˆ¬å–è¯¥å…³é”®è¯ä¸‹å½“å¤©çš„æ‰€æœ‰è§†é¢‘
                    #!é™¤äº†ä»…ä¿ç•™ç°åœ¨åŸæœ‰çš„ try, except Exception è¯­å¥å¤–ï¼Œä¸è¦å†æ·»åŠ å…¶ä»–çš„å¼‚å¸¸å¤„ç†ï¼ï¼ï¼å¦åˆ™å°†ä½¿è¯¥æ®µä»£ç å¤±æ•ˆï¼Œä½¿å…¶ä»…èƒ½çˆ¬å–å½“å¤©ä¸€å¤©æ•°æ®è€Œæ— æ³•è·³è½¬åˆ°ä¸‹ä¸€å¤©
                    #!é™¤éå°†è¯¥æ®µä»£ç çš„é€»è¾‘è¿›è¡Œé‡æ„ä»¥å®ç°ç›¸åŒçš„åŠŸèƒ½ï¼Œå¦åˆ™ä¸è¦è¿›è¡Œä¿®æ”¹ï¼ï¼ï¼
                    while (page - start_page + 1) * bili_limit_count <= config.CRAWLER_MAX_NOTES_COUNT:
                        #! Catch any error if response return nothing, go to next day
                        try:
                            #! Don't skip any page, to make sure gather all video in one day
                            # if page < start_page:
                            #     utils.logger.info(f"[BilibiliCrawler.search] Skip page: {page}")
                            #     page += 1
                            #     continue

                            utils.logger.info(f"[BilibiliCrawler.search] search bilibili keyword: {keyword}, date: {day.ctime()}, page: {page}")
                            video_id_list: List[str] = []
                            videos_res = await self.bili_client.search_video_by_keyword(
                                keyword=keyword,
                                page=page,
                                page_size=bili_limit_count,
                                order=SearchOrderType.DEFAULT,
                                pubtime_begin_s=pubtime_begin_s,  # ä½œå“å‘å¸ƒæ—¥æœŸèµ·å§‹æ—¶é—´æˆ³
                                pubtime_end_s=pubtime_end_s  # ä½œå“å‘å¸ƒæ—¥æœŸç»“æŸæ—¥æœŸæ—¶é—´æˆ³
                            )
                            video_list: List[Dict] = videos_res.get("result")

                            # é™åˆ¶å¹¶å‘æ•°é‡
                            max_concurrent = min(config.MAX_CONCURRENCY_NUM, len(video_list))
                            semaphore = asyncio.Semaphore(max_concurrent)
                            
                            # åˆ†æ‰¹å¤„ç†è§†é¢‘è¯¦æƒ…
                            batch_size = 5
                            video_items = []
                            
                            for i in range(0, len(video_list), batch_size):
                                batch_videos = video_list[i:i + batch_size]
                                task_list = [self.get_video_info_task(aid=video_item.get("aid"), bvid="", semaphore=semaphore) for video_item in batch_videos]
                                
                                try:
                                    batch_results = await asyncio.wait_for(
                                        asyncio.gather(*task_list, return_exceptions=True),
                                        timeout=60
                                    )
                                    video_items.extend([r for r in batch_results if not isinstance(r, Exception)])
                                except asyncio.TimeoutError:
                                    utils.logger.warning(f"[BilibiliCrawler.search] Video batch timeout")
                                    break
                                except Exception as e:
                                    utils.logger.error(f"[BilibiliCrawler.search] Video batch error: {e}")
                                    continue
                                
                                await asyncio.sleep(1)
                            
                            for video_item in video_items:
                                if video_item:
                                    try:
                                        import json
                                        utils.logger.info(f"[BilibiliCrawler] åŸå§‹è§†é¢‘æ•°æ®: {json.dumps(video_item, ensure_ascii=False)}")
                                        
                                        # è·å–è§†é¢‘æ’­æ”¾åœ°å€
                                        video_info = video_item.get("View", {})
                                        aid = video_info.get("aid")
                                        cid = video_info.get("cid")
                                        
                                        if aid and cid:
                                            # è·å–æ’­æ”¾åœ°å€
                                            utils.logger.info(f"[BilibiliCrawler] å¼€å§‹è·å–æ’­æ”¾åœ°å€ - aid: {aid}, cid: {cid}")
                                            play_url_result = await self.get_video_play_url_task(aid, cid, semaphore)
                                            if play_url_result:
                                                # å°†æ’­æ”¾åœ°å€æ•°æ®åˆå¹¶åˆ°è§†é¢‘ä¿¡æ¯ä¸­
                                                video_item.update(play_url_result)
                                                utils.logger.info(f"[BilibiliCrawler] è·å–åˆ°æ’­æ”¾åœ°å€æ•°æ®: {json.dumps(play_url_result, ensure_ascii=False)}")
                                        else:
                                            utils.logger.warning(f"[BilibiliCrawler] ç¼ºå°‘aidæˆ–cid - aid: {aid}, cid: {cid}")
                                        
                                        video_id_list.append(aid)
                                        await self.bilibili_store.update_bilibili_video(video_item, task_id=self.task_id)
                                        await self.bilibili_store.update_up_info(video_item)
                                        await self.get_bilibili_video(video_item, semaphore)
                                        processed_count += 1
                                    except Exception as e:
                                        utils.logger.error(f"[BilibiliCrawler.search] Failed to process video: {e}")
                                        continue
                            
                            page += 1
                            await self.batch_get_video_comments(video_id_list)
                        # go to next day
                        except Exception as e:
                            print(e)
                            break
            
            utils.logger.info(f"[BilibiliCrawler.search] Search completed. Total processed: {processed_count}")

    async def batch_get_video_comments(self, video_id_list: List[str]):
        """
        batch get video comments
        :param video_id_list:
        :return:
        """
        if not config.ENABLE_GET_COMMENTS:
            utils.logger.info(
                f"[BilibiliCrawler.batch_get_note_comments] Crawling comment mode is not enabled")
            return

        utils.logger.info(
            f"[BilibiliCrawler.batch_get_video_comments] video ids:{video_id_list}")
        
        # é™åˆ¶å¹¶å‘æ•°é‡
        max_concurrent = min(config.MAX_CONCURRENCY_NUM, len(video_id_list))
        semaphore = asyncio.Semaphore(max_concurrent)
        
        # åˆ†æ‰¹å¤„ç†è¯„è®º
        batch_size = 3  # æ¯æ‰¹å¤„ç†3ä¸ªè¯„è®ºä»»åŠ¡
        total_processed = 0
        
        for i in range(0, len(video_id_list), batch_size):
            batch_videos = video_id_list[i:i + batch_size]
            
            utils.logger.info(f"[BilibiliCrawler.batch_get_video_comments] Processing comment batch {i//batch_size + 1}, videos: {len(batch_videos)}")
            
            task_list: List[Task] = []
            for video_id in batch_videos:
                task = asyncio.create_task(self.get_comments(
                    video_id, semaphore), name=video_id)
                task_list.append(task)
            
            try:
                # æ·»åŠ è¶…æ—¶æ§åˆ¶
                await asyncio.wait_for(
                    asyncio.gather(*task_list, return_exceptions=True),
                    timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
                )
                total_processed += len(batch_videos)
                utils.logger.info(f"[BilibiliCrawler.batch_get_video_comments] Completed batch {i//batch_size + 1}")
            except asyncio.TimeoutError:
                utils.logger.warning(f"[BilibiliCrawler.batch_get_video_comments] Comment batch timeout")
                break
            except Exception as e:
                utils.logger.error(f"[BilibiliCrawler.batch_get_video_comments] Comment batch error: {e}")
                continue
            
            # æ·»åŠ é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            await asyncio.sleep(2)
        
        utils.logger.info(f"[BilibiliCrawler.batch_get_video_comments] Comment processing completed. Total processed: {total_processed}")

    async def get_comments(self, video_id: str, semaphore: asyncio.Semaphore):
        """
        get comment for video id
        :param video_id:
        :param semaphore:
        :return:
        """
        async with semaphore:
            try:
                utils.logger.info(
                    f"[BilibiliCrawler.get_comments] begin get video_id: {video_id} comments ...")
                await self.bili_client.get_video_all_comments(
                    video_id=video_id,
                    crawl_interval=random.random(),
                    is_fetch_sub_comments=config.ENABLE_GET_SUB_COMMENTS,
                    callback=self.bilibili_store.batch_update_bilibili_video_comments,
                    max_count=config.CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES,
                )

            except DataFetchError as ex:
                utils.logger.error(
                    f"[BilibiliCrawler.get_comments] get video_id: {video_id} comment error: {ex}")
            except Exception as e:
                utils.logger.error(
                    f"[BilibiliCrawler.get_comments] may be been blocked, err:{e}")

    async def get_creator_videos(self, creator_id: int, max_count: int = None):
        """
        get videos for a creator
        :param creator_id: åˆ›ä½œè€…ID
        :param max_count: æœ€å¤§è·å–æ•°é‡ï¼ŒNoneè¡¨ç¤ºè·å–æ‰€æœ‰
        :return: List[Dict] åˆ›ä½œè€…è§†é¢‘åˆ—è¡¨
        """
        ps = 30
        pn = 1
        max_pages = 10  # æœ€å¤§è·å–10é¡µï¼Œé¿å…æ— é™å¾ªç¯
        video_bvids_list = []
        all_video_list = []
        
        try:
            while True:
                result = await self.bili_client.get_creator_videos(creator_id, pn, ps)
                utils.logger.info(f"[BilibiliCrawler.get_creator_videos] è·å–åˆ›ä½œè€… {creator_id} ç¬¬ {pn} é¡µè§†é¢‘åˆ—è¡¨")
                
                # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼ŒæŸ¥çœ‹APIè¿”å›çš„æ•°æ®ç»“æ„
                import json
                utils.logger.debug(f"[BilibiliCrawler.get_creator_videos] APIè¿”å›æ•°æ®ç»“æ„: {json.dumps(result, ensure_ascii=False)[:500]}...")
                
                if not result:
                    utils.logger.warning(f"[BilibiliCrawler.get_creator_videos] è·å–åˆ›ä½œè€…è§†é¢‘åˆ—è¡¨å¤±è´¥: ç»“æœä¸ºç©º")
                    break
                
                if "list" not in result:
                    utils.logger.warning(f"[BilibiliCrawler.get_creator_videos] è·å–åˆ›ä½œè€…è§†é¢‘åˆ—è¡¨å¤±è´¥: ç¼ºå°‘listå­—æ®µ, {result}")
                    break
                
                if "vlist" not in result["list"]:
                    utils.logger.warning(f"[BilibiliCrawler.get_creator_videos] è·å–åˆ›ä½œè€…è§†é¢‘åˆ—è¡¨å¤±è´¥: ç¼ºå°‘vlistå­—æ®µ, {result['list']}")
                    break
                
                video_list = result["list"]["vlist"]
                utils.logger.info(f"[BilibiliCrawler.get_creator_videos] ç¬¬ {pn} é¡µè·å–åˆ° {len(video_list)} ä¸ªè§†é¢‘")
                
                for video in video_list:
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶
                    if max_count is not None and len(all_video_list) >= max_count:
                        utils.logger.info(f"[BilibiliCrawler.get_creator_videos] å·²è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶ {max_count}ï¼Œåœæ­¢è·å–")
                        break
                    
                    bvid = video.get("bvid", "")
                    if bvid:
                        video_bvids_list.append(bvid)
                    # æ„å»ºåŸºç¡€è§†é¢‘ä¿¡æ¯ - ä½¿ç”¨å®‰å…¨çš„å­—æ®µè®¿é—®
                    video_info = {
                        "bvid": video.get("bvid", ""),
                        "aid": video.get("aid", 0),
                        "title": video.get("title", ""),
                        "desc": video.get("description", ""),
                        "duration": video.get("duration", 0),
                        "pic": video.get("pic", ""),
                        "owner": {
                            "mid": video.get("owner", {}).get("mid", 0),
                            "name": video.get("owner", {}).get("name", ""),
                            "face": video.get("owner", {}).get("face", "")
                        },
                        "stat": {
                            "view": video.get("stat", {}).get("view", 0),
                            "danmaku": video.get("stat", {}).get("danmaku", 0),
                            "reply": video.get("stat", {}).get("reply", 0),
                            "favorite": video.get("stat", {}).get("favorite", 0),
                            "coin": video.get("stat", {}).get("coin", 0),
                            "share": video.get("stat", {}).get("share", 0),
                            "like": video.get("stat", {}).get("like", 0)
                        },
                        "pubdate": video.get("pubdate", 0),
                        "ctime": video.get("ctime", 0)
                    }
                    all_video_list.append(video_info)
                
                # å¦‚æœå·²è¾¾åˆ°æœ€å¤§æ•°é‡é™åˆ¶ï¼Œè·³å‡ºåˆ†é¡µå¾ªç¯
                if max_count is not None and len(all_video_list) >= max_count:
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µ
                page_info = result.get("page", {})
                total_count = page_info.get("count", 0)
                current_count = pn * ps
                
                utils.logger.info(f"[BilibiliCrawler.get_creator_videos] æ€»è§†é¢‘æ•°: {total_count}, å½“å‰å·²è·å–: {current_count}")
                
                if total_count <= current_count:
                    utils.logger.info(f"[BilibiliCrawler.get_creator_videos] å·²è·å–æ‰€æœ‰è§†é¢‘ï¼Œåœæ­¢åˆ†é¡µ")
                    break
                
                if pn >= max_pages:
                    utils.logger.warning(f"[BilibiliCrawler.get_creator_videos] å·²è¾¾åˆ°æœ€å¤§é¡µæ•°é™åˆ¶ {max_pages}ï¼Œåœæ­¢è·å–")
                    break
                
                await asyncio.sleep(random.random())
                pn += 1
            
            utils.logger.info(f"[BilibiliCrawler.get_creator_videos] è·å–åˆ° {len(all_video_list)} ä¸ªè§†é¢‘")
            return all_video_list
            
        except Exception as e:
            utils.logger.error(f"[BilibiliCrawler.get_creator_videos] è·å–åˆ›ä½œè€…è§†é¢‘å¤±è´¥: {e}")
            import traceback
            utils.logger.error(f"[BilibiliCrawler.get_creator_videos] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return []

    async def get_creator_videos_by_keywords(self, creator_id: int, keywords: str, max_count: int = None):
        """
        ä½¿ç”¨å…³é”®è¯æœç´¢è·å–åˆ›ä½œè€…çš„è§†é¢‘
        :param creator_id: åˆ›ä½œè€…ID
        :param keywords: æœç´¢å…³é”®è¯
        :param max_count: æœ€å¤§è·å–æ•°é‡ï¼ŒNoneè¡¨ç¤ºè·å–æ‰€æœ‰
        :return: List[Dict] åŒ¹é…å…³é”®è¯çš„è§†é¢‘åˆ—è¡¨
        """
        try:
            utils.logger.info(f"[BilibiliCrawler.get_creator_videos_by_keywords] å¼€å§‹å…³é”®è¯æœç´¢")
            utils.logger.info(f"[BilibiliCrawler.get_creator_videos_by_keywords] åˆ›ä½œè€…ID: {creator_id}")
            utils.logger.info(f"[BilibiliCrawler.get_creator_videos_by_keywords] æœç´¢å…³é”®è¯: '{keywords}'")
            utils.logger.info(f"[BilibiliCrawler.get_creator_videos_by_keywords] æœ€å¤§æ•°é‡é™åˆ¶: {max_count}")
            
            # ä½¿ç”¨Bç«™åˆ›ä½œè€…ä¸»é¡µä¸“ç”¨æœç´¢API
            search_result = await self.bili_client.search_creator_videos(creator_id, keywords)
            
            utils.logger.info(f"[BilibiliCrawler.get_creator_videos_by_keywords] æœç´¢APIè¿”å›ç»“æœ: {search_result}")
            
            # æ£€æŸ¥è¿”å›çš„æ•°æ®ç»“æ„ - æ ¹æ®çœŸå®APIè¿”å›çš„æ•°æ®ç»“æ„
            if not search_result or "list" not in search_result:
                utils.logger.warning(f"[BilibiliCrawler.get_creator_videos_by_keywords] æœç´¢å¤±è´¥æˆ–æ— ç»“æœ")
                return []
            
            # æ£€æŸ¥è¿”å›çš„æ•°æ®ç»“æ„
            if "vlist" not in search_result["list"]:
                utils.logger.warning(f"[BilibiliCrawler.get_creator_videos_by_keywords] æœç´¢è¿”å›æ•°æ®ç»“æ„å¼‚å¸¸: {search_result}")
                return []
            
            video_list = search_result["list"]["vlist"]
            utils.logger.info(f"[BilibiliCrawler.get_creator_videos_by_keywords] æœç´¢åˆ° {len(video_list)} ä¸ªåŒ¹é…çš„è§†é¢‘")
            
            # æ„å»ºæ ‡å‡†åŒ–çš„è§†é¢‘ä¿¡æ¯
            all_video_list = []
            for video in video_list:
                # æ„å»ºåŸºç¡€è§†é¢‘ä¿¡æ¯ - ä½¿ç”¨å®‰å…¨çš„å­—æ®µè®¿é—®
                # ä¿ç•™åŸå§‹å­—æ®µï¼Œç¡®ä¿åç»­å¤„ç†èƒ½æ­£å¸¸å·¥ä½œ
                video_info = {
                    # æ ¸å¿ƒå­—æ®µ - ç”¨äºåç»­è·å–è¯¦ç»†ä¿¡æ¯
                    "bvid": video.get("bvid", ""),
                    "aid": video.get("aid", 0),
                    "title": video.get("title", ""),
                    "description": video.get("description", ""),  # çœŸå®APIä¸­æ˜¯"description"
                    "content": video.get("description", ""),  # å†…å®¹ä½¿ç”¨æè¿°
                    "content_type": "video",  # å›ºå®šå€¼
                    "content_id": video.get("bvid", ""),  # ä½¿ç”¨bvidä½œä¸ºcontent_id
                    "author_id": video.get("mid", 0),  # çœŸå®APIä¸­æ˜¯"mid"
                    "author_name": video.get("author", ""),  # çœŸå®APIä¸­æ˜¯"author"
                    "author_nickname": video.get("author", ""),  # çœŸå®APIä¸­æ˜¯"author"
                    "author_avatar": "",  # çœŸå®APIä¸­æ²¡æœ‰ç›´æ¥çš„avatarå­—æ®µ
                    "author_signature": "",  # çœŸå®APIä¸­æ²¡æœ‰signatureå­—æ®µ
                    "author_unique_id": "",  # çœŸå®APIä¸­æ²¡æœ‰unique_idå­—æ®µ
                    "author_sec_uid": "",  # çœŸå®APIä¸­æ²¡æœ‰sec_uidå­—æ®µ
                    "author_short_id": "",  # çœŸå®APIä¸­æ²¡æœ‰short_idå­—æ®µ
                    "like_count": 0,  # çœŸå®APIä¸­æ²¡æœ‰likeå­—æ®µ
                    "comment_count": video.get("comment", 0),  # çœŸå®APIä¸­æ˜¯"comment"
                    "share_count": 0,  # çœŸå®APIä¸­æ²¡æœ‰shareå­—æ®µ
                    "collect_count": 0,  # çœŸå®APIä¸­æ²¡æœ‰collectå­—æ®µ
                    "view_count": video.get("play", 0),  # çœŸå®APIä¸­æ˜¯"play"
                    "cover_url": video.get("pic", ""),  # çœŸå®APIä¸­æ˜¯"pic"
                    "video_url": f"https://www.bilibili.com/video/{video.get('bvid', '')}",  # æ„å»ºæ’­æ”¾é¡µé“¾æ¥
                    "video_play_url": f"https://www.bilibili.com/video/{video.get('bvid', '')}",  # æ’­æ”¾é¡µé“¾æ¥
                    "video_download_url": "",  # éœ€è¦å•ç‹¬è·å–
                    "video_share_url": f"https://www.bilibili.com/video/{video.get('bvid', '')}",  # åˆ†äº«é“¾æ¥
                    "image_urls": [],  # çœŸå®APIä¸­æ²¡æœ‰image_urlså­—æ®µ
                    "audio_url": "",  # çœŸå®APIä¸­æ²¡æœ‰audio_urlå­—æ®µ
                    "file_urls": [],  # çœŸå®APIä¸­æ²¡æœ‰file_urlså­—æ®µ
                    "ip_location": "",  # çœŸå®APIä¸­æ²¡æœ‰ip_locationå­—æ®µ
                    "location": "",  # çœŸå®APIä¸­æ²¡æœ‰locationå­—æ®µ
                    "tags": "",  # çœŸå®APIä¸­æ²¡æœ‰tagså­—æ®µ
                    "categories": "",  # çœŸå®APIä¸­æ²¡æœ‰categorieså­—æ®µ
                    "topics": "",  # çœŸå®APIä¸­æ²¡æœ‰topicså­—æ®µ
                    "is_favorite": False,  # çœŸå®APIä¸­æ²¡æœ‰is_favoriteå­—æ®µ
                    "is_deleted": False,  # çœŸå®APIä¸­æ²¡æœ‰is_deletedå­—æ®µ
                    "is_private": False,  # çœŸå®APIä¸­æ²¡æœ‰is_privateå­—æ®µ
                    "is_original": True,  # å‡è®¾ä¸ºåŸåˆ›
                    "minio_url": "",  # éœ€è¦åç»­å¤„ç†
                    "local_path": "",  # éœ€è¦åç»­å¤„ç†
                    "file_size": 0,  # çœŸå®APIä¸­æ²¡æœ‰file_sizeå­—æ®µ
                    "storage_type": "",  # çœŸå®APIä¸­æ²¡æœ‰storage_typeå­—æ®µ
                    "metadata": json.dumps(video.get("meta", {}), ensure_ascii=False),  # åºåˆ—åŒ–metaæ•°æ®
                    "raw_data": json.dumps(video, ensure_ascii=False),  # åŸå§‹æ•°æ®
                    "extra_info": json.dumps({
                        "typeid": video.get("typeid", 0),
                        "copyright": video.get("copyright", "1"),
                        "review": video.get("review", 0),
                        "hide_click": video.get("hide_click", False),
                        "is_pay": video.get("is_pay", 0),
                        "is_union_video": video.get("is_union_video", 0),
                        "is_steins_gate": video.get("is_steins_gate", 0),
                        "is_live_playback": video.get("is_live_playback", 0),
                        "is_lesson_video": video.get("is_lesson_video", 0),
                        "is_lesson_finished": video.get("is_lesson_finished", 0),
                        "is_charging_arc": video.get("is_charging_arc", False),
                        "elec_arc_type": video.get("elec_arc_type", 0),
                        "elec_arc_badge": video.get("elec_arc_badge", ""),
                        "season_id": video.get("season_id", 0),
                        "attribute": video.get("attribute", 0),
                        "subtitle": video.get("subtitle", ""),
                        "jump_url": video.get("jump_url", ""),
                        "length": video.get("length", ""),
                        "video_review": video.get("video_review", 0)
                    }, ensure_ascii=False),
                    "create_time": video.get("created", 0),  # çœŸå®APIä¸­æ˜¯"created"
                    "publish_time": video.get("created", 0),  # çœŸå®APIä¸­æ˜¯"created"
                    "update_time": video.get("created", 0),  # çœŸå®APIä¸­æ˜¯"created"
                    "add_ts": int(time.time()),  # å½“å‰æ—¶é—´æˆ³
                    "last_modify_ts": int(time.time()),  # å½“å‰æ—¶é—´æˆ³
                    "source_keyword": keywords if keywords else "",  # æœç´¢å…³é”®è¯
                    # ä¿ç•™åŸå§‹å­—æ®µç”¨äºå…¼å®¹æ€§
                    "stat": {
                        "view": video.get("play", 0),
                        "danmaku": video.get("video_review", 0),
                        "reply": video.get("comment", 0),
                        "favorite": 0,
                        "coin": 0,
                        "share": 0,
                        "like": 0
                    },
                    "pubdate": video.get("created", 0),
                    "ctime": video.get("created", 0),
                    # æ·»åŠ åŸå§‹æ•°æ®ï¼Œç¡®ä¿åç»­å¤„ç†èƒ½è·å–åˆ°å®Œæ•´ä¿¡æ¯
                    "original_data": video
                }
                all_video_list.append(video_info)
                utils.logger.info(f"[BilibiliCrawler.get_creator_videos_by_keywords] åŒ¹é…è§†é¢‘: {video_info.get('title', 'æ— æ ‡é¢˜')}")
            
            # åº”ç”¨æ•°é‡é™åˆ¶
            if max_count is not None and len(all_video_list) > max_count:
                all_video_list = all_video_list[:max_count]
                utils.logger.info(f"[BilibiliCrawler.get_creator_videos_by_keywords] åº”ç”¨æ•°é‡é™åˆ¶ï¼Œä¿ç•™å‰ {len(all_video_list)} ä¸ªè§†é¢‘")
            
            utils.logger.info(f"[BilibiliCrawler.get_creator_videos_by_keywords] æœ€ç»ˆè¿”å› {len(all_video_list)} ä¸ªè§†é¢‘")
            return all_video_list
            
        except Exception as e:
            utils.logger.error(f"[BilibiliCrawler.get_creator_videos_by_keywords] å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            import traceback
            utils.logger.error(f"[BilibiliCrawler.get_creator_videos_by_keywords] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return []

    async def get_specified_videos(self, bvids_list: List[str]):
        """
        get specified videos info
        :return:
        """
        semaphore = asyncio.Semaphore(config.MAX_CONCURRENCY_NUM)
        task_list = [
            self.get_video_info_task(aid=0, bvid=video_id, semaphore=semaphore) for video_id in
            bvids_list
        ]
        video_details = await asyncio.gather(*task_list)
        video_aids_list = []
        for video_detail in video_details:
            if video_detail is not None:
                import json
                utils.logger.info(f"[BilibiliCrawler] åŸå§‹è§†é¢‘æ•°æ®: {json.dumps(video_detail, ensure_ascii=False)}")
                video_item_view: Dict = video_detail.get("View")
                video_aid: str = video_item_view.get("aid")
                video_cid: str = video_item_view.get("cid")
                
                if video_aid and video_cid:
                    # è·å–æ’­æ”¾åœ°å€
                    play_url_result = await self.get_video_play_url_task(video_aid, video_cid, semaphore)
                    if play_url_result:
                        # å°†æ’­æ”¾åœ°å€æ•°æ®åˆå¹¶åˆ°è§†é¢‘ä¿¡æ¯ä¸­
                        video_detail.update(play_url_result)
                        utils.logger.info(f"[BilibiliCrawler] è·å–åˆ°æ’­æ”¾åœ°å€æ•°æ®: {json.dumps(play_url_result, ensure_ascii=False)}")
                
                if video_aid:
                    video_aids_list.append(video_aid)
                await self.bilibili_store.update_bilibili_video(video_detail, task_id=self.task_id)
                await self.bilibili_store.update_up_info(video_detail)
                await self.get_bilibili_video(video_detail, semaphore)
        await self.batch_get_video_comments(video_aids_list)

    async def get_video_info_task(self, aid: int, bvid: str, semaphore: asyncio.Semaphore) -> Optional[Dict]:
        """
        Get video detail task
        :param aid:
        :param bvid:
        :param semaphore:
        :return:
        """
        async with semaphore:
            try:
                result = await self.bili_client.get_video_info(aid=aid, bvid=bvid)
                return result
            except DataFetchError as ex:
                utils.logger.error(
                    f"[BilibiliCrawler.get_video_info_task] Get video detail error: {ex}")
                return None
            except KeyError as ex:
                utils.logger.error(
                    f"[BilibiliCrawler.get_video_info_task] have not fund note detail video_id:{bvid}, err: {ex}")
                return None

    async def get_video_play_url_task(self, aid: int, cid: int, semaphore: asyncio.Semaphore) -> Union[Dict, None]:
        """
                Get video play url
                :param aid:
                :param cid:
                :param semaphore:
                :return:
                """
        async with semaphore:
            try:
                result = await self.bili_client.get_video_play_url(aid=aid, cid=cid)
                return result
            except DataFetchError as ex:
                utils.logger.error(
                    f"[BilibiliCrawler.get_video_play_url_task] Get video play url error: {ex}")
                return None
            except KeyError as ex:
                utils.logger.error(
                    f"[BilibiliCrawler.get_video_play_url_task] have not fund play url from :{aid}|{cid}, err: {ex}")
                return None

    async def create_bilibili_client(self, httpx_proxy: Optional[str]) -> BilibiliClient:
        """
        create bilibili client
        :param httpx_proxy: httpx proxy
        :return: bilibili client
        """
        utils.logger.info(
            "[BilibiliCrawler.create_bilibili_client] Begin create bilibili API client ...")
        cookie_str, cookie_dict = utils.convert_cookies(await self.browser_context.cookies())
        bilibili_client_obj = BilibiliClient(
            proxies=httpx_proxy,
            headers={
                "User-Agent": self.user_agent,
                "Cookie": cookie_str,
                "Origin": "https://www.bilibili.com",
                "Referer": "https://www.bilibili.com",
                "Content-Type": "application/json;charset=UTF-8"
            },
            playwright_page=self.context_page,
            cookie_dict=cookie_dict,
        )
        return bilibili_client_obj

    @staticmethod
    def format_proxy_info(ip_proxy_info: IpInfoModel) -> Tuple[Optional[Dict], Optional[Dict]]:
        """
        format proxy info for playwright and httpx
        :param ip_proxy_info: ip proxy info
        :return: playwright proxy, httpx proxy
        """
        playwright_proxy = {
            "server": f"{ip_proxy_info.protocol}{ip_proxy_info.ip}:{ip_proxy_info.port}",
            "username": ip_proxy_info.user,
            "password": ip_proxy_info.password,
        }
        httpx_proxy = {
            f"{ip_proxy_info.protocol}": f"http://{ip_proxy_info.user}:{ip_proxy_info.password}@{ip_proxy_info.ip}:{ip_proxy_info.port}"
        }
        return playwright_proxy, httpx_proxy

    async def launch_browser(
            self,
            chromium: BrowserType,
            playwright_proxy: Optional[Dict],
            user_agent: Optional[str],
            headless: bool = True
    ) -> BrowserContext:
        """ 
        launch browser and create browser context
        :param chromium: chromium browser
        :param playwright_proxy: playwright proxy
        :param user_agent: user agent
        :param headless: headless mode
        :return: browser context
        """
        utils.logger.info(
            "[BilibiliCrawler.launch_browser] Begin create browser context ...")
        if config.SAVE_LOGIN_STATE:
            # feat issue #14
            # we will save login state to avoid login every time
            user_data_dir = os.path.join(os.getcwd(), "browser_data",
                                         config.USER_DATA_DIR % config.PLATFORM)  # type: ignore
            browser_context = await chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                accept_downloads=True,
                headless=headless,
                proxy=playwright_proxy,  # type: ignore
                viewport={"width": 1920, "height": 1080},
                user_agent=user_agent
            )
            return browser_context
        else:
            # type: ignore
            browser = await chromium.launch(headless=headless, proxy=playwright_proxy)
            browser_context = await browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent=user_agent
            )
            return browser_context

    async def get_bilibili_video(self, video_item: Dict, semaphore: asyncio.Semaphore):
        """
        download bilibili video
        :param video_item:
        :param semaphore:
        :return:
        """
        if not config.ENABLE_GET_IMAGES:
            utils.logger.info(f"[BilibiliCrawler.get_bilibili_video] Crawling image mode is not enabled")
            return
        video_item_view: Dict = video_item.get("View")
        aid = video_item_view.get("aid")
        cid = video_item_view.get("cid")
        result = await self.get_video_play_url_task(aid, cid, semaphore)
        if result is None:
            utils.logger.info("[BilibiliCrawler.get_bilibili_video] get video play url failed")
            return
        durl_list = result.get("durl")
        max_size = -1
        video_url = ""
        for durl in durl_list:
            size = durl.get("size")
            if size > max_size:
                max_size = size
                video_url = durl.get("url")
        if video_url == "":
            utils.logger.info("[BilibiliCrawler.get_bilibili_video] get video url failed")
            return

        content = await self.bili_client.get_video_media(video_url)
        if content is None:
            return
        extension_file_name = f"video.mp4"
        await self.bilibili_store.store_video(aid, content, extension_file_name)

    async def get_creators_and_notes_from_db(self, creators: List[Dict], max_count: int = 50,
                                           keywords: str = None, account_id: str = None, session_id: str = None,
                                           login_type: str = "qrcode", get_comments: bool = False,
                                           save_data_option: str = "db", use_proxy: bool = False,
                                           proxy_strategy: str = "disabled") -> List[Dict]:
        """
        ä»æ•°æ®åº“è·å–åˆ›ä½œè€…åˆ—è¡¨è¿›è¡Œçˆ¬å–
        Args:
            creators: åˆ›ä½œè€…åˆ—è¡¨ï¼ŒåŒ…å«creator_id, platform, name, nickname
            max_count: æœ€å¤§çˆ¬å–æ•°é‡
            keywords: å…³é”®è¯ï¼ˆå¯é€‰ï¼Œç”¨äºç­›é€‰åˆ›ä½œè€…å†…å®¹ï¼‰
            account_id: è´¦å·ID
            session_id: ä¼šè¯ID
            login_type: ç™»å½•ç±»å‹
            get_comments: æ˜¯å¦è·å–è¯„è®º
            save_data_option: æ•°æ®ä¿å­˜æ–¹å¼
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
            proxy_strategy: ä»£ç†ç­–ç•¥
        Returns:
            List[Dict]: çˆ¬å–ç»“æœåˆ—è¡¨
        """
        try:
            utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] å¼€å§‹çˆ¬å– {len(creators)} ä¸ªåˆ›ä½œè€…")
            utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] æœ€å¤§æ•°é‡é™åˆ¶: {max_count}")
            utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] å…³é”®è¯: '{keywords}'")
            utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] åˆ›ä½œè€…åˆ—è¡¨: {[c.get('name', c.get('nickname', 'æœªçŸ¥')) for c in creators]}")
            
            # ç¡®ä¿å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            if not hasattr(self, 'bili_client') or self.bili_client is None:
                utils.logger.error("[BilibiliCrawler.get_creators_and_notes_from_db] bili_client æœªåˆå§‹åŒ–")
                raise Exception("Bç«™å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨start()æ–¹æ³•")
            
            all_results = []
            
            for creator in creators:
                user_id = creator.get("creator_id")
                creator_name = creator.get("name") or creator.get("nickname") or "æœªçŸ¥åˆ›ä½œè€…"
                
                utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] å¼€å§‹çˆ¬å–åˆ›ä½œè€…: {creator_name} (ID: {user_id})")
                
                try:
                    # è·å–åˆ›ä½œè€…è¯¦ç»†ä¿¡æ¯
                    creator_info: Dict = await self.bili_client.get_creator_info(int(user_id))
                    if creator_info:
                        # æ›´æ–°åˆ›ä½œè€…ä¿¡æ¯åˆ°æ•°æ®åº“
                        await self.bilibili_store.store_creator(creator_info)
                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] åˆ›ä½œè€…ä¿¡æ¯å·²æ›´æ–°: {creator_name}")
                        
                        # æ›´æ–°ä»»åŠ¡çš„creator_ref_idså­—æ®µ
                        try:
                            from api.crawler_core import update_task_creator_ref_ids
                            await update_task_creator_ref_ids(self.task_id, [str(user_id)])
                            utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] ä»»åŠ¡creator_ref_idså·²æ›´æ–°: {user_id}")
                        except Exception as e:
                            utils.logger.error(f"[BilibiliCrawler.get_creators_and_notes_from_db] æ›´æ–°ä»»åŠ¡creator_ref_idså¤±è´¥: {e}")
                    
                    # æ ¹æ®æ˜¯å¦æœ‰å…³é”®è¯é€‰æ‹©ä¸åŒçš„è·å–æ–¹å¼
                    if keywords and keywords.strip():
                        # ä½¿ç”¨å…³é”®è¯æœç´¢è·å–è§†é¢‘
                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] ä½¿ç”¨å…³é”®è¯ '{keywords}' æœç´¢åˆ›ä½œè€… {creator_name} çš„è§†é¢‘")
                        all_video_list = await self.get_creator_videos_by_keywords(int(user_id), keywords, max_count)
                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] å…³é”®è¯æœç´¢å®Œæˆï¼Œè·å–åˆ° {len(all_video_list) if all_video_list else 0} ä¸ªè§†é¢‘")
                    else:
                        # è·å–åˆ›ä½œè€…çš„æ‰€æœ‰è§†é¢‘ï¼ˆåº”ç”¨æ•°é‡é™åˆ¶ï¼‰
                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] è·å–åˆ›ä½œè€… {creator_name} çš„æ‰€æœ‰è§†é¢‘ï¼ˆæ— å…³é”®è¯ç­›é€‰ï¼‰")
                        all_video_list = await self.get_creator_videos(int(user_id), max_count)
                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] è·å–æ‰€æœ‰è§†é¢‘å®Œæˆï¼Œè·å–åˆ° {len(all_video_list) if all_video_list else 0} ä¸ªè§†é¢‘")
                    
                    if all_video_list:
                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] è·å–åˆ° {len(all_video_list)} ä¸ªè§†é¢‘")
                        
                        # å¤„ç†æ¯ä¸ªè§†é¢‘ï¼Œè·å–è¯¦ç»†ä¿¡æ¯
                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] å¼€å§‹å¤„ç† {len(all_video_list)} ä¸ªè§†é¢‘")
                        
                        for i, video_item in enumerate(all_video_list):
                            try:
                                utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] å¤„ç†ç¬¬ {i+1} ä¸ªè§†é¢‘: {video_item.get('title', 'æ— æ ‡é¢˜')}")
                                utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] è§†é¢‘ä¿¡æ¯: aid={video_item.get('aid')}, bvid={video_item.get('bvid')}")
                                
                                # è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
                                video_detail = await self.get_video_info_task(
                                    aid=video_item.get("aid", 0), 
                                    bvid=video_item.get("bvid", ""), 
                                    semaphore=asyncio.Semaphore(5)
                                )
                                
                                if video_detail:
                                    utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] æˆåŠŸè·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯")
                                    
                                    # è·å–æ’­æ”¾åœ°å€
                                    video_aid = video_detail.get("View", {}).get("aid")
                                    video_cid = video_detail.get("View", {}).get("cid")
                                    
                                    if video_aid and video_cid:
                                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] è·å–æ’­æ”¾åœ°å€: aid={video_aid}, cid={video_cid}")
                                        play_url_result = await self.get_video_play_url_task(
                                            video_aid, video_cid, asyncio.Semaphore(5)
                                        )
                                        if play_url_result:
                                            video_detail.update(play_url_result)
                                            utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] æˆåŠŸè·å–æ’­æ”¾åœ°å€")
                                        else:
                                            utils.logger.warning(f"[BilibiliCrawler.get_creators_and_notes_from_db] è·å–æ’­æ”¾åœ°å€å¤±è´¥")
                                    else:
                                        utils.logger.warning(f"[BilibiliCrawler.get_creators_and_notes_from_db] ç¼ºå°‘aidæˆ–cid: aid={video_aid}, cid={video_cid}")
                                    
                                    # ä¿å­˜åˆ°æ•°æ®åº“
                                    utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] å¼€å§‹ä¿å­˜åˆ°æ•°æ®åº“")
                                    try:
                                        await self.bilibili_store.update_bilibili_video(video_detail, task_id=self.task_id)
                                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] è§†é¢‘æ•°æ®ä¿å­˜æˆåŠŸ")
                                    except Exception as e:
                                        utils.logger.error(f"[BilibiliCrawler.get_creators_and_notes_from_db] è§†é¢‘æ•°æ®ä¿å­˜å¤±è´¥: {e}")
                                    
                                    try:
                                        await self.bilibili_store.update_up_info(video_detail)
                                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] UPä¸»ä¿¡æ¯æ›´æ–°æˆåŠŸ")
                                    except Exception as e:
                                        utils.logger.error(f"[BilibiliCrawler.get_creators_and_notes_from_db] UPä¸»ä¿¡æ¯æ›´æ–°å¤±è´¥: {e}")
                                    
                                    try:
                                        await self.get_bilibili_video(video_detail, asyncio.Semaphore(5))
                                        utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] è§†é¢‘ä¸‹è½½å¤„ç†æˆåŠŸ")
                                    except Exception as e:
                                        utils.logger.error(f"[BilibiliCrawler.get_creators_and_notes_from_db] è§†é¢‘ä¸‹è½½å¤„ç†å¤±è´¥: {e}")
                                    
                                    # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                                    all_results.append(video_detail)
                                    utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] è§†é¢‘å¤„ç†å®Œæˆï¼Œå·²æ·»åŠ åˆ°ç»“æœåˆ—è¡¨")
                                else:
                                    utils.logger.warning(f"[BilibiliCrawler.get_creators_and_notes_from_db] è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯å¤±è´¥")
                                
                            except Exception as e:
                                utils.logger.error(f"[BilibiliCrawler.get_creators_and_notes_from_db] å¤„ç†è§†é¢‘å¤±è´¥: {e}")
                                import traceback
                                utils.logger.error(f"[BilibiliCrawler.get_creators_and_notes_from_db] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                                continue
                        
                        # è·å–è¯„è®º
                        if get_comments:
                            video_ids = [video_item.get("bvid") for video_item in all_results if video_item.get("bvid")]
                            if video_ids:
                                utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] ä¸º {len(video_ids)} ä¸ªè§†é¢‘è·å–è¯„è®º")
                                await self.batch_get_video_comments(video_ids)
                    else:
                        utils.logger.warning(f"[BilibiliCrawler.get_creators_and_notes_from_db] åˆ›ä½œè€… {creator_name} æ²¡æœ‰è·å–åˆ°è§†é¢‘")
                
                except Exception as e:
                    utils.logger.error(f"[BilibiliCrawler.get_creators_and_notes_from_db] çˆ¬å–åˆ›ä½œè€… {creator_name} å¤±è´¥: {e}")
                    continue
            
            utils.logger.info(f"[BilibiliCrawler.get_creators_and_notes_from_db] çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_results)} æ¡æ•°æ® (é™åˆ¶: {max_count})")
            return all_results
            
        except Exception as e:
            utils.logger.error(f"[BilibiliCrawler.get_creators_and_notes_from_db] çˆ¬å–å¤±è´¥: {e}")
            raise

    async def get_all_creator_details(self, creator_id_list: List[int]):
        """
        creator_id_list: get details for creator from creator_id_list
        """
        utils.logger.info(
            f"[BilibiliCrawler.get_creator_details] Crawling the detalis of creator")
        utils.logger.info(
            f"[BilibiliCrawler.get_creator_details] creator ids:{creator_id_list}")

        semaphore = asyncio.Semaphore(config.MAX_CONCURRENCY_NUM)
        task_list: List[Task] = []
        try:
            for creator_id in creator_id_list:
                task = asyncio.create_task(self.get_creator_details(
                    creator_id, semaphore), name=creator_id)
                task_list.append(task)
        except Exception as e:
            utils.logger.warning(
                f"[BilibiliCrawler.get_all_creator_details] error in the task list. The creator will not be included. {e}")

        await asyncio.gather(*task_list)

    async def get_creator_details(self, creator_id: int, semaphore: asyncio.Semaphore):
        """
        get details for creator id
        :param creator_id:
        :param semaphore:
        :return:
        """
        async with semaphore:
            creator_unhandled_info: Dict = await self.bili_client.get_creator_info(creator_id)
            creator_info: Dict = {
                "id": creator_id,
                "name": creator_unhandled_info.get("name"),
                "sign": creator_unhandled_info.get("sign"),
                "avatar": creator_unhandled_info.get("face"),
            }
        await self.get_fans(creator_info, semaphore)
        await self.get_followings(creator_info, semaphore)
        await self.get_dynamics(creator_info, semaphore)

    async def get_fans(self, creator_info: Dict, semaphore: asyncio.Semaphore):
        """
        get fans for creator id
        :param creator_info:
        :param semaphore:
        :return:
        """
        creator_id = creator_info["id"]
        async with semaphore:
            try:
                utils.logger.info(
                    f"[BilibiliCrawler.get_fans] begin get creator_id: {creator_id} fans ...")
                await self.bili_client.get_creator_all_fans(
                    creator_info=creator_info,
                    crawl_interval=random.random(),
                    callback=self.bilibili_store.batch_update_bilibili_creator_fans,
                    max_count=config.CRAWLER_MAX_CONTACTS_COUNT_SINGLENOTES,
                )

            except DataFetchError as ex:
                utils.logger.error(
                    f"[BilibiliCrawler.get_fans] get creator_id: {creator_id} fans error: {ex}")
            except Exception as e:
                utils.logger.error(
                    f"[BilibiliCrawler.get_fans] may be been blocked, err:{e}")

    async def get_followings(self, creator_info: Dict, semaphore: asyncio.Semaphore):
        """
        get followings for creator id
        :param creator_info:
        :param semaphore:
        :return:
        """
        creator_id = creator_info["id"]
        async with semaphore:
            try:
                utils.logger.info(
                    f"[BilibiliCrawler.get_followings] begin get creator_id: {creator_id} followings ...")
                await self.bili_client.get_creator_all_followings(
                    creator_info=creator_info,
                    crawl_interval=random.random(),
                    callback=self.bilibili_store.batch_update_bilibili_creator_followings,
                    max_count=config.CRAWLER_MAX_CONTACTS_COUNT_SINGLENOTES,
                )

            except DataFetchError as ex:
                utils.logger.error(
                    f"[BilibiliCrawler.get_followings] get creator_id: {creator_id} followings error: {ex}")
            except Exception as e:
                utils.logger.error(
                    f"[BilibiliCrawler.get_followings] may be been blocked, err:{e}")

    async def get_dynamics(self, creator_info: Dict, semaphore: asyncio.Semaphore):
        """
        get dynamics for creator id
        :param creator_info:
        :param semaphore:
        :return:
        """
        creator_id = creator_info["id"]
        async with semaphore:
            try:
                utils.logger.info(
                    f"[BilibiliCrawler.get_dynamics] begin get creator_id: {creator_id} dynamics ...")
                await self.bili_client.get_creator_all_dynamics(
                    creator_info=creator_info,
                    crawl_interval=random.random(),
                    callback=self.bilibili_store.batch_update_bilibili_creator_dynamics,
                    max_count=config.CRAWLER_MAX_DYNAMICS_COUNT_SINGLENOTES,
                )

            except DataFetchError as ex:
                utils.logger.error(
                    f"[BilibiliCrawler.get_dynamics] get creator_id: {creator_id} dynamics error: {ex}")
            except Exception as e:
                utils.logger.error(
                    f"[BilibiliCrawler.get_dynamics] may be been blocked, err:{e}")

    async def search_by_keywords(self, keywords: str, max_count: int = 50, 
                                account_id: str = None, session_id: str = None,
                                login_type: str = "qrcode", get_comments: bool = False,
                                save_data_option: str = "db", use_proxy: bool = False,
                                proxy_strategy: str = "disabled") -> List[Dict]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢Bç«™è§†é¢‘
        :param keywords: æœç´¢å…³é”®è¯
        :param max_count: æœ€å¤§è·å–æ•°é‡
        :param account_id: è´¦å·ID
        :param session_id: ä¼šè¯ID
        :param login_type: ç™»å½•ç±»å‹
        :param get_comments: æ˜¯å¦è·å–è¯„è®º
        :param save_data_option: æ•°æ®ä¿å­˜æ–¹å¼
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        :param proxy_strategy: ä»£ç†ç­–ç•¥
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            utils.logger.info(f"[BilibiliCrawler.search_by_keywords] å¼€å§‹æœç´¢å…³é”®è¯: {keywords}")
            
            # ğŸ†• è®¾ç½®account_idåˆ°å®ä¾‹å˜é‡ï¼Œä¾›startæ–¹æ³•ä½¿ç”¨
            self.account_id = account_id
            if account_id:
                utils.logger.info(f"[BilibiliCrawler.search_by_keywords] ä½¿ç”¨æŒ‡å®šè´¦å·ID: {account_id}")
            
            # è®¾ç½®é…ç½®
            import config
            # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨åŠ¨æ€å…³é”®å­—ï¼Œå®Œå…¨å¿½ç•¥é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®å­—
            if keywords and keywords.strip():
                # å°†åŠ¨æ€å…³é”®å­—è®¾ç½®åˆ°å®ä¾‹å˜é‡ï¼Œè€Œä¸æ˜¯å…¨å±€é…ç½®
                self.dynamic_keywords = keywords
                utils.logger.info(f"[BilibiliCrawler.search_by_keywords] è®¾ç½®åŠ¨æ€å…³é”®å­—: '{keywords}'")
            else:
                utils.logger.warning("[BilibiliCrawler.search_by_keywords] å…³é”®å­—ä¸ºç©ºï¼Œå°†ä½¿ç”¨é»˜è®¤æœç´¢")
            
            config.CRAWLER_MAX_NOTES_COUNT = max_count
            config.ENABLE_GET_COMMENTS = get_comments
            config.SAVE_DATA_OPTION = save_data_option
            config.ENABLE_IP_PROXY = use_proxy
            
            # å¯åŠ¨çˆ¬è™«
            await self.start()
            
            # æ‰§è¡Œå®é™…çš„æœç´¢
            await self.search()
            
            # ä»ç»Ÿä¸€å­˜å‚¨ä¸­è·å–ç»“æœ
            results = []
            if hasattr(self, 'bilibili_store') and hasattr(self.bilibili_store, 'unified_store'):
                results = await self.bilibili_store.unified_store.get_all_content()
            
            utils.logger.info(f"[BilibiliCrawler.search_by_keywords] æœç´¢å®Œæˆï¼Œè·å– {len(results)} æ¡æ•°æ®")
            return results
            
        except Exception as e:
            utils.logger.error(f"[BilibiliCrawler.search_by_keywords] æœç´¢å¤±è´¥: {e}")
            raise
        finally:
            # å®‰å…¨å…³é—­æµè§ˆå™¨ï¼Œé¿å…é‡å¤å…³é—­
            try:
                if hasattr(self, 'browser_context') and self.browser_context:
                    await self.close()
            except Exception as e:
                utils.logger.warning(f"[BilibiliCrawler.search_by_keywords] å…³é—­æµè§ˆå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")

    async def get_user_notes(self, user_id: str, max_count: int = 50,
                            account_id: str = None, session_id: str = None,
                            login_type: str = "qrcode", get_comments: bool = False,
                            save_data_option: str = "db", use_proxy: bool = False,
                            proxy_strategy: str = "disabled") -> List[Dict]:
        """
        è·å–ç”¨æˆ·å‘å¸ƒçš„è§†é¢‘
        :param user_id: ç”¨æˆ·ID
        :param max_count: æœ€å¤§è·å–æ•°é‡
        :param account_id: è´¦å·ID
        :param session_id: ä¼šè¯ID
        :param login_type: ç™»å½•ç±»å‹
        :param get_comments: æ˜¯å¦è·å–è¯„è®º
        :param save_data_option: æ•°æ®ä¿å­˜æ–¹å¼
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        :param proxy_strategy: ä»£ç†ç­–ç•¥
        :return: è§†é¢‘åˆ—è¡¨
        """
        try:
            utils.logger.info(f"[BilibiliCrawler.get_user_notes] å¼€å§‹è·å–ç”¨æˆ·è§†é¢‘: {user_id}")
            
            # è®¾ç½®é…ç½®
            import config
            # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨åŠ¨æ€ç”¨æˆ·IDï¼Œè€Œä¸æ˜¯ä¿®æ”¹å…¨å±€é…ç½®
            self.dynamic_video_ids = [user_id]
            utils.logger.info(f"[BilibiliCrawler.get_user_notes] è®¾ç½®åŠ¨æ€ç”¨æˆ·ID: {user_id}")
            
            config.CRAWLER_MAX_NOTES_COUNT = max_count
            config.ENABLE_GET_COMMENTS = get_comments
            config.SAVE_DATA_OPTION = save_data_option
            config.ENABLE_IP_PROXY = use_proxy
            
            # å¯åŠ¨çˆ¬è™«
            await self.start()
            
            # è·å–å­˜å‚¨çš„æ•°æ®
            results = []
            if hasattr(self, 'bilibili_store') and hasattr(self.bilibili_store, 'get_all_content'):
                results = await self.bilibili_store.get_all_content()
            
            utils.logger.info(f"[BilibiliCrawler.get_user_notes] è·å–å®Œæˆï¼Œå…± {len(results)} æ¡æ•°æ®")
            return results
            
        except Exception as e:
            utils.logger.error(f"[BilibiliCrawler.get_user_notes] è·å–å¤±è´¥: {e}")
            raise
        finally:
            # å®‰å…¨å…³é—­æµè§ˆå™¨ï¼Œé¿å…é‡å¤å…³é—­
            try:
                if hasattr(self, 'browser_context') and self.browser_context:
                    await self.close()
            except Exception as e:
                utils.logger.warning(f"[BilibiliCrawler.get_user_notes] å…³é—­æµè§ˆå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")

    async def close(self):
        """
        å®‰å…¨å…³é—­æµè§ˆå™¨å’Œç›¸å…³èµ„æº
        """
        try:
            if hasattr(self, 'browser_context') and self.browser_context:
                await self.browser_context.close()
                utils.logger.info("[BilibiliCrawler] æµè§ˆå™¨ä¸Šä¸‹æ–‡å·²å…³é—­")
            
            if hasattr(self, 'context_page') and self.context_page:
                await self.context_page.close()
                utils.logger.info("[BilibiliCrawler] é¡µé¢å·²å…³é—­")
            
            if hasattr(self, 'playwright') and self.playwright:
                await self.playwright.stop()
                utils.logger.info("[BilibiliCrawler] Playwrightå®ä¾‹å·²å…³é—­")
                
        except Exception as e:
            utils.logger.warning(f"[BilibiliCrawler.close] å…³é—­èµ„æºæ—¶å‡ºç°è­¦å‘Š: {e}")
