# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚


import asyncio
import os
import random
import time
from asyncio import Task
from typing import Dict, List, Optional, Tuple

from playwright.async_api import BrowserContext, BrowserType, Page, async_playwright

import config
from base.base_crawler import AbstractCrawler
from proxy.proxy_ip_pool import IpInfoModel, create_ip_pool
from store import kuaishou as kuaishou_store
from tools import utils
from var import comment_tasks_var, crawler_type_var, source_keyword_var

from .client import KuaiShouClient
from .exception import DataFetchError, FrequencyLimitError, IPBlockError
from .login import KuaishouLogin
from utils.db_utils import get_cookies_from_database


class KuaishouCrawler(AbstractCrawler):
    context_page: Page
    ks_client: KuaiShouClient
    browser_context: BrowserContext

    def __init__(self, task_id: str = None):
        self.index_url = "https://www.kuaishou.com"
        # ä½¿ç”¨å­˜å‚¨å·¥å‚åˆ›å»ºå­˜å‚¨å¯¹è±¡
        from store.kuaishou import KuaishouStoreFactory
        self.kuaishou_store = KuaishouStoreFactory.create_store()
        self.user_agent = utils.get_user_agent()
        self.task_id = task_id

    async def start(self, start_page: int = 1) -> None:
        playwright_proxy_format, httpx_proxy_format = None, None
        if config.ENABLE_IP_PROXY:
            ip_proxy_pool = await create_ip_pool(config.IP_PROXY_POOL_COUNT, enable_validate_ip=True)
            ip_proxy_info: IpInfoModel = await ip_proxy_pool.get_proxy()
            playwright_proxy_format, httpx_proxy_format = self.format_proxy_info(ip_proxy_info)

        async with async_playwright() as playwright:
            # Launch a browser context.
            chromium = playwright.chromium
            self.browser_context = await self.launch_browser(
                chromium,
                None,
                user_agent=None,
                headless=config.HEADLESS
            )
            # stealth.min.js is a js script to prevent the website from detecting the crawler.
            await self.browser_context.add_init_script(path="libs/stealth.min.js")
            self.context_page = await self.browser_context.new_page()
            await self.context_page.goto(self.index_url)

            # æ·»åŠ æ–¹æ³•å­˜åœ¨æ€§æ£€æŸ¥
            if not hasattr(self, 'create_ks_client'):
                raise AttributeError("KuaishouCrawler ç¼ºå°‘ create_ks_client æ–¹æ³•")
            
            self.ks_client = await self.create_ks_client(httpx_proxy_format)
            
            # ğŸ†• ç®€åŒ–ï¼šç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­çš„tokenï¼Œæ— éœ€å¤æ‚ç™»å½•æµç¨‹
            utils.logger.debug("[KuaishouCrawler] å¼€å§‹ä½¿ç”¨æ•°æ®åº“ä¸­çš„ç™»å½•å‡­è¯...")
            
            utils.logger.info("[KuaishouCrawler.start] çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼Œæµè§ˆå™¨ä¸Šä¸‹æ–‡å·²åˆ›å»º")
            
    async def _init_crawler_only(self) -> None:
        """
        ä»…åˆå§‹åŒ–çˆ¬è™«ï¼ˆåˆ›å»ºå®¢æˆ·ç«¯ç­‰ï¼‰ï¼Œä½†ä¸æ‰§è¡Œstart()ä¸­çš„çˆ¬å–é€»è¾‘
        ç”¨äºåˆ›ä½œè€…æ¨¡å¼ï¼Œé¿å…é‡å¤æ‰§è¡Œçˆ¬å–é€»è¾‘
        """
        try:
            utils.logger.info("[KuaishouCrawler._init_crawler_only] å¼€å§‹åˆå§‹åŒ–çˆ¬è™«ï¼ˆä»…åˆå§‹åŒ–æ¨¡å¼ï¼‰")
            
            # åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
            await self._create_browser_context()
            
            # åˆå§‹åŒ–ç™»å½•å‡­è¯
            utils.logger.info("[KuaishouCrawler._init_crawler_only] å¼€å§‹ä½¿ç”¨æ•°æ®åº“ä¸­çš„ç™»å½•å‡­è¯...")
            
            # ä»ä¼ å…¥çš„å‚æ•°ä¸­è·å–account_id
            account_id = getattr(self, 'account_id', None)
            if account_id:
                utils.logger.info(f"[KuaishouCrawler._init_crawler_only] ä½¿ç”¨æŒ‡å®šè´¦å·: {account_id}")
            else:
                utils.logger.info(f"[KuaishouCrawler._init_crawler_only] ä½¿ç”¨é»˜è®¤è´¦å·ï¼ˆæœ€æ–°ç™»å½•ï¼‰")
            
            # ä»æ•°æ®åº“è·å–cookies
            cookie_str = await get_cookies_from_database("ks", account_id)
            
            if cookie_str:
                utils.logger.info("[KuaishouCrawler._init_crawler_only] å‘ç°æ•°æ®åº“ä¸­çš„cookiesï¼Œç›´æ¥ä½¿ç”¨...")
                try:
                    # è®¾ç½®cookiesåˆ°æµè§ˆå™¨
                    await self.ks_client.set_cookies_from_string(cookie_str)
                    utils.logger.info("[KuaishouCrawler._init_crawler_only] âœ… è·³è¿‡cookieséªŒè¯ï¼Œç›´æ¥ä½¿ç”¨æ•°æ®åº“ä¸­çš„cookies")
                except Exception as e:
                    utils.logger.error(f"[KuaishouCrawler._init_crawler_only] ä½¿ç”¨æ•°æ®åº“cookieså¤±è´¥: {e}")
                    raise Exception(f"ä½¿ç”¨æ•°æ®åº“ç™»å½•å‡­è¯å¤±è´¥: {str(e)}")
            else:
                utils.logger.error("[KuaishouCrawler._init_crawler_only] âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç™»å½•å‡­è¯")
                raise Exception("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç™»å½•å‡­è¯ï¼Œè¯·å…ˆç™»å½•")
            
            utils.logger.info("[KuaishouCrawler._init_crawler_only] âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆï¼ˆä»…åˆå§‹åŒ–æ¨¡å¼ï¼‰")
            
        except Exception as e:
            utils.logger.error(f"[KuaishouCrawler._init_crawler_only] åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _create_browser_context(self) -> None:
        """
        åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡
        """
        try:
            utils.logger.info("[KuaishouCrawler._create_browser_context] å¼€å§‹åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡")
            
            playwright_proxy_format, httpx_proxy_format = None, None
            if config.ENABLE_IP_PROXY:
                ip_proxy_pool = await create_ip_pool(config.IP_PROXY_POOL_COUNT, enable_validate_ip=True)
                ip_proxy_info: IpInfoModel = await ip_proxy_pool.get_proxy()
                playwright_proxy_format, httpx_proxy_format = self.format_proxy_info(ip_proxy_info)

            # åˆ›å»ºplaywrightå®ä¾‹
            self.playwright = await async_playwright().start()
            
            # Launch a browser context.
            chromium = self.playwright.chromium
            self.browser_context = await self.launch_browser(
                chromium,
                None,
                user_agent=None,
                headless=config.HEADLESS
            )
            
            # stealth.min.js is a js script to prevent the website from detecting the crawler.
            await self.browser_context.add_init_script(path="libs/stealth.min.js")
            
            self.context_page = await self.browser_context.new_page()
            await self.context_page.goto(self.index_url)

            # æ·»åŠ æ–¹æ³•å­˜åœ¨æ€§æ£€æŸ¥
            if not hasattr(self, 'create_ks_client'):
                raise AttributeError("KuaishouCrawler ç¼ºå°‘ create_ks_client æ–¹æ³•")
            
            self.ks_client = await self.create_ks_client(httpx_proxy_format)
            
            utils.logger.info("[KuaishouCrawler._create_browser_context] âœ… æµè§ˆå™¨ä¸Šä¸‹æ–‡åˆ›å»ºå®Œæˆ")
            
        except Exception as e:
            utils.logger.error(f"[KuaishouCrawler._create_browser_context] åˆ›å»ºæµè§ˆå™¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            raise
            
            # ä»ä¼ å…¥çš„å‚æ•°ä¸­è·å–account_id
            account_id = getattr(self, 'account_id', None)
            if account_id:
                utils.logger.debug(f"[KuaishouCrawler] ä½¿ç”¨æŒ‡å®šè´¦å·: {account_id}")
            else:
                utils.logger.debug(f"[KuaishouCrawler] ä½¿ç”¨é»˜è®¤è´¦å·ï¼ˆæœ€æ–°ç™»å½•ï¼‰")
            
            # ä»æ•°æ®åº“è·å–cookies
            cookie_str = await get_cookies_from_database("ks", account_id)
            
            if cookie_str:
                utils.logger.debug("[KuaishouCrawler] å‘ç°æ•°æ®åº“ä¸­çš„cookiesï¼Œç›´æ¥ä½¿ç”¨...")
                try:
                    # è®¾ç½®cookiesåˆ°æµè§ˆå™¨
                    await self.ks_client.set_cookies_from_string(cookie_str)
                    
                    # éªŒè¯cookiesæ˜¯å¦æœ‰æ•ˆ
                    # if await self.ks_client.pong():
                    #     utils.logger.info("[KuaishouCrawler] âœ… æ•°æ®åº“ä¸­çš„cookiesæœ‰æ•ˆï¼Œå¼€å§‹çˆ¬å–")
                    #     # æ›´æ–°cookiesåˆ°å®¢æˆ·ç«¯
                    #     await self.ks_client.update_cookies(browser_context=self.browser_context)
                    # else:
                    #     utils.logger.error("[KuaishouCrawler] âŒ æ•°æ®åº“ä¸­çš„cookiesæ— æ•ˆï¼Œæ— æ³•ç»§ç»­")
                    #     raise Exception("æ•°æ®åº“ä¸­çš„ç™»å½•å‡­è¯æ— æ•ˆï¼Œè¯·é‡æ–°ç™»å½•")
                except Exception as e:
                    utils.logger.error(f"[KuaishouCrawler] ä½¿ç”¨æ•°æ®åº“cookieså¤±è´¥: {e}")
                    raise Exception(f"ä½¿ç”¨æ•°æ®åº“ç™»å½•å‡­è¯å¤±è´¥: {str(e)}")
            else:
                utils.logger.error("[KuaishouCrawler] âŒ æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç™»å½•å‡­è¯")
                raise Exception("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç™»å½•å‡­è¯ï¼Œè¯·å…ˆç™»å½•")
            
            # ğŸ†• ä¿®å¤ï¼šæ ¹æ®åŠ¨æ€å‚æ•°å†³å®šæ‰§è¡Œé€»è¾‘ï¼Œè€Œä¸æ˜¯ä¾èµ–é…ç½®æ–‡ä»¶
            crawler_type_var.set(config.CRAWLER_TYPE)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åŠ¨æ€å…³é”®å­—ï¼Œå¦‚æœæœ‰åˆ™æ‰§è¡Œæœç´¢
            if hasattr(self, 'dynamic_keywords') and self.dynamic_keywords:
                utils.logger.debug(f"[KuaishouCrawler.start] æ£€æµ‹åˆ°åŠ¨æ€å…³é”®å­—: {self.dynamic_keywords}")
                utils.logger.debug(f"[KuaishouCrawler.start] æ‰§è¡Œå…³é”®è¯æœç´¢æ¨¡å¼")
                await self.search(start_page=start_page)
            elif hasattr(self, 'dynamic_video_ids') and self.dynamic_video_ids:
                utils.logger.debug(f"[KuaishouCrawler.start] æ£€æµ‹åˆ°åŠ¨æ€è§†é¢‘ID: {self.dynamic_video_ids}")
                utils.logger.debug(f"[KuaishouCrawler.start] æ‰§è¡ŒæŒ‡å®šè§†é¢‘æ¨¡å¼")
                await self.get_specified_videos()
            elif hasattr(self, 'dynamic_creators') and self.dynamic_creators:
                utils.logger.debug(f"[KuaishouCrawler.start] æ£€æµ‹åˆ°åŠ¨æ€åˆ›ä½œè€…: {self.dynamic_creators}")
                utils.logger.debug(f"[KuaishouCrawler.start] æ‰§è¡Œåˆ›ä½œè€…æ¨¡å¼")
                await self.get_creators_and_notes()
            else:
                # å¦‚æœæ²¡æœ‰åŠ¨æ€å‚æ•°ï¼Œåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
                utils.logger.debug(f"[KuaishouCrawler.start] ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„çˆ¬å–ç±»å‹: {config.CRAWLER_TYPE}")
                if config.CRAWLER_TYPE == "search":
                    # Search for notes and retrieve their comment information.
                    await self.search(start_page=start_page)
                elif config.CRAWLER_TYPE == "detail":
                    # Get the information and comments of the specified post
                    await self.get_specified_notes()
                elif config.CRAWLER_TYPE == "creator":
                    # Get the information and comments of the specified creator
                    await self.get_creators_and_notes()

            utils.logger.debug("[KuaishouCrawler.start] Kuaishou Crawler finished ...")

    async def search(self, start_page: int = 1):
        utils.logger.debug("[KuaishouCrawler.search] Begin search kuaishou keywords")
        ks_limit_count = 20  # kuaishou limit page fixed value
        # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨å®ä¾‹å˜é‡æ›¿ä»£config.CRAWLER_MAX_NOTES_COUNT
        max_notes_count = getattr(self, 'max_notes_count', 20)
        if max_notes_count < ks_limit_count:
            max_notes_count = ks_limit_count
        
        # æ·»åŠ èµ„æºç›‘æ§
        start_time = time.time()
        processed_count = 0
        
        # ğŸ†• ä¿®å¤ï¼šå®Œå…¨å¿½ç•¥é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®å­—ï¼Œåªä½¿ç”¨åŠ¨æ€ä¼ å…¥çš„å…³é”®å­—
        # ä»å®ä¾‹å˜é‡è·å–å…³é”®å­—
        keywords_to_search = getattr(self, 'dynamic_keywords', None)
        if not keywords_to_search:
            utils.logger.error("[KuaishouCrawler.search] æ²¡æœ‰æ‰¾åˆ°åŠ¨æ€å…³é”®å­—ï¼Œæ— æ³•è¿›è¡Œæœç´¢")
            utils.logger.error("[KuaishouCrawler.search] è¯·ç¡®ä¿åœ¨è°ƒç”¨searchæ–¹æ³•å‰è®¾ç½®äº†dynamic_keywords")
            return
        
        # ç¡®ä¿å…³é”®å­—ä¸ä¸ºç©º
        if not keywords_to_search.strip():
            utils.logger.error("[KuaishouCrawler.search] å…³é”®å­—ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæœç´¢")
            return
        
        # å¤„ç†å¤šä¸ªå…³é”®å­—ï¼ˆç”¨é€—å·åˆ†éš”ï¼‰
        keyword_list = [kw.strip() for kw in keywords_to_search.split(",") if kw.strip()]
        
        for keyword in keyword_list:
            search_session_id = ""
            source_keyword_var.set(keyword)
            utils.logger.debug(
                f"[KuaishouCrawler.search] Current search keyword: {keyword}"
            )
            page = 1
            
            # ğŸ†• æ·»åŠ é‡è¯•æ¬¡æ•°é™åˆ¶
            max_retries = 3
            retry_count = 0
            
            while (
                page - start_page + 1
            ) * ks_limit_count <= max_notes_count:
                if page < start_page:
                    utils.logger.debug(f"[KuaishouCrawler.search] Skip page: {page}")
                    page += 1
                    continue
                
                try:
                    utils.logger.debug(
                        f"[KuaishouCrawler.search] search kuaishou keyword: {keyword}, page: {page}"
                    )
                    video_id_list: List[str] = []
                    videos_res = await self.ks_client.search_info_by_keyword(
                        keyword=keyword,
                        pcursor=str(page),
                        search_session_id=search_session_id,
                    )
                    
                    utils.logger.info(f"[KuaishouCrawler.search] æœç´¢APIåŸå§‹è¿”å›: {videos_res}")
                    
                    if not videos_res:
                        utils.logger.error(
                            f"[KuaishouCrawler.search] search info by keyword:{keyword} not found data"
                        )
                        continue

                    vision_search_photo: Dict = videos_res.get("visionSearchPhoto")
                    utils.logger.info(f"[KuaishouCrawler.search] visionSearchPhoto: {vision_search_photo}")
                    
                    if not vision_search_photo:
                        utils.logger.error(f"[KuaishouCrawler.search] visionSearchPhoto ä¸ºç©º")
                        continue
                        
                    if vision_search_photo.get("result") != 1:
                        result_code = vision_search_photo.get("result")
                        utils.logger.error(
                            f"[KuaishouCrawler.search] search info by keyword:{keyword} result != 1, result: {result_code}"
                        )
                        
                        # ğŸ†• æ£€æµ‹åçˆ¬è™«æœºåˆ¶
                        if result_code == 400002:
                            utils.logger.error("ğŸš¨ æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶ï¼šéœ€è¦éªŒè¯ç éªŒè¯")
                            raise Exception("åçˆ¬è™«æœºåˆ¶è§¦å‘ï¼šéœ€è¦éªŒè¯ç éªŒè¯ï¼Œè¯·é‡æ–°ç™»å½•æˆ–ç¨åé‡è¯•")
                        elif result_code == 429:
                            utils.logger.error("ğŸš¨ æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶ï¼šè¯·æ±‚è¿‡äºé¢‘ç¹")
                            raise Exception("åçˆ¬è™«æœºåˆ¶è§¦å‘ï¼šè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•")
                        elif result_code == 403:
                            utils.logger.error("ğŸš¨ æ£€æµ‹åˆ°åçˆ¬è™«æœºåˆ¶ï¼šè®¿é—®è¢«ç¦æ­¢")
                            raise Exception("åçˆ¬è™«æœºåˆ¶è§¦å‘ï¼šè®¿é—®è¢«ç¦æ­¢")
                        else:
                            utils.logger.error(f"ğŸš¨ æœªçŸ¥é”™è¯¯ç : {result_code}")
                        
                        continue
                    search_session_id = vision_search_photo.get("searchSessionId", "")
                    
                    # åˆ†æ‰¹å¤„ç†è§†é¢‘è¯¦æƒ…
                    feeds = vision_search_photo.get("feeds", [])
                    batch_size = 5  # æ¯æ‰¹å¤„ç†5ä¸ªè§†é¢‘
                    
                    for i in range(0, len(feeds), batch_size):
                        batch_feeds = feeds[i:i + batch_size]
                        utils.logger.debug(f"[KuaishouCrawler.search] Processing video batch {i//batch_size + 1}, items: {len(batch_feeds)}")
                        
                        for video_detail in batch_feeds:
                            try:
                                # utils.logger.debug(f"[KuaishouCrawler] åŸå§‹è§†é¢‘æ•°æ®: {json.dumps(video_detail, ensure_ascii=False)}")
                                video_id_list.append(video_detail.get("photo", {}).get("id"))
                                await self.kuaishou_store.update_kuaishou_video(video_item=video_detail, task_id=self.task_id)
                                processed_count += 1
                            except Exception as e:
                                utils.logger.error(f"[KuaishouCrawler.search] Failed to process video: {e}")
                                continue
                        
                        # æ·»åŠ é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                        await asyncio.sleep(1)
                    
                    # æ£€æŸ¥å¤„ç†æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´è¿è¡Œ
                    elapsed_time = time.time() - start_time
                    if elapsed_time > 300:  # 5åˆ†é’Ÿè¶…æ—¶
                        utils.logger.warning(f"[KuaishouCrawler.search] Processing time exceeded 5 minutes, stopping")
                        break
                    
                    # è·å–è¯„è®ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨å®ä¾‹å˜é‡æ›¿ä»£config.ENABLE_GET_COMMENTS
                    get_comments = getattr(self, 'get_comments', False)
                    if get_comments and video_id_list:
                        try:
                            await self.batch_get_video_comments(video_id_list)
                        except Exception as e:
                            utils.logger.error(f"[KuaishouCrawler.search] Failed to get comments: {e}")
                    
                    page += 1
                    
                except Exception as e:
                    utils.logger.error(
                        f"[KuaishouCrawler.search] Unexpected error during search: {e}"
                    )
                    page += 1
                    continue
            
            utils.logger.debug(f"[KuaishouCrawler.search] Search completed. Total processed: {processed_count}")

    async def get_specified_videos(self):
        """Get the information and comments of the specified post"""
        # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨åŠ¨æ€ä¼ å…¥çš„è§†é¢‘IDåˆ—è¡¨ï¼Œè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„é™æ€åˆ—è¡¨
        video_id_list = getattr(self, 'dynamic_video_ids', None)
        if not video_id_list:
            utils.logger.warning("[KuaishouCrawler.get_specified_videos] æœªæ‰¾åˆ°åŠ¨æ€è§†é¢‘IDåˆ—è¡¨ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„åˆ—è¡¨ï¼ˆå‘åå…¼å®¹ï¼‰")
            video_id_list = config.KS_SPECIFIED_ID_LIST
        
        if not video_id_list:
            utils.logger.error("[KuaishouCrawler.get_specified_videos] æ²¡æœ‰æœ‰æ•ˆçš„è§†é¢‘IDåˆ—è¡¨ï¼Œæ— æ³•è·å–è§†é¢‘")
            return
        
        utils.logger.debug(f"[KuaishouCrawler.get_specified_videos] å¼€å§‹è·å– {len(video_id_list)} ä¸ªæŒ‡å®šè§†é¢‘")
        
        semaphore = asyncio.Semaphore(config.MAX_CONCURRENCY_NUM)
        task_list = [
            self.get_video_info_task(video_id=video_id, semaphore=semaphore)
            for video_id in video_id_list
        ]
        video_details = await asyncio.gather(*task_list)
        for video_detail in video_details:
            if video_detail is not None:
                await self.kuaishou_store.update_kuaishou_video(video_detail, task_id=self.task_id)
        await self.batch_get_video_comments(video_id_list)

    async def get_video_info_task(
        self, video_id: str, semaphore: asyncio.Semaphore
    ) -> Optional[Dict]:
        """Get video detail task"""
        async with semaphore:
            try:
                result = await self.ks_client.get_video_info(video_id)
                utils.logger.debug(
                    f"[KuaishouCrawler.get_video_info_task] Get video_id:{video_id} info result: {result} ..."
                )
                return result.get("visionVideoDetail")
            except FrequencyLimitError as ex:
                retry_count += 1
                utils.logger.error(f"[KuaishouCrawler.get_video_info_task] è®¿é—®é¢‘æ¬¡å¼‚å¸¸ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´: {ex} (é‡è¯• {retry_count}/{max_retries})")
                
                if retry_count >= max_retries:
                    utils.logger.error(f"[KuaishouCrawler.get_video_info_task] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œç»ˆæ­¢è§†é¢‘è¯¦æƒ…è·å–")
                    return None
                
                # é¢‘ç‡é™åˆ¶é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•
                await asyncio.sleep(30)  # ç­‰å¾…30ç§’
                return None
            except DataFetchError as ex:
                utils.logger.error(
                    f"[KuaishouCrawler.get_video_info_task] Get video detail error: {ex}"
                )
                return None
            except KeyError as ex:
                utils.logger.error(
                    f"[KuaishouCrawler.get_video_info_task] have not fund video detail video_id:{video_id}, err: {ex}"
                )
                return None

    async def batch_get_video_comments(self, video_id_list: List[str]):
        """
        batch get video comments
        :param video_id_list:
        :return:
        """
        if not config.ENABLE_GET_COMMENTS:
            utils.logger.debug(
                f"[KuaishouCrawler.batch_get_video_comments] Crawling comment mode is not enabled"
            )
            return

        utils.logger.debug(
            f"[KuaishouCrawler.batch_get_video_comments] video ids:{video_id_list}"
        )
        
        # é™åˆ¶å¹¶å‘æ•°é‡
        max_concurrent = min(config.MAX_CONCURRENCY_NUM, len(video_id_list))
        semaphore = asyncio.Semaphore(max_concurrent)
        
        # åˆ†æ‰¹å¤„ç†è¯„è®º
        batch_size = 3  # æ¯æ‰¹å¤„ç†3ä¸ªè¯„è®ºä»»åŠ¡
        total_processed = 0
        
        for i in range(0, len(video_id_list), batch_size):
            batch_videos = video_id_list[i:i + batch_size]
            
            utils.logger.debug(f"[KuaishouCrawler.batch_get_video_comments] Processing comment batch {i//batch_size + 1}, videos: {len(batch_videos)}")
            
            task_list: List[Task] = []
            for video_id in batch_videos:
                task = asyncio.create_task(
                    self.get_comments(video_id, semaphore), name=video_id
                )
                task_list.append(task)

            try:
                # æ·»åŠ è¶…æ—¶æ§åˆ¶
                await asyncio.wait_for(
                    asyncio.gather(*task_list, return_exceptions=True),
                    timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
                )
                total_processed += len(batch_videos)
                utils.logger.debug(f"[KuaishouCrawler.batch_get_video_comments] Completed batch {i//batch_size + 1}")
            except asyncio.TimeoutError:
                utils.logger.warning(f"[KuaishouCrawler.batch_get_video_comments] Comment batch timeout")
                break
            except Exception as e:
                utils.logger.error(f"[KuaishouCrawler.batch_get_video_comments] Comment batch error: {e}")
                continue
            
            # æ·»åŠ é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            await asyncio.sleep(2)
        
        utils.logger.debug(f"[KuaishouCrawler.batch_get_video_comments] Comment processing completed. Total processed: {total_processed}")

    async def get_comments(self, video_id: str, semaphore: asyncio.Semaphore):
        """
        get comment for video id
        :param video_id:
        :param semaphore:
        :return:
        """
        async with semaphore:
            try:
                utils.logger.debug(
                    f"[KuaishouCrawler.get_comments] begin get video_id: {video_id} comments ..."
                )
                await self.ks_client.get_video_all_comments(
                    photo_id=video_id,
                    crawl_interval=random.random(),
                    callback=self.kuaishou_store.batch_update_ks_video_comments,
                    max_count=config.CRAWLER_MAX_COMMENTS_COUNT_SINGLENOTES,
                )
            except FrequencyLimitError as ex:
                retry_count += 1
                utils.logger.error(f"[KuaishouCrawler.get_comments] è®¿é—®é¢‘æ¬¡å¼‚å¸¸ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´: {ex} (é‡è¯• {retry_count}/{max_retries})")
                
                if retry_count >= max_retries:
                    utils.logger.error(f"[KuaishouCrawler.get_comments] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œç»ˆæ­¢è¯„è®ºè·å–")
                    return
                
                # é¢‘ç‡é™åˆ¶é”™è¯¯ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´åé‡è¯•
                await asyncio.sleep(30)  # ç­‰å¾…30ç§’
            except DataFetchError as ex:
                retry_count += 1
                utils.logger.error(f"[KuaishouCrawler.get_comments] get video_id: {video_id} comment error: {ex} (é‡è¯• {retry_count}/{max_retries})")
                
                if retry_count >= max_retries:
                    utils.logger.error(f"[KuaishouCrawler.get_comments] è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}ï¼Œç»ˆæ­¢è¯„è®ºè·å–")
                    return
            except Exception as e:
                utils.logger.error(
                    f"[KuaishouCrawler.get_comments] may be been blocked, err:{e}"
                )
                # use time.sleeep block main coroutine instead of asyncio.sleep and cacel running comment task
                # maybe kuaishou block our request, we will take a nap and update the cookie again
                current_running_tasks = comment_tasks_var.get()
                for task in current_running_tasks:
                    task.cancel()
                time.sleep(20)
                await self.context_page.goto(f"{self.index_url}?isHome=1")
                await self.ks_client.update_cookies(
                    browser_context=self.browser_context
                )

    @staticmethod
    def format_proxy_info(
        ip_proxy_info: IpInfoModel,
    ) -> Tuple[Optional[Dict], Optional[Dict]]:
        """format proxy info for playwright and httpx"""
        playwright_proxy = {
            "server": f"{ip_proxy_info.protocol}{ip_proxy_info.ip}:{ip_proxy_info.port}",
            "username": ip_proxy_info.user,
            "password": ip_proxy_info.password,
        }
        httpx_proxy = {
            f"{ip_proxy_info.protocol}": f"http://{ip_proxy_info.user}:{ip_proxy_info.password}@{ip_proxy_info.ip}:{ip_proxy_info.port}"
        }
        return playwright_proxy, httpx_proxy

    async def create_ks_client(self, httpx_proxy: Optional[str]) -> KuaiShouClient:
        """Create ks client"""
        utils.logger.debug(
            "[KuaishouCrawler.create_ks_client] Begin create kuaishou API client ..."
        )
        cookie_str, cookie_dict = utils.convert_cookies(
            await self.browser_context.cookies()
        )
        ks_client_obj = KuaiShouClient(
            proxies=httpx_proxy,
            headers={
                "User-Agent": self.user_agent,
                "Cookie": cookie_str,
                "Origin": self.index_url,
                "Referer": self.index_url,
                "Content-Type": "application/json;charset=UTF-8",
            },
            playwright_page=self.context_page,
            cookie_dict=cookie_dict,
        )
        return ks_client_obj

    async def launch_browser(
        self,
        chromium: BrowserType,
        playwright_proxy: Optional[Dict],
        user_agent: Optional[str],
        headless: bool = True,
    ) -> BrowserContext:
        """Launch browser and create browser context"""
        utils.logger.debug(
            "[KuaishouCrawler.launch_browser] Begin create browser context ..."
        )
        if config.SAVE_LOGIN_STATE:
            user_data_dir = os.path.join(
                os.getcwd(), "browser_data", config.USER_DATA_DIR % config.PLATFORM
            )  # type: ignore
            browser_context = await chromium.launch_persistent_context(
                user_data_dir=user_data_dir,
                accept_downloads=True,
                headless=headless,
                proxy=playwright_proxy,  # type: ignore
                viewport={"width": 1920, "height": 1080},
                user_agent=user_agent,
            )
            return browser_context
        else:
            browser = await chromium.launch(headless=headless, proxy=playwright_proxy)  # type: ignore
            browser_context = await browser.new_context(
                viewport={"width": 1920, "height": 1080}, user_agent=user_agent
            )
            return browser_context

    async def get_creators_and_notes(self) -> None:
        """Get creator's videos and retrieve their comment information."""
        utils.logger.debug(
            "[KuaiShouCrawler.get_creators_and_notes] Begin get kuaishou creators"
        )
        
        # ğŸ†• ä¿®å¤ï¼šåœ¨ä»»åŠ¡æ¨¡å¼ä¸‹ï¼Œä¸ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„åˆ›ä½œè€…IDåˆ—è¡¨
        if hasattr(self, 'task_id') and self.task_id:
            utils.logger.info("[KuaiShouCrawler.get_creators_and_notes] ä»»åŠ¡æ¨¡å¼ï¼Œè·³è¿‡æ­¤æ–¹æ³•ï¼Œç”±get_creators_and_notes_from_dbå¤„ç†")
            return
        
        # ä»é…ç½®ä¸­è·å–åˆ›ä½œè€…IDåˆ—è¡¨ï¼ˆä»…ç”¨äºéä»»åŠ¡æ¨¡å¼ï¼‰
        if not hasattr(config, 'KS_CREATOR_ID_LIST') or not config.KS_CREATOR_ID_LIST:
            utils.logger.warning("[KuaiShouCrawler.get_creators_and_notes] æ²¡æœ‰é…ç½®åˆ›ä½œè€…IDåˆ—è¡¨ï¼Œæ— æ³•è¿›è¡Œåˆ›ä½œè€…çˆ¬å–")
            return
        
        for user_id in config.KS_CREATOR_ID_LIST:
            utils.logger.info(f"[KuaiShouCrawler.get_creators_and_notes] å¼€å§‹çˆ¬å–åˆ›ä½œè€…: {user_id}")
            
            # get creator detail info from web html content
            createor_info: Dict = await self.ks_client.get_creator_info(user_id=user_id)
            if createor_info:
                await self.kuaishou_store.store_creator(createor_info)

            # Get all video information of the creator
            all_video_list = await self.ks_client.get_all_videos_by_creator(
                user_id=user_id,
                crawl_interval=random.random(),
                callback=self.fetch_creator_video_detail,
            )

            video_ids = [
                video_item.get("photo", {}).get("id") for video_item in all_video_list
            ]
            await self.batch_get_video_comments(video_ids)

    async def get_creators_and_videos(self) -> None:
        """Get creator's videos and retrieve their comment information."""
        utils.logger.debug(
            "[KuaiShouCrawler.get_creators_and_videos] Begin get kuaishou creators"
        )
        
        # ğŸ†• ä¿®å¤ï¼šåœ¨ä»»åŠ¡æ¨¡å¼ä¸‹ï¼Œä¸ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„åˆ›ä½œè€…IDåˆ—è¡¨
        if hasattr(self, 'task_id') and self.task_id:
            utils.logger.info("[KuaiShouCrawler.get_creators_and_videos] ä»»åŠ¡æ¨¡å¼ï¼Œè·³è¿‡æ­¤æ–¹æ³•ï¼Œç”±get_creators_and_notes_from_dbå¤„ç†")
            return
        
        # ä»é…ç½®ä¸­è·å–åˆ›ä½œè€…IDåˆ—è¡¨ï¼ˆä»…ç”¨äºéä»»åŠ¡æ¨¡å¼ï¼‰
        if not hasattr(config, 'KS_CREATOR_ID_LIST') or not config.KS_CREATOR_ID_LIST:
            utils.logger.warning("[KuaiShouCrawler.get_creators_and_videos] æ²¡æœ‰é…ç½®åˆ›ä½œè€…IDåˆ—è¡¨ï¼Œæ— æ³•è¿›è¡Œåˆ›ä½œè€…çˆ¬å–")
            return
        
        for user_id in config.KS_CREATOR_ID_LIST:
            # get creator detail info from web html content
            createor_info: Dict = await self.ks_client.get_creator_info(user_id=user_id)
            if createor_info:
                await self.kuaishou_store.store_creator(createor_info)

            # Get all video information of the creator
            all_video_list = await self.ks_client.get_all_videos_by_creator(
                user_id=user_id,
                crawl_interval=random.random(),
                callback=self.fetch_creator_video_detail,
            )

            video_ids = [
                video_item.get("photo", {}).get("id") for video_item in all_video_list
            ]
            await self.batch_get_video_comments(video_ids)

    async def get_creators_and_notes_from_db(self, creators: List[Dict], max_count: int = 50,
                                           keywords: str = None, account_id: str = None, session_id: str = None,
                                           login_type: str = "qrcode", get_comments: bool = False,
                                           save_data_option: str = "db", use_proxy: bool = False,
                                           proxy_ip: str = None) -> List[Dict]:
        """
        ä»æ•°æ®åº“è·å–åˆ›ä½œè€…åˆ—è¡¨è¿›è¡Œçˆ¬å–ï¼ˆå‚è€ƒBç«™å®ç°ï¼‰
        Args:
            creators: åˆ›ä½œè€…åˆ—è¡¨ï¼ŒåŒ…å«creator_id, platform, name, nickname
            max_count: æœ€å¤§çˆ¬å–æ•°é‡
            keywords: å…³é”®è¯ï¼ˆå¯é€‰ï¼Œç”¨äºç­›é€‰åˆ›ä½œè€…å†…å®¹ï¼‰
            account_id: è´¦å·ID
            session_id: ä¼šè¯ID
            login_type: ç™»å½•ç±»å‹
            get_comments: æ˜¯å¦è·å–è¯„è®º
            save_data_option: æ•°æ®ä¿å­˜æ–¹å¼
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
            proxy_ip: æŒ‡å®šä»£ç†IPåœ°å€
        Returns:
            List[Dict]: çˆ¬å–ç»“æœåˆ—è¡¨
        """
        try:
            utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] å¼€å§‹çˆ¬å– {len(creators)} ä¸ªåˆ›ä½œè€…")
            utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] æœ€å¤§æ•°é‡é™åˆ¶: {max_count}")
            utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] å…³é”®è¯: '{keywords}'")
            utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] åˆ›ä½œè€…åˆ—è¡¨: {[c.get('name', c.get('nickname', 'æœªçŸ¥')) for c in creators]}")
            
            # ç¡®ä¿å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            if not hasattr(self, 'ks_client') or self.ks_client is None:
                utils.logger.error("[KuaishouCrawler.get_creators_and_notes_from_db] ks_client æœªåˆå§‹åŒ–")
                raise Exception("å¿«æ‰‹å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨start()æ–¹æ³•")
            
            all_results = []
            
            for creator in creators:
                user_id = creator.get("creator_id")
                creator_name = creator.get("name") or creator.get("nickname") or "æœªçŸ¥åˆ›ä½œè€…"
                
                utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] å¼€å§‹çˆ¬å–åˆ›ä½œè€…: {creator_name} (ID: {user_id})")
                
                try:
                    # è·å–åˆ›ä½œè€…è¯¦ç»†ä¿¡æ¯
                    creator_info: Dict = await self.ks_client.get_creator_info(user_id=user_id)
                    if creator_info:
                        # æ›´æ–°åˆ›ä½œè€…ä¿¡æ¯åˆ°æ•°æ®åº“
                        await self.kuaishou_store.store_creator(creator_info)
                        utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] åˆ›ä½œè€…ä¿¡æ¯å·²æ›´æ–°: {creator_name}")
                        
                        # æ›´æ–°ä»»åŠ¡çš„creator_ref_idså­—æ®µï¼ˆå‚è€ƒBç«™å®ç°ï¼‰
                        try:
                            from api.crawler_core import update_task_creator_ref_ids
                            await update_task_creator_ref_ids(self.task_id, [str(user_id)])
                            utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] ä»»åŠ¡creator_ref_idså·²æ›´æ–°: {user_id}")
                        except Exception as e:
                            utils.logger.error(f"[KuaishouCrawler.get_creators_and_notes_from_db] æ›´æ–°ä»»åŠ¡creator_ref_idså¤±è´¥: {e}")
                    
                    # ğŸ†• ä¼˜åŒ–ï¼šæ ¹æ®æ˜¯å¦æœ‰å…³é”®è¯é€‰æ‹©ä¸åŒçš„è·å–æ–¹å¼
                    if keywords and keywords.strip():
                        # ä½¿ç”¨å…³é”®è¯æœç´¢è·å–è§†é¢‘
                        utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] ä½¿ç”¨å…³é”®è¯ '{keywords}' æœç´¢åˆ›ä½œè€… {creator_name} çš„è§†é¢‘")
                        utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] å…³é”®è¯ç±»å‹: {type(keywords)}, é•¿åº¦: {len(keywords)}")
                        
                        # ç¡®ä¿å…³é”®è¯ä¸ä¸ºç©ºä¸”æœ‰æ•ˆ
                        clean_keywords = keywords.strip()
                        if clean_keywords:
                            all_video_list = await self.ks_client.search_user_videos(user_id, clean_keywords, max_count)
                            utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] å…³é”®è¯æœç´¢å®Œæˆï¼Œè·å–åˆ° {len(all_video_list) if all_video_list else 0} ä¸ªè§†é¢‘")
                        else:
                            utils.logger.warning(f"[KuaishouCrawler.get_creators_and_notes_from_db] å…³é”®è¯ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤è·å–æ–¹å¼")
                            all_video_list = await self.ks_client.get_all_videos_by_creator(
                                user_id=user_id,
                                crawl_interval=random.random(),
                                callback=self.fetch_creator_video_detail,
                            )
                    else:
                        # è·å–åˆ›ä½œè€…çš„æ‰€æœ‰è§†é¢‘ï¼ˆé™åˆ¶æ•°é‡ï¼‰
                        utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] è·å–åˆ›ä½œè€… {creator_name} çš„è§†é¢‘ï¼ˆæ¯ä¸ªåˆ›ä½œè€…é™åˆ¶: {max_count} ä¸ªï¼‰")
                        all_video_list = await self.ks_client.get_all_videos_by_creator(
                            user_id=user_id,
                            max_count=max_count,  # æ¯ä¸ªåˆ›ä½œè€…éƒ½çˆ¬å–æŒ‡å®šæ•°é‡
                            crawl_interval=random.random(),
                            callback=self.fetch_creator_video_detail,
                        )
                        utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] è·å–è§†é¢‘å®Œæˆï¼Œè·å–åˆ° {len(all_video_list) if all_video_list else 0} ä¸ªè§†é¢‘")
                    
                    if all_video_list:
                        utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] è·å–åˆ° {len(all_video_list)} ä¸ªè§†é¢‘")
                        
                        # å¤„ç†æ¯ä¸ªè§†é¢‘ï¼Œè·å–è¯¦ç»†ä¿¡æ¯ï¼ˆå‚è€ƒBç«™å®ç°ï¼‰
                        utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] å¼€å§‹å¤„ç† {len(all_video_list)} ä¸ªè§†é¢‘")
                        
                        for i, video_item in enumerate(all_video_list):
                            try:
                                utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] å¤„ç†ç¬¬ {i+1} ä¸ªè§†é¢‘")
                                
                                # ä¿å­˜åˆ°æ•°æ®åº“
                                utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] å¼€å§‹ä¿å­˜åˆ°æ•°æ®åº“")
                                try:
                                    await self.kuaishou_store.update_kuaishou_video(video_item, task_id=self.task_id)
                                    utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] è§†é¢‘æ•°æ®ä¿å­˜æˆåŠŸ")
                                except Exception as e:
                                    utils.logger.error(f"[KuaishouCrawler.get_creators_and_notes_from_db] è§†é¢‘æ•°æ®ä¿å­˜å¤±è´¥: {e}")
                                
                                # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                                all_results.append(video_item)
                                utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] è§†é¢‘å¤„ç†å®Œæˆï¼Œå·²æ·»åŠ åˆ°ç»“æœåˆ—è¡¨")
                                

                                
                            except Exception as e:
                                utils.logger.error(f"[KuaishouCrawler.get_creators_and_notes_from_db] å¤„ç†è§†é¢‘å¤±è´¥: {e}")
                                continue
                        
                        # è·å–è¯„è®ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        if get_comments and all_video_list:
                            try:
                                video_ids = [video_item.get("photo", {}).get("id") for video_item in all_video_list if video_item.get("photo", {}).get("id")]
                                await self.batch_get_video_comments(video_ids)
                                utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] è¯„è®ºè·å–å®Œæˆ")
                            except Exception as e:
                                utils.logger.error(f"[KuaishouCrawler.get_creators_and_notes_from_db] è·å–è¯„è®ºå¤±è´¥: {e}")
                    else:
                        utils.logger.warning(f"[KuaishouCrawler.get_creators_and_notes_from_db] åˆ›ä½œè€… {creator_name} æ²¡æœ‰è·å–åˆ°è§†é¢‘")
                
                except Exception as e:
                    utils.logger.error(f"[KuaishouCrawler.get_creators_and_notes_from_db] çˆ¬å–åˆ›ä½œè€… {creator_name} å¤±è´¥: {e}")
                    continue
            
            utils.logger.debug(f"[KuaishouCrawler.get_creators_and_notes_from_db] çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(all_results)} æ¡æ•°æ®")
            return all_results
            
        except Exception as e:
            utils.logger.error(f"[KuaishouCrawler.get_creators_and_notes_from_db] çˆ¬å–å¤±è´¥: {e}")
            raise

    async def fetch_creator_video_detail(self, video_list: List[Dict]):
        """
        Concurrently obtain the specified post list and save the data
        """
        semaphore = asyncio.Semaphore(config.MAX_CONCURRENCY_NUM)
        task_list = [
            self.get_video_info_task(post_item.get("photo", {}).get("id"), semaphore)
            for post_item in video_list
        ]

        video_details = await asyncio.gather(*task_list)
        for video_detail in video_details:
            if video_detail is not None:
                await self.kuaishou_store.update_kuaishou_video(video_detail, task_id=self.task_id)

    async def close(self):
        """Close browser context"""
        await self.browser_context.close()
        utils.logger.debug("[KuaishouCrawler.close] Browser context closed ...")

    async def search_by_keywords(self, keywords: str, max_count: int = 50, 
                                account_id: str = None, session_id: str = None,
                                login_type: str = "qrcode", get_comments: bool = False,
                                save_data_option: str = "db", use_proxy: bool = False,
                                proxy_ip: str = None, start_page: int = 1) -> List[Dict]:
        """
        æ ¹æ®å…³é”®è¯æœç´¢å¿«æ‰‹è§†é¢‘
        :param keywords: æœç´¢å…³é”®è¯
        :param max_count: æœ€å¤§è·å–æ•°é‡
        :param account_id: è´¦å·ID
        :param session_id: ä¼šè¯ID
        :param login_type: ç™»å½•ç±»å‹
        :param get_comments: æ˜¯å¦è·å–è¯„è®º
        :param save_data_option: æ•°æ®ä¿å­˜æ–¹å¼
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        :param proxy_ip: æŒ‡å®šä»£ç†IPåœ°å€
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            utils.logger.debug(f"[KuaishouCrawler.search_by_keywords] å¼€å§‹æœç´¢å…³é”®è¯: {keywords}")
            
            # ğŸ†• è®¾ç½®account_idåˆ°å®ä¾‹å˜é‡ï¼Œä¾›startæ–¹æ³•ä½¿ç”¨
            self.account_id = account_id
            if account_id:
                utils.logger.debug(f"[KuaishouCrawler.search_by_keywords] ä½¿ç”¨æŒ‡å®šè´¦å·ID: {account_id}")
            
            # è®¾ç½®é…ç½®
            import config
            # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨åŠ¨æ€å…³é”®å­—ï¼Œå®Œå…¨å¿½ç•¥é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®å­—
            if keywords and keywords.strip():
                # å°†åŠ¨æ€å…³é”®å­—è®¾ç½®åˆ°å®ä¾‹å˜é‡ï¼Œè€Œä¸æ˜¯å…¨å±€é…ç½®
                self.dynamic_keywords = keywords
                utils.logger.debug(f"[KuaishouCrawler.search_by_keywords] è®¾ç½®åŠ¨æ€å…³é”®å­—: '{keywords}'")
            else:
                utils.logger.warning("[KuaishouCrawler.search_by_keywords] å…³é”®å­—ä¸ºç©ºï¼Œå°†ä½¿ç”¨é»˜è®¤æœç´¢")
            
            # ğŸ†• ä¿®å¤ï¼šå°†å…³é”®å‚æ•°è®¾ç½®åˆ°å®ä¾‹å˜é‡ï¼Œè€Œä¸æ˜¯å…¨å±€é…ç½®
            self.max_notes_count = max_count
            self.get_comments = get_comments
            self.save_data_option = save_data_option
            # ä¿ç•™å…¶ä»–é…ç½®ä½¿ç”¨å…¨å±€config
            config.ENABLE_IP_PROXY = use_proxy
            
            # ğŸ†• æ¸…ç©ºä¹‹å‰æ”¶é›†çš„æ•°æ®ï¼Œç¡®ä¿æ–°ä»»åŠ¡çš„æ•°æ®æ­£ç¡®
            try:
                from store.kuaishou import _clear_collected_data
                _clear_collected_data()
            except Exception as e:
                utils.logger.warning(f"[KuaishouCrawler] æ¸…ç©ºæ•°æ®å¤±è´¥: {e}")
            
            # å¯åŠ¨çˆ¬è™«
            await self.start(start_page=start_page)
            
            # ç”±äºRediså­˜å‚¨æ˜¯é€šè¿‡å›è°ƒå‡½æ•°å¤„ç†çš„ï¼Œæˆ‘ä»¬éœ€è¦ä»Redisä¸­è·å–æ•°æ®
            # æˆ–è€…ç›´æ¥è¿”å›çˆ¬å–è¿‡ç¨‹ä¸­æ”¶é›†çš„æ•°æ®
            results = []
            
            # å¦‚æœä½¿ç”¨äº†Rediså­˜å‚¨ï¼Œå°è¯•ä»Redisè·å–æ•°æ®
            if hasattr(self, 'kuaishou_store') and hasattr(self.kuaishou_store, 'get_all_content'):
                results = await self.kuaishou_store.get_all_content()
            
            # å¦‚æœRedisä¸­æ²¡æœ‰æ•°æ®ï¼Œå°è¯•ä»ä»»åŠ¡ç»“æœä¸­è·å–
            if not results and hasattr(self, 'task_id'):
                from utils.redis_manager import redis_manager
                try:
                    task_videos = await redis_manager.get_task_videos(self.task_id, "ks")
                    results = task_videos
                except Exception as e:
                    utils.logger.warning(f"[KuaishouCrawler.search_by_keywords] ä»Redisè·å–æ•°æ®å¤±è´¥: {e}")
            
            utils.logger.debug(f"[KuaishouCrawler.search_by_keywords] æœç´¢å®Œæˆï¼Œè·å– {len(results)} æ¡æ•°æ®")
            return results
            
        except Exception as e:
            utils.logger.error(f"[KuaishouCrawler.search_by_keywords] æœç´¢å¤±è´¥: {e}")
            raise
        finally:
            # ğŸ†• ä¿®å¤ï¼šé¿å…é‡å¤å…³é—­æµè§ˆå™¨ï¼Œåªåœ¨æ²¡æœ‰å¤–éƒ¨ç®¡ç†æ—¶å…³é—­
            try:
                if hasattr(self, 'browser_context') and self.browser_context:
                    # æ£€æŸ¥æ˜¯å¦ç”±å¤–éƒ¨ç®¡ç†ï¼ˆå¦‚crawler_core.pyï¼‰
                    if not hasattr(self, '_externally_managed') or not self._externally_managed:
                        await self.close()
                        utils.logger.info("[KuaishouCrawler.search_by_keywords] æµè§ˆå™¨å·²å…³é—­")
                    else:
                        utils.logger.info("[KuaishouCrawler.search_by_keywords] æµè§ˆå™¨ç”±å¤–éƒ¨ç®¡ç†ï¼Œè·³è¿‡å…³é—­")
            except Exception as e:
                utils.logger.warning(f"[KuaishouCrawler.search_by_keywords] å…³é—­æµè§ˆå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")

    async def get_user_notes(self, user_id: str, max_count: int = 50,
                            account_id: str = None, session_id: str = None,
                            login_type: str = "qrcode", get_comments: bool = False,
                            save_data_option: str = "db", use_proxy: bool = False,
                            proxy_strategy: str = "disabled") -> List[Dict]:
        """
        è·å–ç”¨æˆ·å‘å¸ƒçš„è§†é¢‘
        :param user_id: ç”¨æˆ·ID
        :param max_count: æœ€å¤§è·å–æ•°é‡
        :param account_id: è´¦å·ID
        :param session_id: ä¼šè¯ID
        :param login_type: ç™»å½•ç±»å‹
        :param get_comments: æ˜¯å¦è·å–è¯„è®º
        :param save_data_option: æ•°æ®ä¿å­˜æ–¹å¼
        :param use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        :param proxy_strategy: ä»£ç†ç­–ç•¥
        :return: è§†é¢‘åˆ—è¡¨
        """
        try:
            utils.logger.debug(f"[KuaishouCrawler.get_user_notes] å¼€å§‹è·å–ç”¨æˆ·è§†é¢‘: {user_id}")
            
            # è®¾ç½®é…ç½®
            import config
            # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨åŠ¨æ€ç”¨æˆ·IDï¼Œè€Œä¸æ˜¯ä¿®æ”¹å…¨å±€é…ç½®
            self.dynamic_video_ids = [user_id]
            utils.logger.debug(f"[KuaishouCrawler.get_user_notes] è®¾ç½®åŠ¨æ€ç”¨æˆ·ID: {user_id}")
            
            # ğŸ†• ä¿®å¤ï¼šå°†å…³é”®å‚æ•°è®¾ç½®åˆ°å®ä¾‹å˜é‡ï¼Œè€Œä¸æ˜¯å…¨å±€é…ç½®
            self.max_notes_count = max_count
            self.get_comments = get_comments
            self.save_data_option = save_data_option
            # ä¿ç•™å…¶ä»–é…ç½®ä½¿ç”¨å…¨å±€config
            config.ENABLE_IP_PROXY = use_proxy
            
            # å¯åŠ¨çˆ¬è™«
            await self.start()
            
            # è·å–å­˜å‚¨çš„æ•°æ®
            results = []
            if hasattr(self, 'kuaishou_store') and hasattr(self.kuaishou_store, 'get_all_content'):
                results = await self.kuaishou_store.get_all_content()
            
            utils.logger.debug(f"[KuaishouCrawler.get_user_notes] è·å–å®Œæˆï¼Œå…± {len(results)} æ¡æ•°æ®")
            return results
            
        except Exception as e:
            utils.logger.error(f"[KuaishouCrawler.get_user_notes] è·å–å¤±è´¥: {e}")
            raise
        finally:
            # ğŸ†• ä¿®å¤ï¼šé¿å…é‡å¤å…³é—­æµè§ˆå™¨ï¼Œåªåœ¨æ²¡æœ‰å¤–éƒ¨ç®¡ç†æ—¶å…³é—­
            try:
                if hasattr(self, 'browser_context') and self.browser_context:
                    # æ£€æŸ¥æ˜¯å¦ç”±å¤–éƒ¨ç®¡ç†ï¼ˆå¦‚crawler_core.pyï¼‰
                    if not hasattr(self, '_externally_managed') or not self._externally_managed:
                        await self.close()
                        utils.logger.info("[KuaishouCrawler.get_user_notes] æµè§ˆå™¨å·²å…³é—­")
                    else:
                        utils.logger.info("[KuaishouCrawler.get_user_notes] æµè§ˆå™¨ç”±å¤–éƒ¨ç®¡ç†ï¼Œè·³è¿‡å…³é—­")
            except Exception as e:
                utils.logger.warning(f"[KuaishouCrawler.get_user_notes] å…³é—­æµè§ˆå™¨æ—¶å‡ºç°è­¦å‘Š: {e}")
