#!/usr/bin/env python3
"""
æµ‹è¯•creator_ref_idså­—æ®µè¿ç§»
"""

import asyncio
import json
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

async def test_creator_ref_ids_migration():
    """æµ‹è¯•creator_ref_idså­—æ®µè¿ç§»"""
    try:
        from config.env_config_loader import config_loader
        import aiomysql
        
        # è·å–æ•°æ®åº“é…ç½®
        db_config = config_loader.get_database_config()
        
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        pool = await aiomysql.create_pool(
            host=db_config['host'],
            port=db_config['port'],
            user=db_config['username'],
            password=db_config['password'],
            db=db_config['database'],
            autocommit=True,
            minsize=1,
            maxsize=10,
        )
        
        async with pool.acquire() as conn:
            async with conn.cursor() as cursor:
                print("ğŸ” å¼€å§‹æµ‹è¯•creator_ref_idså­—æ®µ...")
                
                # 1. æ£€æŸ¥å­—æ®µæ˜¯å¦å­˜åœ¨
                print("\nğŸ“ æ­¥éª¤1: æ£€æŸ¥å­—æ®µç»“æ„")
                await cursor.execute("""
                    SELECT COLUMN_NAME, DATA_TYPE, IS_NULLABLE, COLUMN_COMMENT
                    FROM INFORMATION_SCHEMA.COLUMNS 
                    WHERE TABLE_SCHEMA = %s 
                    AND TABLE_NAME = 'crawler_tasks' 
                    AND COLUMN_NAME IN ('creator_ref_id', 'creator_ref_ids')
                    ORDER BY COLUMN_NAME
                """, (db_config['database'],))
                
                columns = await cursor.fetchall()
                print("ğŸ“Š å­—æ®µä¿¡æ¯:")
                for column in columns:
                    print(f"  - {column[0]}: {column[1]} ({column[2]}) - {column[3]}")
                
                # 2. æ£€æŸ¥ç´¢å¼•
                print("\nğŸ“ æ­¥éª¤2: æ£€æŸ¥ç´¢å¼•")
                await cursor.execute("""
                    SELECT INDEX_NAME, COLUMN_NAME
                    FROM INFORMATION_SCHEMA.STATISTICS 
                    WHERE TABLE_SCHEMA = %s 
                    AND TABLE_NAME = 'crawler_tasks' 
                    AND INDEX_NAME LIKE '%creator_ref%'
                    ORDER BY INDEX_NAME
                """, (db_config['database'],))
                
                indexes = await cursor.fetchall()
                print("ğŸ“Š ç´¢å¼•ä¿¡æ¯:")
                for index in indexes:
                    print(f"  - {index[0]}: {index[1]}")
                
                # 3. æ£€æŸ¥æ•°æ®
                print("\nğŸ“ æ­¥éª¤3: æ£€æŸ¥æ•°æ®")
                await cursor.execute("""
                    SELECT id, platform, crawler_type, creator_ref_ids, keywords, created_at
                    FROM crawler_tasks 
                    WHERE crawler_type = 'creator' 
                    ORDER BY created_at DESC
                    LIMIT 5
                """)
                
                rows = await cursor.fetchall()
                if rows:
                    print("ğŸ“Š åˆ›ä½œè€…ä»»åŠ¡æ•°æ®:")
                    for row in rows:
                        task_id, platform, crawler_type, creator_ref_ids, keywords, created_at = row
                        print(f"  - ä»»åŠ¡ID: {task_id}")
                        print(f"    å¹³å°: {platform}")
                        print(f"    ç±»å‹: {crawler_type}")
                        print(f"    åˆ›ä½œè€…IDs: {creator_ref_ids}")
                        print(f"    å…³é”®è¯: {keywords}")
                        print(f"    åˆ›å»ºæ—¶é—´: {created_at}")
                        
                        # æµ‹è¯•JSONè§£æ
                        if creator_ref_ids:
                            try:
                                parsed_ids = json.loads(creator_ref_ids)
                                print(f"    âœ… JSONè§£ææˆåŠŸ: {parsed_ids}")
                                print(f"    åˆ›ä½œè€…æ•°é‡: {len(parsed_ids)}")
                            except json.JSONDecodeError as e:
                                print(f"    âŒ JSONè§£æå¤±è´¥: {e}")
                        else:
                            print("    âš ï¸ åˆ›ä½œè€…IDsä¸ºç©º")
                        print()
                else:
                    print("ğŸ“Š æš‚æ— åˆ›ä½œè€…ä»»åŠ¡æ•°æ®")
                
                # 4. æµ‹è¯•æ’å…¥æ–°æ•°æ®
                print("\nğŸ“ æ­¥éª¤4: æµ‹è¯•æ’å…¥æ–°æ•°æ®")
                test_task_id = "test_creator_ref_ids_" + str(int(asyncio.get_event_loop().time()))
                test_creator_ids = ["test_creator_1", "test_creator_2", "test_creator_3"]
                
                try:
                    await cursor.execute("""
                        INSERT INTO crawler_tasks (
                            id, platform, task_type, crawler_type, creator_ref_ids, 
                            keywords, status, progress, result_count, 
                            created_at, updated_at
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        test_task_id, "ks", "single_platform", "creator", 
                        json.dumps(test_creator_ids), "æµ‹è¯•å…³é”®è¯", "completed", 
                        100.0, 0, "2024-01-01 00:00:00", "2024-01-01 00:00:00"
                    ))
                    
                    print(f"âœ… æµ‹è¯•æ•°æ®æ’å…¥æˆåŠŸ: {test_task_id}")
                    
                    # éªŒè¯æ’å…¥çš„æ•°æ®
                    await cursor.execute("""
                        SELECT creator_ref_ids FROM crawler_tasks WHERE id = %s
                    """, (test_task_id,))
                    
                    result = await cursor.fetchone()
                    if result:
                        stored_ids = result[0]
                        print(f"ğŸ“Š å­˜å‚¨çš„æ•°æ®: {stored_ids}")
                        
                        # è§£æå¹¶éªŒè¯
                        parsed_ids = json.loads(stored_ids)
                        print(f"ğŸ“Š è§£æåçš„æ•°æ®: {parsed_ids}")
                        print(f"ğŸ“Š æ•°æ®ç±»å‹: {type(parsed_ids)}")
                        print(f"ğŸ“Š æ•°æ®é•¿åº¦: {len(parsed_ids)}")
                        
                        if parsed_ids == test_creator_ids:
                            print("âœ… æ•°æ®éªŒè¯æˆåŠŸï¼")
                        else:
                            print("âŒ æ•°æ®éªŒè¯å¤±è´¥ï¼")
                    
                    # æ¸…ç†æµ‹è¯•æ•°æ®
                    await cursor.execute("DELETE FROM crawler_tasks WHERE id = %s", (test_task_id,))
                    print("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
                    
                except Exception as e:
                    print(f"âŒ æµ‹è¯•æ•°æ®æ’å…¥å¤±è´¥: {e}")
                
                print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
                
        pool.close()
        await pool.wait_closed()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•creator_ref_idså­—æ®µè¿ç§»")
    
    # æ‰§è¡Œæµ‹è¯•
    asyncio.run(test_creator_ref_ids_migration())
    
    print("\nğŸ¯ æµ‹è¯•è¯´æ˜:")
    print("1. æ£€æŸ¥å­—æ®µç»“æ„æ˜¯å¦æ­£ç¡®")
    print("2. æ£€æŸ¥ç´¢å¼•æ˜¯å¦æ­£ç¡®")
    print("3. æ£€æŸ¥ç°æœ‰æ•°æ®")
    print("4. æµ‹è¯•æ–°æ•°æ®æ’å…¥å’Œè§£æ")

if __name__ == "__main__":
    main() 