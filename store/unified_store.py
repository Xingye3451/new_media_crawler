# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š  
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚  
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚  
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚  
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚   
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#   
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚  
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚  

# -*- coding: utf-8 -*-
# @Author  : relakkes@gmail.com
# @Time    : 2024/4/6 15:30
# @Desc    : ç»Ÿä¸€å­˜å‚¨ç³»ç»Ÿ - å¤šå¹³å°å†…å®¹ç»Ÿä¸€å­˜å‚¨

import json
import time
from typing import Dict, List, Optional
from tools import utils

from db import AsyncMysqlDB
from var import media_crawler_db_var


# ç»Ÿä¸€å†…å®¹è¡¨å­—æ®µå®šä¹‰
UNIFIED_CONTENT_FIELDS = {
    # åŸºç¡€æ ‡è¯†å­—æ®µ
    "content_id", "platform", "content_type", "task_id", "source_keyword",
    
    # å†…å®¹ä¿¡æ¯å­—æ®µ
    "title", "description", "content", "create_time", "publish_time", "update_time",
    
    # ä½œè€…ä¿¡æ¯å­—æ®µ
    "author_id", "author_name", "author_nickname", "author_avatar", "author_signature",
    "author_unique_id", "author_sec_uid", "author_short_id",
    
    # ç»Ÿè®¡å­—æ®µ
    "like_count", "comment_count", "share_count", "collect_count", "view_count",
    
    # åª’ä½“å­—æ®µ
    "cover_url", "video_url", "video_download_url", "video_play_url", "video_share_url",
    "image_urls", "audio_url", "file_urls",
    
    # ä½ç½®ä¿¡æ¯
    "ip_location", "location",
    
    # æ ‡ç­¾å’Œåˆ†ç±»
    "tags", "categories", "topics",
    
    # çŠ¶æ€å­—æ®µ
    "is_favorite", "is_deleted", "is_private", "is_original",
    
    # å­˜å‚¨å­—æ®µ
    "minio_url", "local_path", "file_size", "storage_type",
    
    # æ‰©å±•å­—æ®µ
    "metadata", "raw_data", "extra_info",
    
    # æ—¶é—´æˆ³
    "add_ts", "last_modify_ts"
}

# ç»Ÿä¸€è¯„è®ºè¡¨å­—æ®µå®šä¹‰
UNIFIED_COMMENT_FIELDS = {
    # åŸºç¡€æ ‡è¯†å­—æ®µ
    "comment_id", "content_id", "platform", "parent_id", "reply_to_id",
    
    # è¯„è®ºå†…å®¹
    "content", "text", "html_content",
    
    # ä½œè€…ä¿¡æ¯
    "author_id", "author_name", "author_nickname", "author_avatar",
    
    # ç»Ÿè®¡å­—æ®µ
    "like_count", "reply_count", "share_count",
    
    # æ—¶é—´å­—æ®µ
    "create_time", "publish_time",
    
    # çŠ¶æ€å­—æ®µ
    "is_deleted", "is_hidden", "is_top",
    
    # æ‰©å±•å­—æ®µ
    "metadata", "raw_data",
    
    # æ—¶é—´æˆ³
    "add_ts", "last_modify_ts"
}

# å¹³å°å­—æ®µæ˜ å°„é…ç½®
PLATFORM_FIELD_MAPPINGS = {
    "douyin": {
        "content_id": "aweme_id",
        "content_type": "video",  # å›ºå®šå€¼
        "title": "desc",
        "description": "desc",
        "content": "desc",
        "author_id": "author.uid",
        "author_name": "author.nickname",
        "author_nickname": "author.nickname",
        "author_avatar": "author.avatar_thumb.url_list.0",
        "author_signature": "author.signature",
        "author_unique_id": "author.unique_id",
        "author_sec_uid": "author.sec_uid",
        "author_short_id": "author.short_id",
        "like_count": "statistics.digg_count",
        "comment_count": "statistics.comment_count",
        "share_count": "statistics.share_count",
        "collect_count": "statistics.collect_count",
        "view_count": "statistics.play_count",
        "cover_url": "video.cover.url_list.0",
        "video_url": "aweme_url",  # æ’­æ”¾é¡µé“¾æ¥
        "video_download_url": "download_url",  # ä¸‹è½½é“¾æ¥
        "video_play_url": "aweme_url",  # æ’­æ”¾é¡µé“¾æ¥
        "video_share_url": "aweme_url",  # åˆ†äº«é“¾æ¥ï¼ˆä½¿ç”¨æ’­æ”¾é¡µé“¾æ¥ï¼‰
        "audio_url": "music.play_url.uri",
        "ip_location": "ip_location",
        "create_time": "create_time",
        "publish_time": "create_time",
        "update_time": "create_time",
        "topics": "cha_list",
        "raw_data": "raw_data"
    },
    "xhs": {
        "content_id": "id",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„idå­—æ®µ
        "content_type": "type",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„typeå­—æ®µ
        "title": "desc",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„descå­—æ®µ
        "description": "desc",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„descå­—æ®µ
        "content": "desc",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„descå­—æ®µ
        "author_id": "user.user_id",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„user.user_id
        "author_name": "user.nickname",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„user.nickname
        "author_nickname": "user.nickname",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„user.nickname
        "author_avatar": "user.avatar",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„user.avatar
        "like_count": "interact_info.liked_count",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„interact_info.liked_count
        "comment_count": "interact_info.comment_count",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„interact_info.comment_count
        "share_count": "interact_info.share_count",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„interact_info.share_count
        "collect_count": "interact_info.collected_count",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„interact_info.collected_count
        "cover_url": "image_list.0.url_default",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„image_listç¬¬ä¸€ä¸ªå›¾ç‰‡çš„url_default
        "image_urls": "image_list",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„image_list
        "video_url": "video.media.stream.h264.0.master_url",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„è§†é¢‘æµURL
        "video_download_url": "video.media.stream.h264.0.master_url",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„è§†é¢‘æµURL
        "video_play_url": "video.media.stream.h264.0.master_url",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„è§†é¢‘æµURL
        "video_share_url": "video.media.stream.h264.0.master_url",  # ä¿®å¤ï¼šä½¿ç”¨åµŒå¥—çš„è§†é¢‘æµURL
        "ip_location": "ip_location",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„ip_location
        "create_time": "time",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„timeå­—æ®µ
        "publish_time": "time",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„timeå­—æ®µ
        "update_time": "last_update_time",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„last_update_time
        "tags": "tag_list",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„tag_list
        "topics": "tag_list",  # ä¿®å¤ï¼šä½¿ç”¨é¡¶å±‚çš„tag_list
        "raw_data": "raw_data"
    },
    "kuaishou": {
        "content_id": "video_id",
        "content_type": "video_type",
        "title": "title",
        "description": "desc",
        "content": "desc",
        "author_id": "user_id",
        "author_name": "nickname",
        "author_nickname": "nickname",
        "author_avatar": "avatar",
        "like_count": "liked_count",
        "comment_count": "comment_count",
        "share_count": "share_count",
        "collect_count": "collect_count",
        "view_count": "viewd_count",
        "cover_url": "video_cover_url",
        "video_url": "video_url",
        "video_play_url": "video_play_url",
        "video_download_url": "video_download_url",
        "create_time": "create_time",
        "publish_time": "create_time",
        "update_time": "last_modify_ts",
        "raw_data": "raw_data",
        # æ–°å¢å­—æ®µæ˜ å°„
        "author_signature": "author_signature",
        "author_unique_id": "author_unique_id",
        "author_sec_uid": "author_sec_uid",
        "author_short_id": "author_short_id",
        "video_share_url": "video_share_url",
        "image_urls": "image_urls",
        "audio_url": "audio_url",
        "file_urls": "file_urls",
        "ip_location": "ip_location",
        "location": "location",
        "tags": "tags",
        "categories": "categories",
        "topics": "topics",
        "is_favorite": "is_favorite",
        "is_deleted": "is_deleted",
        "is_private": "is_private",
        "is_original": "is_original",
        "minio_url": "minio_url",
        "local_path": "local_path"
    },
    "bilibili": {
        "content_id": "content_id",  # ç›´æ¥ä½¿ç”¨content_idå­—æ®µ
        "content_type": "content_type",
        "title": "title",
        "description": "description",
        "content": "content",
        "source_keyword": "source_keyword",
        "create_time": "create_time",
        "publish_time": "publish_time",
        "update_time": "update_time",
        "author_id": "author_id",
        "author_name": "author_name",
        "author_nickname": "author_nickname",
        "author_avatar": "author_avatar",
        "author_signature": "author_signature",
        "author_unique_id": "author_unique_id",
        "author_sec_uid": "author_sec_uid",
        "author_short_id": "author_short_id",
        "like_count": "like_count",
        "comment_count": "comment_count",
        "share_count": "share_count",
        "collect_count": "collect_count",
        "view_count": "view_count",
        "cover_url": "cover_url",
        "video_url": "video_url",
        "video_play_url": "video_play_url",
        "video_download_url": "video_download_url",
        "video_share_url": "video_share_url",
        "image_urls": "image_urls",
        "audio_url": "audio_url",
        "file_urls": "file_urls",
        "ip_location": "ip_location",
        "location": "location",
        "tags": "tags",
        "categories": "categories",
        "topics": "topics",
        "is_favorite": "is_favorite",
        "is_deleted": "is_deleted",
        "is_private": "is_private",
        "is_original": "is_original",
        "minio_url": "minio_url",
        "local_path": "local_path",
        "file_size": "file_size",
        "storage_type": "storage_type",
        "metadata": "metadata",
        "raw_data": "raw_data",
        "extra_info": "extra_info",
        "add_ts": "add_ts",
        "last_modify_ts": "last_modify_ts"
    },
    "weibo": {
        "content_id": "id",
        "content_type": "weibo_type",
        "title": "title",
        "content": "text",
        "author_id": "user_id",
        "author_name": "screen_name",
        "author_nickname": "screen_name",
        "author_avatar": "avatar_hd",
        "author_signature": "description",
        "like_count": "attitudes_count",
        "comment_count": "comments_count",
        "share_count": "reposts_count",
        "cover_url": "thumbnail_pic",
        "image_urls": "pics",
        "create_time": "created_at"
    },
    "zhihu": {
        "content_id": "id",
        "content_type": "content_type",
        "title": "title",
        "content": "content",
        "author_id": "author_id",
        "author_name": "author_name",
        "author_nickname": "author_name",
        "author_avatar": "author_avatar",
        "like_count": "voteup_count",
        "comment_count": "comment_count",
        "share_count": "share_count",
        "create_time": "created_time",
        "publish_time": "updated_time"
    },
    "tieba": {
        "content_id": "tid",
        "content_type": "post_type",
        "title": "title",
        "content": "content",
        "author_id": "author_id",
        "author_name": "author_name",
        "author_nickname": "author_name",
        "like_count": "like_count",
        "comment_count": "reply_count",
        "create_time": "create_time"
    }
}


def filter_fields_for_table(item: dict, allowed_fields: set) -> dict:
    """è¿‡æ»¤å­—æ®µï¼Œåªä¿ç•™å…è®¸çš„å­—æ®µ"""
    return {k: v for k, v in item.items() if k in allowed_fields}


def serialize_for_db(data):
    """åºåˆ—åŒ–æ•°æ®ï¼Œå°†dict/listè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
    if isinstance(data, dict):
        new_data = {}
        for k, v in data.items():
            if isinstance(v, (dict, list)):
                new_data[k] = json.dumps(v, ensure_ascii=False)
            else:
                new_data[k] = v
        return new_data
    elif isinstance(data, list):
        return json.dumps(data, ensure_ascii=False)
    else:
        return data


def get_nested_value(data: Dict, path: str):
    """
    æ ¹æ®è·¯å¾„è·å–åµŒå¥—å­—å…¸ä¸­çš„å€¼
    
    Args:
        data (Dict): æ•°æ®å­—å…¸
        path (str): è·¯å¾„ï¼Œå¦‚ "author.nickname" æˆ– "video.cover.url_list.0"
    
    Returns:
        ä»»æ„ç±»å‹: æ‰¾åˆ°çš„å€¼ï¼Œå¦‚æœè·¯å¾„ä¸å­˜åœ¨åˆ™è¿”å›None
    """
    if not path or not data:
        return None
    
    keys = path.split('.')
    current = data
    
    for key in keys:
        if isinstance(current, dict):
            if key.isdigit() and isinstance(current, list):
                # å¤„ç†æ•°ç»„ç´¢å¼•
                try:
                    current = current[int(key)]
                except (IndexError, ValueError):
                    return None
            else:
                current = current.get(key)
        elif isinstance(current, list) and key.isdigit():
            # å¤„ç†æ•°ç»„ç´¢å¼•
            try:
                current = current[int(key)]
            except (IndexError, ValueError):
                return None
        else:
            return None
        
        if current is None:
            return None
    
    return current


def map_platform_fields(platform: str, data: Dict) -> Dict:
    """å°†å¹³å°ç‰¹å®šå­—æ®µæ˜ å°„åˆ°ç»Ÿä¸€å­—æ®µ"""
    if platform not in PLATFORM_FIELD_MAPPINGS:
        return data
    
    mapping = PLATFORM_FIELD_MAPPINGS[platform]
    mapped_data = {}
    
    # ğŸ†• æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œæ‰“å°åŸå§‹æ•°æ®
    utils.logger.info(f"[map_platform_fields] å¹³å°: {platform}")
    utils.logger.info(f"[map_platform_fields] åŸå§‹æ•°æ®å­—æ®µ: {list(data.keys())}")
    utils.logger.info(f"[map_platform_fields] åŸå§‹æ•°æ®å†…å®¹: {data}")
    
    # æ·»åŠ å¹³å°æ ‡è¯†
    mapped_data["platform"] = platform
    
    # æ•°å€¼å­—æ®µåˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºæ•´æ•°
    numeric_fields = {
        "like_count", "comment_count", "share_count", "collect_count", 
        "view_count", "create_time", "publish_time", "update_time",
        "add_ts", "last_modify_ts"
    }
    
    # æ˜ å°„å­—æ®µ
    for unified_field, platform_field in mapping.items():
        if platform_field == "raw_data":
            # ç‰¹æ®Šå¤„ç†åŸå§‹æ•°æ®å­—æ®µ
            mapped_data[unified_field] = json.dumps(data, ensure_ascii=False)
        elif platform_field == "topics" and platform_field in data:
            # ç‰¹æ®Šå¤„ç†è¯é¢˜å­—æ®µ
            topics_data = data.get(platform_field, [])
            if isinstance(topics_data, list):
                topic_names = []
                for topic in topics_data:
                    if isinstance(topic, dict):
                        topic_names.append(topic.get("cha_name", ""))
                    else:
                        topic_names.append(str(topic))
                mapped_data[unified_field] = json.dumps(topic_names, ensure_ascii=False)
        elif platform_field in ["video", "image", "note", "post"]:
            # å›ºå®šå€¼æ˜ å°„
            mapped_data[unified_field] = platform_field
        elif "." not in platform_field and platform_field not in data:
            # å…¶ä»–å›ºå®šå€¼æ˜ å°„ï¼ˆå¦‚æœå­—æ®µä¸åœ¨æ•°æ®ä¸­ï¼‰
            # ä¿®å¤ï¼šä¸åº”è¯¥å°†å­—æ®µåä½œä¸ºå€¼ä¼ é€’
            # mapped_data[unified_field] = platform_field
            pass  # è·³è¿‡ä¸å­˜åœ¨çš„å­—æ®µ
        else:
            # å¤„ç†åµŒå¥—å­—æ®µè·¯å¾„æˆ–ç›´æ¥å­—æ®µ
            value = None
            if "." in platform_field:
                # åµŒå¥—å­—æ®µè·¯å¾„
                value = get_nested_value(data, platform_field)
            else:
                # ç›´æ¥å­—æ®µ
                if platform_field in data:
                    value = data[platform_field]
            
            # ğŸ†• ä¿®å¤ï¼šç‰¹æ®Šå¤„ç†content_idå­—æ®µï¼Œå¦‚æœä¸ºç©ºåˆ™ç”Ÿæˆä¸´æ—¶ID
            if unified_field == "content_id" and (value is None or value == ""):
                value = f"temp_{platform}_{int(time.time() * 1000)}"
                utils.logger.warning(f"[map_platform_fields] {platform} content_idä¸ºç©ºï¼Œç”Ÿæˆä¸´æ—¶ID: {value}")
            
            # ğŸ†• ä¿®å¤ï¼šç¡®ä¿content_idå­—æ®µæ€»æ˜¯è¢«æ·»åŠ ï¼Œå³ä½¿valueä¸ºNone
            if unified_field == "content_id" and value is None:
                value = f"temp_{platform}_{int(time.time() * 1000)}"
                utils.logger.warning(f"[map_platform_fields] {platform} content_idå­—æ®µä¸å­˜åœ¨ï¼Œç”Ÿæˆä¸´æ—¶ID: {value}")
            
            if value is not None:
                # ğŸ†• æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œæ‰“å°æ˜ å°„ç»“æœ
                utils.logger.info(f"[map_platform_fields] å­—æ®µæ˜ å°„: {platform_field} -> {unified_field} = {value}")
                # å¯¹æ•°å€¼å­—æ®µè¿›è¡Œç±»å‹è½¬æ¢
                if unified_field in numeric_fields:
                    try:
                        if isinstance(value, str):
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è½¬æ¢ä¸ºæ•´æ•°
                            if value.isdigit():
                                mapped_data[unified_field] = int(value)
                            else:
                                mapped_data[unified_field] = 0
                        elif isinstance(value, (int, float)):
                            mapped_data[unified_field] = int(value)
                        else:
                            mapped_data[unified_field] = 0
                    except (ValueError, TypeError):
                        mapped_data[unified_field] = 0
                    
                    # ç‰¹æ®Šå¤„ç†Bç«™æ—¶é—´æˆ³ï¼š10ä½è½¬13ä½
                    if platform == "bilibili" and unified_field in ["create_time", "publish_time", "update_time"]:
                        if mapped_data[unified_field] > 0:
                            # æ£€æŸ¥æ˜¯å¦ä¸º10ä½æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
                            timestamp = mapped_data[unified_field]
                            if timestamp < 10000000000:  # 10ä½æ—¶é—´æˆ³
                                mapped_data[unified_field] = timestamp * 1000  # è½¬æ¢ä¸º13ä½æ—¶é—´æˆ³
                                utils.logger.info(f"[map_platform_fields] Bç«™æ—¶é—´æˆ³è½¬æ¢: {timestamp} -> {mapped_data[unified_field]}")
                else:
                    mapped_data[unified_field] = value
    
    # ğŸ†• æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œæ‰“å°æœ€ç»ˆæ˜ å°„ç»“æœ
    utils.logger.info(f"[map_platform_fields] æœ€ç»ˆæ˜ å°„ç»“æœ: {mapped_data}")
    utils.logger.info(f"[map_platform_fields] content_idå€¼: {mapped_data.get('content_id', 'NOT_FOUND')}")
    
    return mapped_data


async def _get_db_connection() -> AsyncMysqlDB:
    """è·å–æ•°æ®åº“è¿æ¥"""
    try:
        return media_crawler_db_var.get()
    except LookupError:
        try:
            from db import init_mediacrawler_db
            await init_mediacrawler_db()
            return media_crawler_db_var.get()
        except Exception as e:
            utils.logger.error(f"æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise


async def query_content_by_content_id(platform: str, content_id: str) -> Dict:
    """æŸ¥è¯¢å†…å®¹è®°å½•"""
    try:
        async_db_conn: AsyncMysqlDB = await _get_db_connection()
        sql = f"SELECT * FROM unified_content WHERE platform = '{platform}' AND content_id = '{content_id}'"
        rows: List[Dict] = await async_db_conn.query(sql)
        if len(rows) > 0:
            return rows[0]
        return dict()
    except Exception as e:
        utils.logger.error(f"æŸ¥è¯¢å†…å®¹å¤±è´¥: {platform}/{content_id}, é”™è¯¯: {e}")
        return dict()


async def add_new_content(platform: str, content_item: Dict, task_id: str = None) -> int:
    """æ–°å¢å†…å®¹è®°å½•"""
    try:
        utils.logger.debug(f"[add_new_content] å¼€å§‹æ–°å¢å†…å®¹: {platform}/{content_item.get('content_id', 'unknown')}")
        
        async_db_conn: AsyncMysqlDB = await _get_db_connection()
        utils.logger.debug(f"[add_new_content] æ•°æ®åº“è¿æ¥è·å–æˆåŠŸ")
        
        # æ˜ å°„å­—æ®µ
        mapped_data = map_platform_fields(platform, content_item)
        utils.logger.debug(f"[add_new_content] å­—æ®µæ˜ å°„å®Œæˆï¼Œæ˜ å°„åå­—æ®µæ•°: {len(mapped_data)}")
        
        # ğŸ†• æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œæ£€æŸ¥æ˜ å°„åçš„æ•°æ®
        utils.logger.info(f"[add_new_content] æ˜ å°„åæ•°æ®å­—æ®µ: {list(mapped_data.keys())}")
        utils.logger.info(f"[add_new_content] content_idæ˜¯å¦å­˜åœ¨: {'content_id' in mapped_data}")
        utils.logger.info(f"[add_new_content] content_idå€¼: {mapped_data.get('content_id', 'NOT_FOUND')}")
        
        # æ·»åŠ ä»»åŠ¡ID
        if task_id:
            mapped_data["task_id"] = task_id
            utils.logger.debug(f"[add_new_content] æ·»åŠ ä»»åŠ¡ID: {task_id}")
        
        # æ·»åŠ æ—¶é—´æˆ³
        now_ts = int(time.time() * 1000)
        if "add_ts" not in mapped_data:
            mapped_data["add_ts"] = now_ts
        if "last_modify_ts" not in mapped_data:
            mapped_data["last_modify_ts"] = now_ts
        utils.logger.debug(f"[add_new_content] æ·»åŠ æ—¶é—´æˆ³: {now_ts}")
        
        # åºåˆ—åŒ–æ•°æ®
        safe_item = serialize_for_db(mapped_data)
        utils.logger.debug(f"[add_new_content] æ•°æ®åºåˆ—åŒ–å®Œæˆï¼Œå­—æ®µæ•°: {len(safe_item)}")
        
        # è¿‡æ»¤å­—æ®µ
        safe_item = filter_fields_for_table(safe_item, UNIFIED_CONTENT_FIELDS)
        utils.logger.debug(f"[add_new_content] å­—æ®µè¿‡æ»¤å®Œæˆï¼Œæœ€ç»ˆå­—æ®µæ•°: {len(safe_item)}")
        
        # ğŸ†• æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œæ£€æŸ¥è¿‡æ»¤åçš„æ•°æ®
        utils.logger.info(f"[add_new_content] è¿‡æ»¤åæ•°æ®å­—æ®µ: {list(safe_item.keys())}")
        utils.logger.info(f"[add_new_content] è¿‡æ»¤åcontent_idæ˜¯å¦å­˜åœ¨: {'content_id' in safe_item}")
        utils.logger.info(f"[add_new_content] è¿‡æ»¤åcontent_idå€¼: {safe_item.get('content_id', 'NOT_FOUND')}")
        
        # æ’å…¥æ•°æ®åº“
        utils.logger.debug(f"[add_new_content] å¼€å§‹æ’å…¥æ•°æ®åº“ï¼Œè¡¨å: unified_content")
        last_row_id: int = await async_db_conn.item_to_table("unified_content", safe_item)
        utils.logger.debug(f"[add_new_content] æ•°æ®åº“æ’å…¥æˆåŠŸï¼Œè¿”å›ID: {last_row_id}")
        
        return last_row_id
        
    except Exception as e:
        utils.logger.error(f"æ–°å¢å†…å®¹å¤±è´¥: {platform}/{content_item.get('content_id', 'unknown')}, é”™è¯¯: {e}")
        import traceback
        utils.logger.error(f"[add_new_content] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise


async def update_content_by_content_id(platform: str, content_id: str, content_item: Dict) -> int:
    """æ›´æ–°å†…å®¹è®°å½•"""
    try:
        async_db_conn: AsyncMysqlDB = await _get_db_connection()
        
        # æ˜ å°„å­—æ®µ
        mapped_data = map_platform_fields(platform, content_item)
        
        # æ›´æ–°æ—¶é—´æˆ³
        mapped_data["last_modify_ts"] = int(time.time() * 1000)
        
        # ç¡®ä¿ä»»åŠ¡IDä¹Ÿè¢«æ›´æ–°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "task_id" in content_item:
            mapped_data["task_id"] = content_item["task_id"]
            utils.logger.debug(f"[update_content_by_content_id] æ›´æ–°ä»»åŠ¡ID: {content_item['task_id']}")
        
        # åºåˆ—åŒ–æ•°æ®
        safe_item = serialize_for_db(mapped_data)
        
        # è¿‡æ»¤å­—æ®µ
        safe_item = filter_fields_for_table(safe_item, UNIFIED_CONTENT_FIELDS)
        
        # æ›´æ–°æ•°æ®åº“ - ä½¿ç”¨å¤åˆæ¡ä»¶
        where_condition = f"platform = '{platform}' AND content_id = '{content_id}'"
        
        # æ„å»ºSETå­å¥
        set_clauses = []
        values = []
        for key, value in safe_item.items():
            set_clauses.append(f"`{key}` = %s")
            values.append(value)
        
        set_clause = ", ".join(set_clauses)
        sql = f"UPDATE unified_content SET {set_clause} WHERE {where_condition}"
        
        utils.logger.debug(f"[update_content_by_content_id] æ‰§è¡Œæ›´æ–°SQL: {sql}")
        utils.logger.debug(f"[update_content_by_content_id] æ›´æ–°å­—æ®µ: {list(safe_item.keys())}")
        
        result = await async_db_conn.execute(sql, *values)
        utils.logger.debug(f"[update_content_by_content_id] æ›´æ–°æˆåŠŸï¼Œå½±å“è¡Œæ•°: {result}")
        return result
        
    except Exception as e:
        utils.logger.error(f"æ›´æ–°å†…å®¹å¤±è´¥: {platform}/{content_id}, é”™è¯¯: {e}")
        import traceback
        utils.logger.error(f"[update_content_by_content_id] é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        raise


async def query_comment_by_comment_id(platform: str, comment_id: str) -> Dict:
    """æŸ¥è¯¢è¯„è®ºè®°å½•"""
    try:
        async_db_conn: AsyncMysqlDB = await _get_db_connection()
        sql = f"SELECT * FROM unified_comment WHERE platform = '{platform}' AND comment_id = '{comment_id}'"
        rows: List[Dict] = await async_db_conn.query(sql)
        if len(rows) > 0:
            return rows[0]
        return dict()
    except Exception as e:
        utils.logger.error(f"æŸ¥è¯¢è¯„è®ºå¤±è´¥: {platform}/{comment_id}, é”™è¯¯: {e}")
        return dict()


async def add_new_comment(platform: str, comment_item: Dict) -> int:
    """æ–°å¢è¯„è®ºè®°å½•"""
    try:
        async_db_conn: AsyncMysqlDB = await _get_db_connection()
        
        # æ˜ å°„å­—æ®µ
        mapped_data = map_platform_fields(platform, comment_item)
        
        # æ·»åŠ æ—¶é—´æˆ³
        now_ts = int(time.time() * 1000)
        if "add_ts" not in mapped_data:
            mapped_data["add_ts"] = now_ts
        if "last_modify_ts" not in mapped_data:
            mapped_data["last_modify_ts"] = now_ts
        
        # åºåˆ—åŒ–æ•°æ®
        safe_item = serialize_for_db(mapped_data)
        
        # è¿‡æ»¤å­—æ®µ
        safe_item = filter_fields_for_table(safe_item, UNIFIED_COMMENT_FIELDS)
        
        # æ’å…¥æ•°æ®åº“
        last_row_id: int = await async_db_conn.item_to_table("unified_comment", safe_item)
        return last_row_id
        
    except Exception as e:
        utils.logger.error(f"æ–°å¢è¯„è®ºå¤±è´¥: {platform}/{comment_item.get('comment_id', 'unknown')}, é”™è¯¯: {e}")
        raise


async def update_comment_by_comment_id(platform: str, comment_id: str, comment_item: Dict) -> int:
    """æ›´æ–°è¯„è®ºè®°å½•"""
    try:
        async_db_conn: AsyncMysqlDB = await _get_db_connection()
        
        # æ˜ å°„å­—æ®µ
        mapped_data = map_platform_fields(platform, comment_item)
        
        # æ›´æ–°æ—¶é—´æˆ³
        mapped_data["last_modify_ts"] = int(time.time() * 1000)
        
        # åºåˆ—åŒ–æ•°æ®
        safe_item = serialize_for_db(mapped_data)
        
        # è¿‡æ»¤å­—æ®µ
        safe_item = filter_fields_for_table(safe_item, UNIFIED_COMMENT_FIELDS)
        
        # æ›´æ–°æ•°æ®åº“ - ä½¿ç”¨å¤åˆæ¡ä»¶
        where_condition = f"platform = '{platform}' AND comment_id = '{comment_id}'"
        
        # æ„å»ºSETå­å¥
        set_clauses = []
        values = []
        for key, value in safe_item.items():
            set_clauses.append(f"`{key}` = %s")
            values.append(value)
        
        set_clause = ", ".join(set_clauses)
        sql = f"UPDATE unified_comment SET {set_clause} WHERE {where_condition}"
        
        result = await async_db_conn.execute(sql, *values)
        return result
        
    except Exception as e:
        utils.logger.error(f"æ›´æ–°è¯„è®ºå¤±è´¥: {platform}/{comment_id}, é”™è¯¯: {e}")
        raise


async def get_content_list(platform: str = None, task_id: str = None, page: int = 1, page_size: int = 20) -> Dict:
    """è·å–å†…å®¹åˆ—è¡¨"""
    try:
        async_db_conn: AsyncMysqlDB = await _get_db_connection()
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_conditions = {}
        if platform:
            where_conditions["platform"] = platform
        if task_id:
            where_conditions["task_id"] = task_id
        
        # æŸ¥è¯¢æ€»æ•°
        count_sql = "SELECT COUNT(*) as total FROM unified_content"
        if where_conditions:
            conditions = " AND ".join([f"{k} = '{v}'" for k, v in where_conditions.items()])
            count_sql += f" WHERE {conditions}"
        
        count_result = await async_db_conn.get_first(count_sql)
        total = count_result['total'] if count_result else 0
        
        # æŸ¥è¯¢æ•°æ®
        offset = (page - 1) * page_size
        query_sql = "SELECT * FROM unified_content"
        if where_conditions:
            conditions = " AND ".join([f"{k} = '{v}'" for k, v in where_conditions.items()])
            query_sql += f" WHERE {conditions}"
        query_sql += f" ORDER BY add_ts DESC LIMIT {page_size} OFFSET {offset}"
        
        results = await async_db_conn.query(query_sql)
        
        return {
            "total": total,
            "page": page,
            "page_size": page_size,
            "total_pages": (total + page_size - 1) // page_size,
            "items": results
        }
        
    except Exception as e:
        utils.logger.error(f"è·å–å†…å®¹åˆ—è¡¨å¤±è´¥: {e}")
        raise


async def get_comment_list(content_id: str = None, platform: str = None, page: int = 1, page_size: int = 20) -> Dict:
    """è·å–è¯„è®ºåˆ—è¡¨"""
    try:
        async_db_conn: AsyncMysqlDB = await _get_db_connection()
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_conditions = {}
        if content_id:
            where_conditions["content_id"] = content_id
        if platform:
            where_conditions["platform"] = platform
        
        # æŸ¥è¯¢æ€»æ•°
        count_sql = "SELECT COUNT(*) as total FROM unified_comment"
        if where_conditions:
            conditions = " AND ".join([f"{k} = '{v}'" for k, v in where_conditions.items()])
            count_sql += f" WHERE {conditions}"
        
        count_result = await async_db_conn.get_first(count_sql)
        total = count_result['total'] if count_result else 0
        
        # æŸ¥è¯¢æ•°æ®
        offset = (page - 1) * page_size
        query_sql = "SELECT * FROM unified_comment"
        if where_conditions:
            conditions = " AND ".join([f"{k} = '{v}'" for k, v in where_conditions.items()])
            query_sql += f" WHERE {conditions}"
        query_sql += f" ORDER BY add_ts DESC LIMIT {page_size} OFFSET {offset}"
        
        results = await async_db_conn.query(query_sql)
        
        return {
            "total": total,
            "page": page,
            "page_size": page_size,
            "total_pages": (total + page_size - 1) // page_size,
            "items": results
        }
        
    except Exception as e:
        utils.logger.error(f"è·å–è¯„è®ºåˆ—è¡¨å¤±è´¥: {e}")
        raise 