# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š  
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚  
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚  
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚  
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚   
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#   
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚  
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚  


# -*- coding: utf-8 -*-
# @Author  : relakkes@gmail.com
# @Time    : 2024/1/14 18:46
# @Desc    : æŠ–éŸ³å­˜å‚¨å®ç°ç±»
import asyncio
import csv
import json
import os
import pathlib
from typing import Dict, List

import aiofiles

import config
from base.base_crawler import AbstractStore
from tools import utils, words
from var import crawler_type_var


def calculate_number_of_files(file_store_path: str) -> int:
    """è®¡ç®—æ•°æ®ä¿å­˜æ–‡ä»¶çš„å‰éƒ¨åˆ†æ’åºæ•°å­—ï¼Œæ”¯æŒæ¯æ¬¡è¿è¡Œä»£ç ä¸å†™åˆ°åŒä¸€ä¸ªæ–‡ä»¶ä¸­
    Args:
        file_store_path;
    Returns:
        file nums
    """
    if not os.path.exists(file_store_path):
        return 1
    try:
        return max([int(file_name.split("_")[0]) for file_name in os.listdir(file_store_path)]) + 1
    except ValueError:
        return 1


class DouyinCsvStoreImplement(AbstractStore):
    csv_store_path: str = "data/douyin"
    file_count: int = calculate_number_of_files(csv_store_path)

    def make_save_file_name(self, store_type: str) -> str:
        """
        make save file name by store type
        Args:
            store_type: contents or comments

        Returns: eg: data/douyin/search_comments_20240114.csv ...

        """
        return f"{self.csv_store_path}/{self.file_count}_{crawler_type_var.get()}_{store_type}_{utils.get_current_date()}.csv"

    async def save_data_to_csv(self, save_item: Dict, store_type: str):
        """
        Below is a simple way to save it in CSV format.
        Args:
            save_item:  save content dict info
            store_type: Save type contains content and commentsï¼ˆcontents | commentsï¼‰

        Returns: no returns

        """
        pathlib.Path(self.csv_store_path).mkdir(parents=True, exist_ok=True)
        save_file_name = self.make_save_file_name(store_type=store_type)
        async with aiofiles.open(save_file_name, mode='a+', encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            if await f.tell() == 0:
                await writer.writerow(save_item.keys())
            await writer.writerow(save_item.values())

    async def store_content(self, content_item: Dict):
        """
        Douyin content CSV storage implementation
        Args:
            content_item: note item dict

        Returns:

        """
        await self.save_data_to_csv(save_item=content_item, store_type="contents")

    async def store_comment(self, comment_item: Dict):
        """
        Douyin comment CSV storage implementation
        Args:
            comment_item: comment item dict

        Returns:

        """
        await self.save_data_to_csv(save_item=comment_item, store_type="comments")

    async def store_creator(self, creator: Dict):
        """
        Douyin creator CSV storage implementation
        Args:
            creator: creator item dict

        Returns:

        """
        await self.save_data_to_csv(save_item=creator, store_type="creator")


class DouyinDbStoreImplement(AbstractStore):
    
    def _extract_video_download_url(self, aweme_detail: Dict) -> str:
        """
        æå–è§†é¢‘ä¸‹è½½åœ°å€

        Args:
            aweme_detail (Dict): æŠ–éŸ³è§†é¢‘

        Returns:
            str: è§†é¢‘ä¸‹è½½åœ°å€
        """
        # ä¼˜å…ˆä½¿ç”¨ download_addr
        video_item = aweme_detail.get("video", {})
        download_addr = video_item.get("download_addr", {})
        if download_addr and download_addr.get("url_list"):
            return download_addr["url_list"][0]
        
        # å¤‡ç”¨ play_addr
        play_addr = video_item.get("play_addr", {})
        if play_addr and play_addr.get("url_list"):
            return play_addr["url_list"][0]
        
        return ""
    
    def _extract_video_play_url(self, aweme_detail: Dict) -> str:
        """
        æå–è§†é¢‘æ’­æ”¾é¡µé“¾æ¥

        Args:
            aweme_detail (Dict): æŠ–éŸ³è§†é¢‘

        Returns:
            str: è§†é¢‘æ’­æ”¾é¡µé“¾æ¥
        """
        aweme_id = aweme_detail.get("aweme_id", "")
        if aweme_id:
            return f"https://www.douyin.com/video/{aweme_id}"
        return ""
    
    def _extract_content_cover_url(self, aweme_detail: Dict) -> str:
        """
        æå–è§†é¢‘å°é¢åœ°å€

        Args:
            aweme_detail (Dict): æŠ–éŸ³å†…å®¹è¯¦æƒ…

        Returns:
            str: è§†é¢‘å°é¢åœ°å€
        """
        video_item = aweme_detail.get("video", {})
        
        # ä¼˜å…ˆä½¿ç”¨ cover
        cover = video_item.get("cover", {})
        if cover and cover.get("url_list"):
            return cover["url_list"][0]
        
        # å¤‡ç”¨ origin_cover
        origin_cover = video_item.get("origin_cover", {})
        if origin_cover and origin_cover.get("url_list"):
            return origin_cover["url_list"][0]
        
        return ""
    
    def _extract_author_info(self, aweme_detail: Dict) -> Dict:
        """
        æå–ä½œè€…ä¿¡æ¯

        Args:
            aweme_detail (Dict): æŠ–éŸ³å†…å®¹è¯¦æƒ…

        Returns:
            Dict: ä½œè€…ä¿¡æ¯
        """
        author = aweme_detail.get("author", {})
        return {
            "author_id": author.get("uid", ""),
            "author_name": author.get("nickname", ""),
            "author_nickname": author.get("nickname", ""),
            "author_avatar": author.get("avatar_thumb", {}).get("url_list", [""])[0] if author.get("avatar_thumb") else "",
            "author_signature": author.get("signature", ""),
            "author_unique_id": author.get("unique_id", ""),
            "author_sec_uid": author.get("sec_uid", ""),
            "author_short_id": author.get("short_id", "")
        }
    
    def _extract_video_info(self, aweme_detail: Dict) -> Dict:
        """
        æå–è§†é¢‘ä¿¡æ¯

        Args:
            aweme_detail (Dict): æŠ–éŸ³å†…å®¹è¯¦æƒ…

        Returns:
            Dict: è§†é¢‘ä¿¡æ¯
        """
        video_item = aweme_detail.get("video", {})
        return {
            "video_url": self._extract_video_download_url(aweme_detail),
            "video_download_url": self._extract_video_download_url(aweme_detail),
            "video_play_url": self._extract_video_play_url(aweme_detail),
            "cover_url": self._extract_content_cover_url(aweme_detail),
            "file_size": video_item.get("data_size", 0),
            "duration": video_item.get("duration", 0)
        }
    
    def _flatten_douyin_data(self, content_item: Dict) -> Dict:
        """
        å°†æŠ–éŸ³åŸå§‹æ•°æ®æ‰å¹³åŒ–ä¸ºç»Ÿä¸€è¡¨ç»“æ„

        Args:
            content_item (Dict): æŠ–éŸ³åŸå§‹æ•°æ®

        Returns:
            Dict: æ‰å¹³åŒ–åçš„æ•°æ®
        """
        # ğŸ†• ä¿®å¤ï¼šç¡®ä¿source_keywordæ­£ç¡®ä¼ é€’
        source_keyword = content_item.get("source_keyword", "")
        if not source_keyword:
            # å¦‚æœæ²¡æœ‰ç›´æ¥ä¼ é€’ï¼Œå°è¯•ä»å…¨å±€å˜é‡è·å–
            from var import source_keyword_var
            source_keyword = source_keyword_var.get()
        
        # åŸºç¡€ä¿¡æ¯
        flattened = {
            "content_id": content_item.get("aweme_id", ""),
            "platform": "douyin",
            "content_type": "video",
            "task_id": content_item.get("task_id", ""),
            "source_keyword": source_keyword,  # ğŸ†• ä¿®å¤ï¼šç¡®ä¿source_keywordæ­£ç¡®è®¾ç½®
            
            # å†…å®¹ä¿¡æ¯
            "title": content_item.get("desc", ""),
            "description": content_item.get("desc", ""),
            "content": content_item.get("desc", ""),
            # ğŸ†• ä¿®å¤ï¼šå°†10ä½æ—¶é—´æˆ³è½¬æ¢ä¸º13ä½æ—¶é—´æˆ³
            "create_time": content_item.get("create_time", 0) * 1000 if content_item.get("create_time", 0) < 1000000000000 else content_item.get("create_time", 0),
            "publish_time": content_item.get("create_time", 0) * 1000 if content_item.get("create_time", 0) < 1000000000000 else content_item.get("create_time", 0),
            "update_time": content_item.get("create_time", 0) * 1000 if content_item.get("create_time", 0) < 1000000000000 else content_item.get("create_time", 0),
            
            # ç»Ÿè®¡ä¿¡æ¯
            "like_count": content_item.get("statistics", {}).get("digg_count", 0),
            "comment_count": content_item.get("statistics", {}).get("comment_count", 0),
            "share_count": content_item.get("statistics", {}).get("share_count", 0),
            "collect_count": content_item.get("statistics", {}).get("collect_count", 0),
            "view_count": content_item.get("statistics", {}).get("play_count", 0),
            
            # çŠ¶æ€ä¿¡æ¯
            "is_favorite": content_item.get("is_favorite", False),
            "is_deleted": content_item.get("is_deleted", False),
            "is_private": content_item.get("is_private", False),
            "is_original": content_item.get("is_original", False),
            
            # å­˜å‚¨ä¿¡æ¯
            "storage_type": "url_only",
            "raw_data": json.dumps(content_item, ensure_ascii=False),
            
            # æ—¶é—´æˆ³
            "add_ts": utils.get_current_timestamp(),
            "last_modify_ts": utils.get_current_timestamp(),
            
            # ä½œè€…ä¿¡æ¯
            "author_id": content_item.get("author", {}).get("uid", ""),
            "author_name": content_item.get("author", {}).get("nickname", ""),
            "author_nickname": content_item.get("author", {}).get("nickname", ""),
            "author_avatar": content_item.get("author", {}).get("avatar_thumb", {}).get("url_list", [""])[0] if content_item.get("author", {}).get("avatar_thumb") else "",
            "author_signature": content_item.get("author", {}).get("signature", ""),
            "author_unique_id": content_item.get("author", {}).get("unique_id", ""),
            "author_sec_uid": content_item.get("author", {}).get("sec_uid", ""),
            "author_short_id": content_item.get("author", {}).get("short_id", ""),
            
            # åª’ä½“ä¿¡æ¯
            "cover_url": self._extract_content_cover_url(content_item),
            "video_url": self._extract_video_play_url(content_item),
            "video_download_url": self._extract_video_download_url(content_item),
            "video_play_url": self._extract_video_play_url(content_item),
            "video_share_url": self._extract_video_play_url(content_item),
            
            # ä½ç½®ä¿¡æ¯
            "ip_location": content_item.get("ip_location", ""),
            "location": content_item.get("location", ""),
            
            # æ ‡ç­¾å’Œåˆ†ç±»
            "tags": json.dumps(content_item.get("tag_list", []), ensure_ascii=False),
            "categories": json.dumps([], ensure_ascii=False),
            "topics": json.dumps(content_item.get("cha_list", []), ensure_ascii=False),
            
            # æ‰©å±•ä¿¡æ¯
            "metadata": json.dumps({}, ensure_ascii=False),
            "extra_info": json.dumps({}, ensure_ascii=False)
        }
        
        return flattened

    async def store_content(self, content_item: Dict):
        """
        Douyin content DB storage implementation
        Args:
            content_item: content item dict

        Returns:

        """
        from .douyin_store_sql import (add_new_content,
                                       query_content_by_content_id,
                                       update_content_by_content_id)
        
        # æ‰å¹³åŒ–æ•°æ®
        flattened_data = self._flatten_douyin_data(content_item)
        content_id = flattened_data.get("content_id")
        
        if not content_id:
            utils.logger.error("å†…å®¹IDä¸ºç©ºï¼Œè·³è¿‡å­˜å‚¨")
            return
        
        # æŸ¥è¯¢æ˜¯å¦å·²å­˜åœ¨
        existing_content: Dict = await query_content_by_content_id(content_id=content_id)
        task_id = content_item.get("task_id")
        
        if not existing_content:
            # æ–°å¢å†…å®¹
            await add_new_content(flattened_data, task_id=task_id)
            utils.logger.info(f"âœ… æ–°å¢æŠ–éŸ³å†…å®¹: {content_id}")
        else:
            # æ›´æ–°å†…å®¹
            await update_content_by_content_id(content_id, content_item=flattened_data)
            utils.logger.info(f"âœ… æ›´æ–°æŠ–éŸ³å†…å®¹: {content_id}")

    async def store_comment(self, comment_item: Dict):
        """
        Douyin content DB storage implementation
        Args:
            comment_item: comment item dict

        Returns:

        """
        from .douyin_store_sql import (add_new_comment,
                                       query_comment_by_comment_id,
                                       update_comment_by_comment_id)
        
        # æ‰å¹³åŒ–è¯„è®ºæ•°æ®
        flattened_comment = {
            "comment_id": comment_item.get("cid", ""),
            "content_id": comment_item.get("aweme_id", ""),
            "platform": "douyin",
            "content": comment_item.get("text", ""),
            "text": comment_item.get("text", ""),
            "author_id": comment_item.get("user", {}).get("uid", ""),
            "author_name": comment_item.get("user", {}).get("nickname", ""),
            "author_nickname": comment_item.get("user", {}).get("nickname", ""),
            "author_avatar": comment_item.get("user", {}).get("avatar_thumb", {}).get("url_list", [""])[0] if comment_item.get("user", {}).get("avatar_thumb") else "",
            "like_count": comment_item.get("digg_count", 0),
            "reply_count": comment_item.get("reply_comment_total", 0),
            "create_time": comment_item.get("create_time", 0),
            "is_deleted": comment_item.get("is_deleted", False),
            "is_hidden": comment_item.get("is_hidden", False),
            "is_top": comment_item.get("is_top", False),
            "raw_data": json.dumps(comment_item, ensure_ascii=False),
            "add_ts": utils.get_current_timestamp(),
            "last_modify_ts": utils.get_current_timestamp()
        }
        
        comment_id = flattened_comment.get("comment_id")
        if not comment_id:
            utils.logger.error("è¯„è®ºIDä¸ºç©ºï¼Œè·³è¿‡å­˜å‚¨")
            return
        
        comment_detail: Dict = await query_comment_by_comment_id(comment_id=comment_id)
        if not comment_detail:
            await add_new_comment(flattened_comment)
            utils.logger.info(f"âœ… æ–°å¢æŠ–éŸ³è¯„è®º: {comment_id}")
        else:
            await update_comment_by_comment_id(comment_id, comment_item=flattened_comment)
            utils.logger.info(f"âœ… æ›´æ–°æŠ–éŸ³è¯„è®º: {comment_id}")

    async def store_creator(self, creator: Dict):
        """
        Douyin content DB storage implementation
        Args:
            creator: creator dict

        Returns:

        """
        from .douyin_store_sql import (add_new_creator,
                                       query_creator_by_user_id,
                                       update_creator_by_user_id)
        user_id = creator.get("user_id")
        user_detail: Dict = await query_creator_by_user_id(user_id)
        if not user_detail:
            creator["add_ts"] = utils.get_current_timestamp()
            await add_new_creator(creator)
        else:
            await update_creator_by_user_id(user_id, creator)

class DouyinJsonStoreImplement(AbstractStore):
    json_store_path: str = "data/douyin/json"
    words_store_path: str = "data/douyin/words"

    lock = asyncio.Lock()
    file_count: int = calculate_number_of_files(json_store_path)
    WordCloud = words.AsyncWordCloudGenerator()

    def make_save_file_name(self, store_type: str) -> (str,str):
        """
        make save file name by store type
        Args:
            store_type: Save type contains content and commentsï¼ˆcontents | commentsï¼‰

        Returns:

        """

        return (
            f"{self.json_store_path}/{crawler_type_var.get()}_{store_type}_{utils.get_current_date()}.json",
            f"{self.words_store_path}/{crawler_type_var.get()}_{store_type}_{utils.get_current_date()}"
        )
    async def save_data_to_json(self, save_item: Dict, store_type: str):
        """
        Below is a simple way to save it in json format.
        Args:
            save_item: save content dict info
            store_type: Save type contains content and commentsï¼ˆcontents | commentsï¼‰

        Returns:

        """
        pathlib.Path(self.json_store_path).mkdir(parents=True, exist_ok=True)
        pathlib.Path(self.words_store_path).mkdir(parents=True, exist_ok=True)
        save_file_name,words_file_name_prefix = self.make_save_file_name(store_type=store_type)
        save_data = []

        async with self.lock:
            if os.path.exists(save_file_name):
                async with aiofiles.open(save_file_name, 'r', encoding='utf-8') as file:
                    save_data = json.loads(await file.read())

            save_data.append(save_item)
            async with aiofiles.open(save_file_name, 'w', encoding='utf-8') as file:
                await file.write(json.dumps(save_data, ensure_ascii=False))

            if config.ENABLE_GET_COMMENTS and config.ENABLE_GET_WORDCLOUD:
                try:
                    await self.WordCloud.generate_word_frequency_and_cloud(save_data, words_file_name_prefix)
                except:
                    pass

    async def store_content(self, content_item: Dict):
        """
        content JSON storage implementation
        Args:
            content_item:

        Returns:

        """
        await self.save_data_to_json(content_item, "contents")

    async def store_comment(self, comment_item: Dict):
        """
        comment JSON storage implementation
        Args:
            comment_item:

        Returns:

        """
        await self.save_data_to_json(comment_item, "comments")


    async def store_creator(self, creator: Dict):
        """
        Douyin creator CSV storage implementation
        Args:
            creator: creator item dict

        Returns:

        """
        await self.save_data_to_json(save_item=creator, store_type="creator")


class DouyinRedisStoreImplement(AbstractStore):
    """æŠ–éŸ³Rediså­˜å‚¨å®ç°"""
    
    def __init__(self, redis_callback=None):
        self.redis_callback = redis_callback
        self.collected_data = []
    
    def clear_collected_data(self):
        """æ¸…ç©ºæ”¶é›†çš„æ•°æ®ï¼Œç”¨äºæ–°çš„çˆ¬å–ä»»åŠ¡"""
        self.collected_data.clear()
        utils.logger.info("[DouyinRedisStore] å·²æ¸…ç©ºæ”¶é›†çš„æ•°æ®ï¼Œå‡†å¤‡æ–°çš„çˆ¬å–ä»»åŠ¡")  # æ”¶é›†çˆ¬å–åˆ°çš„æ•°æ®
    
    def set_redis_callback(self, callback):
        """è®¾ç½®Rediså›è°ƒå‡½æ•°"""
        self.redis_callback = callback
    
    def _extract_video_download_url(self, aweme_detail: Dict) -> str:
        """
        æå–è§†é¢‘ä¸‹è½½åœ°å€

        Args:
            aweme_detail (Dict): æŠ–éŸ³è§†é¢‘

        Returns:
            str: è§†é¢‘ä¸‹è½½åœ°å€
        """
        # ä¼˜å…ˆä½¿ç”¨ download_addr
        video_item = aweme_detail.get("video", {})
        download_addr = video_item.get("download_addr", {})
        if download_addr and download_addr.get("url_list"):
            return download_addr["url_list"][0]
        
        # å¤‡ç”¨ play_addr
        play_addr = video_item.get("play_addr", {})
        if play_addr and play_addr.get("url_list"):
            return play_addr["url_list"][0]
        
        return ""
    
    def _extract_video_play_url(self, aweme_detail: Dict) -> str:
        """
        æå–è§†é¢‘æ’­æ”¾é¡µé“¾æ¥

        Args:
            aweme_detail (Dict): æŠ–éŸ³è§†é¢‘

        Returns:
            str: è§†é¢‘æ’­æ”¾é¡µé“¾æ¥
        """
        aweme_id = aweme_detail.get("aweme_id", "")
        if aweme_id:
            return f"https://www.douyin.com/video/{aweme_id}"
            return ""
    
    def _extract_content_cover_url(self, aweme_detail: Dict) -> str:
        """
        æå–è§†é¢‘å°é¢åœ°å€

        Args:
            aweme_detail (Dict): æŠ–éŸ³å†…å®¹è¯¦æƒ…

        Returns:
            str: è§†é¢‘å°é¢åœ°å€
        """
        video_item = aweme_detail.get("video", {})
        
        # ä¼˜å…ˆä½¿ç”¨ cover
        cover = video_item.get("cover", {})
        if cover and cover.get("url_list"):
            return cover["url_list"][0]
        
        # å¤‡ç”¨ origin_cover
        origin_cover = video_item.get("origin_cover", {})
        if origin_cover and origin_cover.get("url_list"):
            return origin_cover["url_list"][0]

        return ""
    
    async def store_content(self, content_item: Dict):
        """
        æŠ–éŸ³å†…å®¹Rediså­˜å‚¨å®ç°ï¼ˆæ ‡å‡†åŒ–å­—æ®µï¼‰
        Args:
            content_item: è§†é¢‘å†…å®¹å­—å…¸
        """
        # å…¼å®¹ä¸åŒå­—æ®µæ¥æº
        aweme_id = content_item.get("aweme_id") or content_item.get("video_id")
        user_info = content_item.get("author", {})
        interact_info = content_item.get("statistics", {})

        processed_content = {
            "aweme_id": aweme_id,
            "aweme_url": self._extract_video_play_url(content_item),  # å¿…é¡»
            "download_url": self._extract_video_download_url(content_item),  # æœ‰åˆ™å­˜
            "cover_url": self._extract_content_cover_url(content_item),
            "title": content_item.get("desc", "") or content_item.get("title", ""),
            "author_id": user_info.get("uid") or content_item.get("author_id", ""),
            "author_name": user_info.get("nickname") or content_item.get("author_name", ""),
            "avatar": user_info.get("avatar_thumb", {}).get("url_list", [""])[0] if user_info.get("avatar_thumb") else "",
            "create_time": content_item.get("create_time"),
            "liked_count": str(interact_info.get("digg_count") or content_item.get("liked_count", "")),
            "comment_count": str(interact_info.get("comment_count") or content_item.get("comment_count", "")),
            "collected_count": str(interact_info.get("collect_count") or content_item.get("collected_count", "")),
            "share_count": str(interact_info.get("share_count") or content_item.get("share_count", "")),
            "platform": "dy",
            "source_keyword": content_item.get("source_keyword", ""),
            "task_id": content_item.get("task_id", ""),
            "stored_at": content_item.get("stored_at", ""),
        }

        # ğŸ†• ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒIDçš„æ•°æ®ï¼Œé¿å…é‡å¤å­˜å‚¨
        existing_ids = [item.get("aweme_id") for item in self.collected_data]
        if aweme_id not in existing_ids:
            # æ”¶é›†æ•°æ®ç”¨äºè¿”å›
            self.collected_data.append(processed_content)
            utils.logger.debug(f"ğŸ†• [DouyinRedisStore] æ–°å¢æ•°æ®: {aweme_id}")
        else:
            utils.logger.debug(f"ğŸ†• [DouyinRedisStore] è·³è¿‡é‡å¤æ•°æ®: {aweme_id}")

        # æ—¥å¿—è¾“å‡º
        utils.logger.info(f"ğŸ¬ [DouyinRedisStore] è§†é¢‘ID: {aweme_id}, æ ‡é¢˜: {processed_content.get('title')}")
        utils.logger.info(f"ğŸ”— [DouyinRedisStore] æ’­æ”¾é¡µé“¾æ¥: {processed_content.get('aweme_url')}")
        utils.logger.info(f"ğŸ“¥ [DouyinRedisStore] ä¸‹è½½é“¾æ¥: {processed_content.get('download_url')}")

        # åŒæ—¶å­˜å‚¨åˆ°æ•°æ®åº“
        try:
            from .douyin_store_sql import (add_new_content,
                                           query_content_by_content_id,
                                           update_content_by_content_id)
            
            aweme_detail: Dict = await query_content_by_content_id(content_id=aweme_id)
            task_id = content_item.get("task_id")
            if not aweme_detail:
                # ä½¿ç”¨å¤„ç†è¿‡çš„æ•°æ®å­˜å‚¨åˆ°æ•°æ®åº“ï¼Œç¡®ä¿URLæ­£ç¡®
                db_content_item = content_item.copy()
                db_content_item.update({
                    "aweme_url": processed_content.get("aweme_url"),
                    "download_url": processed_content.get("download_url"),
                    "video_download_url": processed_content.get("download_url"),  # ç¡®ä¿æ˜ å°„åˆ°video_download_urlå­—æ®µ
                    "video_url": processed_content.get("aweme_url"),  # ç¡®ä¿æ˜ å°„åˆ°video_urlå­—æ®µ
                    "video_play_url": processed_content.get("aweme_url"),  # ç¡®ä¿æ˜ å°„åˆ°video_play_urlå­—æ®µ
                    "add_ts": utils.get_current_timestamp()
                })
                if db_content_item.get("title") or db_content_item.get("desc"):
                    await add_new_content(db_content_item, task_id=task_id)
                    utils.logger.info(f"âœ… [DouyinRedisStore] æ•°æ®å·²å­˜å‚¨åˆ°æ•°æ®åº“: {aweme_id}")
            else:
                # æ›´æ–°æ—¶ä¹Ÿä½¿ç”¨å¤„ç†è¿‡çš„URL
                update_content_item = content_item.copy()
                update_content_item.update({
                    "aweme_url": processed_content.get("aweme_url"),
                    "download_url": processed_content.get("download_url"),
                    "video_download_url": processed_content.get("download_url"),  # ç¡®ä¿æ˜ å°„åˆ°video_download_urlå­—æ®µ
                    "video_url": processed_content.get("aweme_url"),  # ç¡®ä¿æ˜ å°„åˆ°video_urlå­—æ®µ
                    "video_play_url": processed_content.get("aweme_url"),  # ç¡®ä¿æ˜ å°„åˆ°video_play_urlå­—æ®µ
                })
                await update_content_by_content_id(aweme_id, content_item=update_content_item)
                utils.logger.info(f"âœ… [DouyinRedisStore] æ•°æ®å·²æ›´æ–°åˆ°æ•°æ®åº“: {aweme_id}")
        except Exception as e:
            utils.logger.error(f"âŒ [DouyinRedisStore] æ•°æ®åº“å­˜å‚¨å¤±è´¥: {aweme_id}, é”™è¯¯: {e}")

        # å­˜å‚¨åˆ°Redis
        if self.redis_callback:
            await self.redis_callback("dy", processed_content, "video")

    async def store_comment(self, comment_item: Dict):
        """
        æŠ–éŸ³è¯„è®ºRediså­˜å‚¨å®ç°
        Args:
            comment_item: è¯„è®ºå­—å…¸
        """
        if self.redis_callback:
            await self.redis_callback("dy", comment_item, "comment")

    async def store_creator(self, creator: Dict):
        """
        æŠ–éŸ³åˆ›ä½œè€…Rediså­˜å‚¨å®ç°
        Args:
            creator: åˆ›ä½œè€…å­—å…¸
        """
        if self.redis_callback:
            await self.redis_callback("dy", creator, "creator")

    async def save_creator(self, user_id: str, creator: Dict):
        """
        ä¿å­˜æŠ–éŸ³åˆ›ä½œè€… - å…¼å®¹æ€§æ–¹æ³•
        Args:
            user_id: ç”¨æˆ·ID
            creator: åˆ›ä½œè€…æ•°æ®
        """
        try:
            # æ„å»ºç»Ÿä¸€æ ¼å¼çš„åˆ›ä½œè€…æ•°æ®
            unified_creator = {
                "creator_id": user_id,
                "platform": "dy",
                "author_id": user_id,
                "author_name": creator.get('nickname', ''),
                "author_nickname": creator.get('nickname', ''),
                "author_avatar": creator.get('avatar_thumb', {}).get('url_list', [''])[0] if creator.get('avatar_thumb') else '',
                "author_signature": creator.get('signature', ''),
                "gender": creator.get('gender', ''),
                "ip_location": creator.get('location', ''),
                "follows": creator.get('following_count', 0),
                "fans": creator.get('follower_count', 0),
                "interaction": creator.get('total_favorited', 0),
                "tags": json.dumps(creator.get('tags', {}), ensure_ascii=False),
                "raw_data": json.dumps(creator, ensure_ascii=False),
                "add_ts": utils.get_current_timestamp(),
                "last_modify_ts": utils.get_current_timestamp()
            }
            
            # è°ƒç”¨store_creatoræ–¹æ³•
            await self.store_creator(unified_creator)
            
            utils.logger.info(f"âœ… [DouyinRedisStore] åˆ›ä½œè€…æ•°æ®å·²ä¿å­˜: {user_id}")
            
        except Exception as e:
            utils.logger.error(f"âŒ [DouyinRedisStore] ä¿å­˜åˆ›ä½œè€…æ•°æ®å¤±è´¥: {user_id}, é”™è¯¯: {e}")

    async def get_all_content(self) -> List[Dict]:
        """
        è·å–æ‰€æœ‰å­˜å‚¨çš„å†…å®¹
        Returns:
            List[Dict]: å†…å®¹åˆ—è¡¨
        """
        utils.logger.info(f"[DouyinRedisStore] è·å–å­˜å‚¨å†…å®¹ - å…±æ”¶é›†åˆ° {len(self.collected_data)} æ¡æ•°æ®")
        return self.collected_data