# å£°æ˜ï¼šæœ¬ä»£ç ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ç›®çš„ä½¿ç”¨ã€‚ä½¿ç”¨è€…åº”éµå®ˆä»¥ä¸‹åŸåˆ™ï¼š  
# 1. ä¸å¾—ç”¨äºä»»ä½•å•†ä¸šç”¨é€”ã€‚  
# 2. ä½¿ç”¨æ—¶åº”éµå®ˆç›®æ ‡å¹³å°çš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™ã€‚  
# 3. ä¸å¾—è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–æˆ–å¯¹å¹³å°é€ æˆè¿è¥å¹²æ‰°ã€‚  
# 4. åº”åˆç†æ§åˆ¶è¯·æ±‚é¢‘ç‡ï¼Œé¿å…ç»™ç›®æ ‡å¹³å°å¸¦æ¥ä¸å¿…è¦çš„è´Ÿæ‹…ã€‚   
# 5. ä¸å¾—ç”¨äºä»»ä½•éæ³•æˆ–ä¸å½“çš„ç”¨é€”ã€‚
#   
# è¯¦ç»†è®¸å¯æ¡æ¬¾è¯·å‚é˜…é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„LICENSEæ–‡ä»¶ã€‚  
# ä½¿ç”¨æœ¬ä»£ç å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä¸Šè¿°åŸåˆ™å’ŒLICENSEä¸­çš„æ‰€æœ‰æ¡æ¬¾ã€‚  

import asyncio
import json
import os
import sys
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
import json

import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks, Request
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.exceptions import RequestValidationError
from pydantic import BaseModel, Field
import random
import time

# å¯¼å…¥æ•°æ®æ¨¡å‹
from models.content_models import (
    ContentType, Platform, UnifiedContent, ContentListRequest, ContentListResponse,
    CrawlerRequest, MultiPlatformCrawlerRequest, CrawlerResponse, TaskStatusResponse,
    MultiPlatformTaskStatusResponse, UnifiedResultResponse, PLATFORM_MAPPING, SUPPORTED_PLATFORMS
)

import db
import config  # å¯¼å…¥é…ç½®æ¨¡å—
from base.base_crawler import AbstractCrawler
from db_init import DatabaseInitializer
from config.env_config_loader import config_loader
from tools import utils

# å¯¼å…¥å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
from utils.scheduler import start_scheduler, stop_scheduler, get_scheduler_status
from utils.db_utils import (
    get_account_list_by_platform, 
    check_token_validity,
    cleanup_expired_tokens
)

# å¯¼å…¥Redisç®¡ç†å™¨
from utils.redis_manager import store_crawler_result, redis_manager

# å¯¼å…¥æ–°çš„APIè·¯ç”±
from api.routes import api_router

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨æ•°æ®åº“åˆå§‹åŒ–å‰å¯¼å…¥
# from media_platform.bilibili import BilibiliCrawler
# from media_platform.douyin import DouYinCrawler
# from media_platform.kuaishou import KuaishouCrawler
# from media_platform.tieba import TieBaCrawler
# from media_platform.weibo import WeiboCrawler
# from media_platform.xhs import XiaoHongShuCrawler
# from media_platform.zhihu import ZhihuCrawler
# from proxy import proxy_router, ProxyManager


# å»¶è¿Ÿå¯¼å…¥å¤šå¹³å°æŠ“å–åŠŸèƒ½
multi_platform_crawler = None

# å»¶è¿Ÿå¯¼å…¥ä»£ç†ç®¡ç†å™¨
proxy_manager = None

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="MediaCrawler API",
    description="å¤šå¹³å°åª’ä½“å†…å®¹çˆ¬è™«APIæœåŠ¡",
    version="1.0.0"
)

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
app.mount("/static", StaticFiles(directory="static"), name="static")


# æ·»åŠ è¯·æ±‚éªŒè¯é”™è¯¯å¤„ç†å™¨
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """å¤„ç†è¯·æ±‚å‚æ•°éªŒè¯é”™è¯¯"""
    utils.logger.error("=" * 80)
    utils.logger.error("[VALIDATION_ERROR] FastAPIå‚æ•°éªŒè¯å¤±è´¥")
    utils.logger.error(f"[VALIDATION_ERROR] è¯·æ±‚URL: {request.url}")
    utils.logger.error(f"[VALIDATION_ERROR] è¯·æ±‚æ–¹æ³•: {request.method}")
    
    # è·å–è¯·æ±‚ä½“ï¼ˆå¦‚æœæ˜¯POSTè¯·æ±‚ï¼‰
    try:
        if request.method in ["POST", "PUT", "PATCH"]:
            body = await request.body()
            if body:
                utils.logger.error(f"[VALIDATION_ERROR] è¯·æ±‚ä½“: {body.decode('utf-8')}")
    except Exception as e:
        utils.logger.error(f"[VALIDATION_ERROR] æ— æ³•è¯»å–è¯·æ±‚ä½“: {e}")
    
    # è¯¦ç»†è®°å½•éªŒè¯é”™è¯¯
    utils.logger.error(f"[VALIDATION_ERROR] éªŒè¯é”™è¯¯è¯¦æƒ…:")
    for i, error in enumerate(exc.errors()):
        utils.logger.error(f"  é”™è¯¯ {i+1}:")
        utils.logger.error(f"    - ä½ç½®: {error['loc']}")
        utils.logger.error(f"    - æ¶ˆæ¯: {error['msg']}")
        utils.logger.error(f"    - ç±»å‹: {error['type']}")
        if 'input' in error:
            utils.logger.error(f"    - è¾“å…¥å€¼: {error['input']}")
    
    utils.logger.error("=" * 80)
    
    # æ„é€ å‹å¥½çš„é”™è¯¯æ¶ˆæ¯
    error_details = []
    for error in exc.errors():
        field_path = " -> ".join(str(loc) for loc in error['loc'])
        error_details.append(f"{field_path}: {error['msg']}")
    
    return JSONResponse(
        status_code=422,
        content={
            "error": "è¯·æ±‚å‚æ•°éªŒè¯å¤±è´¥",
            "details": error_details,
            "validation_errors": exc.errors()
        }
    )


# æ•°æ®åº“åˆå§‹åŒ–çŠ¶æ€
db_initialized = False

# å»¶è¿Ÿæ³¨å†Œè·¯ç”±ï¼Œåœ¨æ•°æ®åº“åˆå§‹åŒ–åæ³¨å†Œ
# app.include_router(proxy_router)
# app.include_router(login_router)
# app.include_router(account_router)
# app.include_router(login_management_router)

# ä»»åŠ¡çŠ¶æ€å­˜å‚¨
task_status = {}

# ğŸ†• ä»»åŠ¡ç®¡ç†ç›¸å…³å‡½æ•°
async def create_task_record(task_id: str, request: CrawlerRequest):
    """åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“"""
    try:
        # æ„å»ºä»»åŠ¡å‚æ•°JSON
        task_params = {
            "platform": request.platform,
            "keywords": request.keywords,
            "max_notes_count": request.max_notes_count,
            "account_id": request.account_id,
            "session_id": request.session_id,
            "login_type": request.login_type,
            "crawler_type": request.crawler_type,
            "get_comments": request.get_comments,
            "save_data_option": request.save_data_option,
            "use_proxy": request.use_proxy,
            "proxy_strategy": request.proxy_strategy
        }
        
        # æ’å…¥ä»»åŠ¡è®°å½•
        sql = """
        INSERT INTO crawler_tasks (
            id, platform, task_type, keywords, status, progress, 
            user_id, params, priority, is_favorite, deleted, is_pinned,
            ip_address, user_security_id, user_signature,
            created_at, updated_at
        ) VALUES (
            %s, %s, %s, %s, %s, %s, 
            %s, %s, %s, %s, %s, %s,
            %s, %s, %s,
            NOW(), NOW()
        )
        """
        
        await db.execute(sql, (
            task_id,
            request.platform,
            "single_platform",
            request.keywords,
            "pending",
            0.0,
            None,  # user_id
            json.dumps(task_params),
            0,  # priority
            False,  # is_favorite
            False,  # deleted
            False,  # is_pinned
            None,  # ip_address
            None,  # user_security_id
            None,  # user_signature
        ))
        
        utils.logger.info(f"[TASK_RECORD] ä»»åŠ¡è®°å½•åˆ›å»ºæˆåŠŸ: {task_id}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_RECORD] åˆ›å»ºä»»åŠ¡è®°å½•å¤±è´¥: {e}")
        raise

async def save_video_to_database(platform: str, video_data: Dict, task_id: str):
    """ä¿å­˜è§†é¢‘æ•°æ®åˆ°æ•°æ®åº“"""
    try:
        if platform == "dy":  # æŠ–éŸ³
            # æ„å»ºæŠ–éŸ³è§†é¢‘æ•°æ®
            aweme_data = {
                "aweme_id": video_data.get("aweme_id", ""),
                "aweme_type": video_data.get("aweme_type", "0"),
                "title": video_data.get("title", ""),
                "desc": video_data.get("desc", ""),
                "create_time": video_data.get("create_time", int(time.time())),
                "user_id": video_data.get("user_id", ""),
                "sec_uid": video_data.get("sec_uid", ""),
                "short_user_id": video_data.get("short_user_id", ""),
                "user_unique_id": video_data.get("user_unique_id", ""),
                "nickname": video_data.get("nickname", ""),
                "avatar": video_data.get("avatar", ""),
                "user_signature": video_data.get("user_signature", ""),
                "ip_location": video_data.get("ip_location", ""),
                "liked_count": str(video_data.get("liked_count", 0)),
                "comment_count": str(video_data.get("comment_count", 0)),
                "share_count": str(video_data.get("share_count", 0)),
                "collected_count": str(video_data.get("collected_count", 0)),
                "aweme_url": video_data.get("aweme_url", ""),
                "cover_url": video_data.get("cover_url", ""),
                "video_download_url": video_data.get("video_url", ""),
                "video_play_url": video_data.get("video_play_url", ""),
                "video_share_url": video_data.get("video_share_url", ""),
                "is_favorite": False,
                "minio_url": None,
                "task_id": task_id,
                "source_keyword": video_data.get("source_keyword", ""),
                "add_ts": int(time.time()),
                "last_modify_ts": int(time.time())
            }
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            check_sql = "SELECT id FROM douyin_aweme WHERE aweme_id = %s"
            existing = await db.query(check_sql, aweme_data["aweme_id"])
            
            if existing:
                # æ›´æ–°ç°æœ‰è®°å½•
                update_sql = """
                UPDATE douyin_aweme SET 
                    title = %s, desc = %s, nickname = %s, avatar = %s,
                    liked_count = %s, comment_count = %s, share_count = %s, collected_count = %s,
                    cover_url = %s, video_download_url = %s, video_play_url = %s, video_share_url = %s,
                    task_id = %s, last_modify_ts = %s
                WHERE aweme_id = %s
                """
                await db.execute(update_sql, (
                    aweme_data["title"], aweme_data["desc"], aweme_data["nickname"], aweme_data["avatar"],
                    aweme_data["liked_count"], aweme_data["comment_count"], aweme_data["share_count"], aweme_data["collected_count"],
                    aweme_data["cover_url"], aweme_data["video_download_url"], aweme_data["video_play_url"], aweme_data["video_share_url"],
                    aweme_data["task_id"], aweme_data["last_modify_ts"], aweme_data["aweme_id"]
                ))
            else:
                # æ’å…¥æ–°è®°å½•
                insert_sql = """
                INSERT INTO douyin_aweme (
                    aweme_id, aweme_type, title, `desc`, create_time, user_id, sec_uid, short_user_id, user_unique_id,
                    nickname, avatar, user_signature, ip_location, liked_count, comment_count, share_count, collected_count,
                    aweme_url, cover_url, video_download_url, video_play_url, video_share_url, is_favorite, minio_url,
                    task_id, source_keyword, add_ts, last_modify_ts
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s
                )
                """
                await db.execute(insert_sql, (
                    aweme_data["aweme_id"], aweme_data["aweme_type"], aweme_data["title"], aweme_data["desc"], aweme_data["create_time"],
                    aweme_data["user_id"], aweme_data["sec_uid"], aweme_data["short_user_id"], aweme_data["user_unique_id"],
                    aweme_data["nickname"], aweme_data["avatar"], aweme_data["user_signature"], aweme_data["ip_location"],
                    aweme_data["liked_count"], aweme_data["comment_count"], aweme_data["share_count"], aweme_data["collected_count"],
                    aweme_data["aweme_url"], aweme_data["cover_url"], aweme_data["video_download_url"], aweme_data["video_play_url"],
                    aweme_data["video_share_url"], aweme_data["is_favorite"], aweme_data["minio_url"],
                    aweme_data["task_id"], aweme_data["source_keyword"], aweme_data["add_ts"], aweme_data["last_modify_ts"]
                ))
            
            utils.logger.info(f"[VIDEO_SAVE] æŠ–éŸ³è§†é¢‘ä¿å­˜æˆåŠŸ: {aweme_data['aweme_id']}")
            
        elif platform == "xhs":  # å°çº¢ä¹¦
            # æ„å»ºå°çº¢ä¹¦ç¬”è®°æ•°æ®
            note_data = {
                "note_id": video_data.get("note_id", ""),
                "type": video_data.get("type", "normal"),
                "title": video_data.get("title", ""),
                "desc": video_data.get("desc", ""),
                "video_url": video_data.get("video_url", ""),
                "time": video_data.get("time", int(time.time())),
                "last_update_time": int(time.time()),
                "user_id": video_data.get("user_id", ""),
                "nickname": video_data.get("nickname", ""),
                "avatar": video_data.get("avatar", ""),
                "ip_location": video_data.get("ip_location", ""),
                "liked_count": str(video_data.get("liked_count", 0)),
                "collected_count": str(video_data.get("collected_count", 0)),
                "comment_count": str(video_data.get("comment_count", 0)),
                "share_count": str(video_data.get("share_count", 0)),
                "image_list": video_data.get("image_list", ""),
                "tag_list": video_data.get("tag_list", ""),
                "note_url": video_data.get("note_url", ""),
                "source_keyword": video_data.get("source_keyword", ""),
                "xsec_token": video_data.get("xsec_token", ""),
                "add_ts": int(time.time()),
                "last_modify_ts": int(time.time())
            }
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            check_sql = "SELECT id FROM xhs_note WHERE note_id = %s"
            existing = await db.query(check_sql, note_data["note_id"])
            
            if existing:
                # æ›´æ–°ç°æœ‰è®°å½•
                update_sql = """
                UPDATE xhs_note SET 
                    title = %s, `desc` = %s, video_url = %s, nickname = %s, avatar = %s,
                    liked_count = %s, collected_count = %s, comment_count = %s, share_count = %s,
                    image_list = %s, tag_list = %s, note_url = %s, last_modify_ts = %s
                WHERE note_id = %s
                """
                await db.execute(update_sql, (
                    note_data["title"], note_data["desc"], note_data["video_url"], note_data["nickname"], note_data["avatar"],
                    note_data["liked_count"], note_data["collected_count"], note_data["comment_count"], note_data["share_count"],
                    note_data["image_list"], note_data["tag_list"], note_data["note_url"], note_data["last_modify_ts"], note_data["note_id"]
                ))
            else:
                # æ’å…¥æ–°è®°å½•
                insert_sql = """
                INSERT INTO xhs_note (
                    note_id, type, title, `desc`, video_url, time, last_update_time, user_id,
                    nickname, avatar, ip_location, liked_count, collected_count, comment_count, share_count,
                    image_list, tag_list, note_url, source_keyword, xsec_token, add_ts, last_modify_ts
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s, %s
                )
                """
                await db.execute(insert_sql, (
                    note_data["note_id"], note_data["type"], note_data["title"], note_data["desc"], note_data["video_url"],
                    note_data["time"], note_data["last_update_time"], note_data["user_id"],
                    note_data["nickname"], note_data["avatar"], note_data["ip_location"], note_data["liked_count"],
                    note_data["collected_count"], note_data["comment_count"], note_data["share_count"],
                    note_data["image_list"], note_data["tag_list"], note_data["note_url"], note_data["source_keyword"],
                    note_data["xsec_token"], note_data["add_ts"], note_data["last_modify_ts"]
                ))
            
            utils.logger.info(f"[VIDEO_SAVE] å°çº¢ä¹¦ç¬”è®°ä¿å­˜æˆåŠŸ: {note_data['note_id']}")
            
        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–å¹³å°çš„å¤„ç†é€»è¾‘...
        
    except Exception as e:
        utils.logger.error(f"[VIDEO_SAVE] ä¿å­˜è§†é¢‘æ•°æ®å¤±è´¥: {e}")
        raise

async def save_comment_to_database(platform: str, comment_data: Dict, task_id: str):
    """ä¿å­˜è¯„è®ºæ•°æ®åˆ°æ•°æ®åº“"""
    try:
        if platform == "dy":  # æŠ–éŸ³è¯„è®º
            comment_record = {
                "comment_id": comment_data.get("comment_id", ""),
                "aweme_id": comment_data.get("aweme_id", ""),
                "content": comment_data.get("content", ""),
                "create_time": comment_data.get("create_time", int(time.time())),
                "user_id": comment_data.get("user_id", ""),
                "sec_uid": comment_data.get("sec_uid", ""),
                "short_user_id": comment_data.get("short_user_id", ""),
                "nickname": comment_data.get("nickname", ""),
                "avatar": comment_data.get("avatar", ""),
                "ip_location": comment_data.get("ip_location", ""),
                "sub_comment_count": str(comment_data.get("sub_comment_count", 0)),
                "parent_comment_id": comment_data.get("parent_comment_id", ""),
                "like_count": str(comment_data.get("like_count", 0)),
                "pictures": comment_data.get("pictures", ""),
                "add_ts": int(time.time()),
                "last_modify_ts": int(time.time())
            }
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            check_sql = "SELECT id FROM douyin_aweme_comment WHERE comment_id = %s"
            existing = await db.query(check_sql, comment_record["comment_id"])
            
            if not existing:
                # æ’å…¥æ–°è¯„è®ºè®°å½•
                insert_sql = """
                INSERT INTO douyin_aweme_comment (
                    comment_id, aweme_id, content, create_time, user_id, sec_uid, short_user_id,
                    nickname, avatar, ip_location, sub_comment_count, parent_comment_id, like_count,
                    pictures, add_ts, last_modify_ts
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s, %s,
                    %s, %s, %s
                )
                """
                await db.execute(insert_sql, (
                    comment_record["comment_id"], comment_record["aweme_id"], comment_record["content"], comment_record["create_time"],
                    comment_record["user_id"], comment_record["sec_uid"], comment_record["short_user_id"],
                    comment_record["nickname"], comment_record["avatar"], comment_record["ip_location"], comment_record["sub_comment_count"],
                    comment_record["parent_comment_id"], comment_record["like_count"], comment_record["pictures"],
                    comment_record["add_ts"], comment_record["last_modify_ts"]
                ))
                
                utils.logger.info(f"[COMMENT_SAVE] æŠ–éŸ³è¯„è®ºä¿å­˜æˆåŠŸ: {comment_record['comment_id']}")
        
        elif platform == "xhs":  # å°çº¢ä¹¦è¯„è®º
            comment_record = {
                "comment_id": comment_data.get("comment_id", ""),
                "create_time": comment_data.get("create_time", int(time.time())),
                "note_id": comment_data.get("note_id", ""),
                "content": comment_data.get("content", ""),
                "user_id": comment_data.get("user_id", ""),
                "nickname": comment_data.get("nickname", ""),
                "avatar": comment_data.get("avatar", ""),
                "ip_location": comment_data.get("ip_location", ""),
                "sub_comment_count": comment_data.get("sub_comment_count", 0),
                "pictures": comment_data.get("pictures", ""),
                "parent_comment_id": comment_data.get("parent_comment_id", ""),
                "like_count": str(comment_data.get("like_count", 0)),
                "add_ts": int(time.time()),
                "last_modify_ts": int(time.time())
            }
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            check_sql = "SELECT id FROM xhs_note_comment WHERE comment_id = %s"
            existing = await db.query(check_sql, comment_record["comment_id"])
            
            if not existing:
                # æ’å…¥æ–°è¯„è®ºè®°å½•
                insert_sql = """
                INSERT INTO xhs_note_comment (
                    comment_id, create_time, note_id, content, user_id, nickname, avatar,
                    ip_location, sub_comment_count, pictures, parent_comment_id, like_count,
                    add_ts, last_modify_ts
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, %s,
                    %s, %s, %s, %s, %s,
                    %s, %s
                )
                """
                await db.execute(insert_sql, (
                    comment_record["comment_id"], comment_record["create_time"], comment_record["note_id"], comment_record["content"],
                    comment_record["user_id"], comment_record["nickname"], comment_record["avatar"],
                    comment_record["ip_location"], comment_record["sub_comment_count"], comment_record["pictures"],
                    comment_record["parent_comment_id"], comment_record["like_count"],
                    comment_record["add_ts"], comment_record["last_modify_ts"]
                ))
                
                utils.logger.info(f"[COMMENT_SAVE] å°çº¢ä¹¦è¯„è®ºä¿å­˜æˆåŠŸ: {comment_record['comment_id']}")
        
        # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–å¹³å°çš„è¯„è®ºå¤„ç†é€»è¾‘...
        
    except Exception as e:
        utils.logger.error(f"[COMMENT_SAVE] ä¿å­˜è¯„è®ºæ•°æ®å¤±è´¥: {e}")
        raise

async def update_task_progress(task_id: str, progress: float, status: str = None, result_count: int = None):
    """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
    try:
        update_fields = ["progress = %s"]
        update_values = [progress]
        
        if status:
            update_fields.append("status = %s")
            update_values.append(status)
        
        if result_count is not None:
            update_fields.append("result_count = %s")
            update_values.append(result_count)
        
        update_fields.append("updated_at = NOW()")
        update_values.append(task_id)
        
        sql = f"UPDATE crawler_tasks SET {', '.join(update_fields)} WHERE id = %s"
        await db.execute(sql, update_values)
        
        utils.logger.info(f"[TASK_PROGRESS] ä»»åŠ¡è¿›åº¦æ›´æ–°: {task_id}, è¿›åº¦: {progress}, çŠ¶æ€: {status}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_PROGRESS] æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

async def log_task_step(task_id: str, platform: str, step: str, message: str, log_level: str = "INFO", progress: int = None):
    """è®°å½•ä»»åŠ¡æ­¥éª¤æ—¥å¿—"""
    try:
        sql = """
        INSERT INTO crawler_task_logs (
            task_id, platform, account_id, log_level, message, step, progress, created_at
        ) VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
        """
        
        await db.execute(sql, (
            task_id,
            platform,
            None,  # account_id
            log_level,
            message,
            step,
            progress or 0
        ))
        
        utils.logger.info(f"[TASK_LOG] {task_id} - {step}: {message}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_LOG] è®°å½•ä»»åŠ¡æ—¥å¿—å¤±è´¥: {e}")

class PlatformComingSoonException(Exception):
    """å¹³å°å³å°†æ”¯æŒå¼‚å¸¸"""
    pass


class CrawlerFactory:
    # è§†é¢‘ä¼˜å…ˆå¹³å° - å½“å‰æ”¯æŒ
    VIDEO_PLATFORMS = ["xhs", "dy", "ks", "bili"]
    
    # æ–‡å­—å¹³å° - å³å°†æ”¯æŒ
    COMING_SOON_PLATFORMS = {
        "wb": "å¾®åš",
        "tieba": "è´´å§", 
        "zhihu": "çŸ¥ä¹"
    }

    @staticmethod
    def _get_crawler_class(platform: str):
        """å»¶è¿Ÿå¯¼å…¥çˆ¬è™«ç±» - ä»…æ”¯æŒè§†é¢‘å¹³å°"""
        if platform == "xhs":
            from media_platform.xhs import XiaoHongShuCrawler
            return XiaoHongShuCrawler
        elif platform == "dy":
            from media_platform.douyin import DouYinCrawler
            return DouYinCrawler
        elif platform == "ks":
            from media_platform.kuaishou import KuaishouCrawler
            return KuaishouCrawler
        elif platform == "bili":
            from media_platform.bilibili import BilibiliCrawler
            return BilibiliCrawler
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")

    @staticmethod
    def create_crawler(platform: str) -> AbstractCrawler:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå³å°†æ”¯æŒçš„å¹³å°
        if platform in CrawlerFactory.COMING_SOON_PLATFORMS:
            platform_name = CrawlerFactory.COMING_SOON_PLATFORMS[platform]
            raise PlatformComingSoonException(f"{platform_name}å¹³å°å³å°†æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ï¼å½“å‰ä¸“æ³¨äºçŸ­è§†é¢‘å¹³å°ä¼˜åŒ–ã€‚")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è§†é¢‘å¹³å°
        crawler_class = CrawlerFactory._get_crawler_class(platform)
        return crawler_class()

async def run_crawler_task(task_id: str, request: CrawlerRequest):
    """åå°è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    try:
        utils.logger.info("â–ˆ" * 100)
        utils.logger.info(f"[TASK_{task_id}] ğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡")
        utils.logger.info(f"[TASK_{task_id}] ğŸ“ è¯·æ±‚å‚æ•°è¯¦æƒ…:")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ platform: {request.platform}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ keywords: {request.keywords}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ max_count: {request.max_notes_count}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ account_id: {request.account_id}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ session_id: {request.session_id}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ login_type: {request.login_type}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ crawler_type: {request.crawler_type}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ get_comments: {request.get_comments}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ save_data_option: {request.save_data_option}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ use_proxy: {request.use_proxy}")
        utils.logger.info(f"[TASK_{task_id}]   â””â”€ proxy_strategy: {request.proxy_strategy}")
        
        # ğŸ†• åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“
        utils.logger.info(f"[TASK_{task_id}] ğŸ“ åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“...")
        await create_task_record(task_id, request)
        utils.logger.info(f"[TASK_{task_id}] âœ… ä»»åŠ¡è®°å½•åˆ›å»ºæˆåŠŸ")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        utils.logger.info(f"[TASK_{task_id}] ğŸ”„ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­...")
        task_status[task_id]["status"] = "running"
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        utils.logger.info(f"[TASK_{task_id}] âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°")
        
        # ğŸ†• æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
        await update_task_progress(task_id, 0.0, "running")
        await log_task_step(task_id, request.platform, "task_start", "ä»»åŠ¡å¼€å§‹æ‰§è¡Œ", "INFO", 0)
        
        # æ£€æŸ¥ç™»å½•çŠ¶æ€
        utils.logger.info(f"[ä»»åŠ¡ {task_id}] æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        await log_task_step(task_id, request.platform, "login_check", "æ£€æŸ¥ç™»å½•çŠ¶æ€", "INFO", 10)
        
        from login_manager import login_manager
        
        if request.session_id:
            utils.logger.info(f"[ä»»åŠ¡ {task_id}] ä½¿ç”¨æŒ‡å®šçš„ä¼šè¯ID: {request.session_id}")
            # ä½¿ç”¨æŒ‡å®šçš„ä¼šè¯ID
            session = await login_manager.check_login_status(request.platform, request.session_id)
            if session.status.value == "need_verification":
                utils.logger.warning(f"[ä»»åŠ¡ {task_id}] éœ€è¦éªŒè¯ï¼Œä¼šè¯çŠ¶æ€: {session.status.value}")
                task_status[task_id]["status"] = "need_verification"
                task_status[task_id]["error"] = "éœ€è¦éªŒè¯"
                task_status[task_id]["session_id"] = session.session_id
                task_status[task_id]["updated_at"] = datetime.now().isoformat()
                await update_task_progress(task_id, 0.0, "need_verification")
                await log_task_step(task_id, request.platform, "login_failed", "éœ€è¦éªŒè¯", "WARN", 0)
                return
            elif session.status.value == "logged_in":
                utils.logger.info(f"[ä»»åŠ¡ {task_id}] ä¼šè¯çŠ¶æ€æ­£å¸¸")
                await log_task_step(task_id, request.platform, "login_success", "ç™»å½•çŠ¶æ€æ­£å¸¸", "INFO", 20)
                # å·²ç™»å½•ï¼Œcookiesç”±çˆ¬è™«ç›´æ¥ä»æ•°æ®åº“è¯»å–
            else:
                utils.logger.warning(f"[ä»»åŠ¡ {task_id}] ä¼šè¯çŠ¶æ€å¼‚å¸¸: {session.status.value}")
                await log_task_step(task_id, request.platform, "login_error", f"ä¼šè¯çŠ¶æ€å¼‚å¸¸: {session.status.value}", "ERROR", 0)
        else:
            utils.logger.info(f"[ä»»åŠ¡ {task_id}] æŸ¥æ‰¾å¹³å° {request.platform} çš„æœ€æ–°ä¼šè¯")
            # æŸ¥æ‰¾å¹³å°çš„æœ€æ–°ä¼šè¯
            session = await login_manager.check_login_status(request.platform)
            if session.status.value == "logged_in":
                utils.logger.info(f"[ä»»åŠ¡ {task_id}] æ‰¾åˆ°æœ‰æ•ˆä¼šè¯")
                await log_task_step(task_id, request.platform, "login_success", "æ‰¾åˆ°æœ‰æ•ˆä¼šè¯", "INFO", 20)
                # å·²ç™»å½•ï¼Œcookiesç”±çˆ¬è™«ç›´æ¥ä»æ•°æ®åº“è¯»å–
            elif session.status.value in ["not_logged_in", "expired", "need_verification"]:
                utils.logger.warning(f"[ä»»åŠ¡ {task_id}] éœ€è¦ç™»å½•ï¼Œä¼šè¯çŠ¶æ€: {session.status.value}")
                # éœ€è¦ç™»å½•æˆ–éªŒè¯
                task_status[task_id]["status"] = "need_login"
                task_status[task_id]["error"] = "éœ€è¦ç™»å½•"
                task_status[task_id]["session_id"] = session.session_id
                task_status[task_id]["updated_at"] = datetime.now().isoformat()
                await update_task_progress(task_id, 0.0, "need_login")
                await log_task_step(task_id, request.platform, "login_failed", "éœ€è¦ç™»å½•", "WARN", 0)
                return
                
        # æ£€æŸ¥æŒ‡å®šè´¦å·çš„å‡­è¯æœ‰æ•ˆæ€§ï¼ˆå¦‚æœæä¾›äº†è´¦å·IDï¼‰
        if request.account_id:
            utils.logger.info(f"[ä»»åŠ¡ {task_id}] æ£€æŸ¥æŒ‡å®šè´¦å·å‡­è¯æœ‰æ•ˆæ€§: {request.account_id}")
            await log_task_step(task_id, request.platform, "account_check", f"æ£€æŸ¥è´¦å·å‡­è¯: {request.account_id}", "INFO", 25)
            
            validity = await check_token_validity(request.platform, request.account_id)
            if validity["status"] not in ["valid", "expiring_soon"]:
                utils.logger.error(f"[ä»»åŠ¡ {task_id}] æŒ‡å®šè´¦å·å‡­è¯æ— æ•ˆ: {validity['message']}")
                task_status[task_id]["status"] = "failed"
                task_status[task_id]["error"] = f"æŒ‡å®šè´¦å·å‡­è¯æ— æ•ˆ: {validity['message']}"
                task_status[task_id]["updated_at"] = datetime.now().isoformat()
                await update_task_progress(task_id, 0.0, "failed")
                await log_task_step(task_id, request.platform, "account_failed", f"è´¦å·å‡­è¯æ— æ•ˆ: {validity['message']}", "ERROR", 0)
                return
            elif validity["status"] == "expiring_soon":
                utils.logger.warning(f"[ä»»åŠ¡ {task_id}] æŒ‡å®šè´¦å·å‡­è¯å³å°†è¿‡æœŸ: {validity['expires_at']}")
                await log_task_step(task_id, request.platform, "account_warning", f"è´¦å·å‡­è¯å³å°†è¿‡æœŸ: {validity['expires_at']}", "WARN", 30)
            else:
                await log_task_step(task_id, request.platform, "account_success", "è´¦å·å‡­è¯æœ‰æ•ˆ", "INFO", 30)
        
        # è®¾ç½®çˆ¬è™«é…ç½®
        utils.logger.info(f"[TASK_{task_id}] âš™ï¸ è®¾ç½®çˆ¬è™«é…ç½®...")
        await log_task_step(task_id, request.platform, "config_setup", "è®¾ç½®çˆ¬è™«é…ç½®", "INFO", 35)
        
        config.PLATFORM = request.platform
        config.KEYWORDS = request.keywords
        config.CRAWLER_MAX_NOTES_COUNT = request.max_notes_count
        config.CRAWLER_TYPE = request.crawler_type or "search"
        config.LOGIN_TYPE = request.login_type or "qrcode"
        config.ENABLE_GET_COMMENTS = request.get_comments if request.get_comments is not None else True
        config.SAVE_DATA_OPTION = request.save_data_option or "json"
        
        # è®¾ç½®ä»£ç†é…ç½®
        if hasattr(config, 'ENABLE_IP_PROXY'):
            config.ENABLE_IP_PROXY = request.use_proxy if request.use_proxy is not None else False
        if hasattr(config, 'PROXY_STRATEGY'):
            config.PROXY_STRATEGY = request.proxy_strategy or "disabled"
        
        # è®¾ç½®è´¦å·IDï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if request.account_id:
            config.ACCOUNT_ID = str(request.account_id)  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå› ä¸ºconfigå¯èƒ½æœŸæœ›å­—ç¬¦ä¸²
            utils.logger.info(f"[TASK_{task_id}] ğŸ¯ è®¾ç½®æŒ‡å®šè´¦å·ID: {request.account_id}")
        else:
            config.ACCOUNT_ID = None
            utils.logger.info(f"[TASK_{task_id}] ğŸ‘¤ ä½¿ç”¨é»˜è®¤è´¦å·")
        
        utils.logger.info(f"[TASK_{task_id}] âœ… é…ç½®å®Œæˆ:")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ Platform: {config.PLATFORM}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ Keywords: {config.KEYWORDS}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ CrawlerType: {config.CRAWLER_TYPE}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ LoginType: {config.LOGIN_TYPE}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ Comments: {config.ENABLE_GET_COMMENTS}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ SaveOption: {config.SAVE_DATA_OPTION}")
        utils.logger.info(f"[TASK_{task_id}]   â””â”€ AccountID: {config.ACCOUNT_ID}")
        
        await log_task_step(task_id, request.platform, "config_complete", "çˆ¬è™«é…ç½®å®Œæˆ", "INFO", 40)
        
        # åˆå§‹åŒ–æ•°æ®åº“ï¼ˆå¦‚æœéœ€è¦ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
        if config.SAVE_DATA_OPTION == "db":
            utils.logger.info(f"[TASK_{task_id}] ğŸ’¾ åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
            await log_task_step(task_id, request.platform, "db_init", "åˆå§‹åŒ–æ•°æ®åº“è¿æ¥", "INFO", 45)
            
            try:
                await db.init_db()
                utils.logger.info(f"[TASK_{task_id}] âœ… æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ")
                await log_task_step(task_id, request.platform, "db_success", "æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ", "INFO", 50)
            except Exception as e:
                utils.logger.error(f"[TASK_{task_id}] âŒ æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
                task_status[task_id]["status"] = "failed"
                task_status[task_id]["error"] = f"æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}"
                task_status[task_id]["updated_at"] = datetime.now().isoformat()
                await update_task_progress(task_id, 0.0, "failed")
                await log_task_step(task_id, request.platform, "db_failed", f"æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}", "ERROR", 0)
                return
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        utils.logger.info(f"[TASK_{task_id}] ğŸ­ åˆ›å»ºçˆ¬è™«å®ä¾‹...")
        await log_task_step(task_id, request.platform, "crawler_create", "åˆ›å»ºçˆ¬è™«å®ä¾‹", "INFO", 55)
        
        crawler_instance: AbstractCrawler = CrawlerFactory.create_crawler(config.PLATFORM)
        utils.logger.info(f"[TASK_{task_id}] âœ… çˆ¬è™«å®ä¾‹åˆ›å»ºæˆåŠŸ: {type(crawler_instance).__name__}")
        await log_task_step(task_id, request.platform, "crawler_ready", f"çˆ¬è™«å®ä¾‹åˆ›å»ºæˆåŠŸ: {type(crawler_instance).__name__}", "INFO", 60)
        
        # ğŸš€ æ–°å¢ï¼šçˆ¬è™«ç›´æ¥å†™å…¥Rediså’Œæ•°æ®åº“
        utils.logger.info(f"[TASK_{task_id}] ğŸ“Š å‡†å¤‡æ¥æ”¶çˆ¬è™«æ•°æ®åˆ°Rediså’Œæ•°æ®åº“...")
        
        try:
            # åˆ›å»ºRediså’Œæ•°æ®åº“å­˜å‚¨å›è°ƒå‡½æ•°ï¼Œè®©çˆ¬è™«ç›´æ¥å†™å…¥
            async def storage_callback(platform: str, data: Dict, data_type: str = "video"):
                """Rediså’Œæ•°æ®åº“å­˜å‚¨å›è°ƒå‡½æ•°ï¼Œä¾›çˆ¬è™«è°ƒç”¨"""
                try:
                    if data_type == "video":
                        # è½¬æ¢è§†é¢‘æ•°æ®æ ¼å¼
                        video_data = {
                            "video_id": data.get("aweme_id" if platform == "dy" else "note_id", ""),
                            "title": data.get("title", ""),
                            "author_name": data.get("author_name", ""),
                            "download_url": data.get("video_url", ""),
                            "liked_count": data.get("liked_count", 0),
                            "comment_count": data.get("comment_count", 0),
                            "play_count": data.get("view_count", 0),
                            "share_count": data.get("share_count", 0),
                            "collected_count": data.get("collected_count", 0),
                            "video_size": 0,
                            "duration": data.get("duration", 0),
                            "cover_url": data.get("cover_url", ""),
                            "create_time": data.get("create_time", ""),
                            "raw_data": data  # ä¿å­˜åŸå§‹æ•°æ®
                        }
                        
                        # å­˜å‚¨åˆ°Redis
                        await redis_manager.store_video_data(task_id, platform, video_data)
                        utils.logger.info(f"[TASK_{task_id}] âœ… è§†é¢‘æ•°æ®å·²å­˜å‚¨åˆ°Redis: {video_data['video_id']}")
                        
                        # ğŸ†• å­˜å‚¨åˆ°æ•°æ®åº“
                        await save_video_to_database(platform, data, task_id)
                        utils.logger.info(f"[TASK_{task_id}] âœ… è§†é¢‘æ•°æ®å·²å­˜å‚¨åˆ°æ•°æ®åº“: {video_data['video_id']}")
                        
                    elif data_type == "comment":
                        # å¤„ç†è¯„è®ºæ•°æ®
                        video_id = data.get("aweme_id" if platform == "dy" else "note_id", "")
                        if video_id:
                            comment_data = {
                                "comment_id": data.get("comment_id", ""),
                                "content": data.get("content", ""),
                                "author_name": data.get("comment_user_name", ""),
                                "liked_count": data.get("liked_count", 0),
                                "create_time": data.get("create_time", ""),
                                "raw_data": data  # ä¿å­˜åŸå§‹æ•°æ®
                            }
                            
                            # å­˜å‚¨è¯„è®ºåˆ°Redis
                            await redis_manager.store_hot_comments(platform, video_id, [comment_data])
                            utils.logger.info(f"[TASK_{task_id}] âœ… è¯„è®ºæ•°æ®å·²å­˜å‚¨åˆ°Redis: {comment_data['comment_id']}")
                            
                            # ğŸ†• å­˜å‚¨è¯„è®ºåˆ°æ•°æ®åº“
                            await save_comment_to_database(platform, data, task_id)
                            utils.logger.info(f"[TASK_{task_id}] âœ… è¯„è®ºæ•°æ®å·²å­˜å‚¨åˆ°æ•°æ®åº“: {comment_data['comment_id']}")
                            
                except Exception as e:
                    utils.logger.error(f"[TASK_{task_id}] âŒ å­˜å‚¨å›è°ƒå¤±è´¥: {e}")
            
            # å°†å›è°ƒå‡½æ•°ä¼ é€’ç»™çˆ¬è™«
            crawler_instance.set_storage_callback(storage_callback)
            
            # æ‰§è¡Œçˆ¬å–
            utils.logger.info(f"[TASK_{task_id}] ğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬å–...")
            await log_task_step(task_id, request.platform, "crawler_start", "å¼€å§‹æ‰§è¡Œçˆ¬å–", "INFO", 65)
            task_status[task_id]["progress"] = 0.1
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            await update_task_progress(task_id, 0.1, "running")
            
            utils.logger.info(f"[TASK_{task_id}] ğŸ“ è°ƒç”¨çˆ¬è™«start()æ–¹æ³•...")
            await crawler_instance.start()
            utils.logger.info(f"[TASK_{task_id}] âœ… çˆ¬è™«æ‰§è¡Œå®Œæˆ")
            await log_task_step(task_id, request.platform, "crawler_complete", "çˆ¬è™«æ‰§è¡Œå®Œæˆ", "INFO", 80)
            
            task_status[task_id]["progress"] = 0.8
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            await update_task_progress(task_id, 0.8, "running")
            
            # ä»Redisè¯»å–ä»»åŠ¡ç»“æœ
            utils.logger.info(f"[TASK_{task_id}] ğŸ“Š ä»Redisè¯»å–ä»»åŠ¡ç»“æœ...")
            await log_task_step(task_id, request.platform, "result_read", "è¯»å–ä»»åŠ¡ç»“æœ", "INFO", 85)
            
            # è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯
            task_result = await redis_manager.get_task_result(task_id)
            if task_result:
                videos_count = int(task_result.get("total_videos", 0))
                comments_count = int(task_result.get("total_comments", 0))
                
                # æ›´æ–°ä»»åŠ¡ç»“æœä¿¡æ¯
                task_status[task_id]["result"] = {
                    "success": True,
                    "data_count": videos_count,
                    "comment_count": comments_count,
                    "message": f"æˆåŠŸçˆ¬å– {videos_count} ä¸ªè§†é¢‘ï¼Œ{comments_count} æ¡è¯„è®º"
                }
                
                utils.logger.info(f"[TASK_{task_id}] âœ… Redisè¯»å–å®Œæˆ: {videos_count} ä¸ªè§†é¢‘ï¼Œ{comments_count} æ¡è¯„è®º")
                await log_task_step(task_id, request.platform, "result_success", f"è¯»å–å®Œæˆ: {videos_count} ä¸ªè§†é¢‘ï¼Œ{comments_count} æ¡è¯„è®º", "INFO", 90)
            else:
                utils.logger.warning(f"[TASK_{task_id}] âš ï¸ æœªæ‰¾åˆ°ä»»åŠ¡ç»“æœæ•°æ®")
                task_status[task_id]["result"] = {
                    "success": True,
                    "data_count": 0,
                    "comment_count": 0,
                    "message": "ä»»åŠ¡å®Œæˆï¼Œä½†æœªæ‰¾åˆ°æ•°æ®"
                }
                await log_task_step(task_id, request.platform, "result_empty", "æœªæ‰¾åˆ°ä»»åŠ¡ç»“æœæ•°æ®", "WARN", 90)
                    
        except Exception as redis_e:
            utils.logger.error(f"[TASK_{task_id}] âŒ Rediså­˜å‚¨å¤±è´¥: {redis_e}")
            # ä¸å½±å“ä¸»ä»»åŠ¡çŠ¶æ€ï¼Œåªè®°å½•é”™è¯¯
            task_status[task_id]["redis_error"] = str(redis_e)

        # å…¼å®¹åŸæœ‰é€»è¾‘ï¼šè¯»å–ç»“æœæ–‡ä»¶ï¼ˆè™½ç„¶ç°åœ¨ä¸å†ä½¿ç”¨æ–‡ä»¶å­˜å‚¨ï¼‰
        utils.logger.info(f"[ä»»åŠ¡ {task_id}] è¯»å–çˆ¬å–ç»“æœ...")
        result_file_path = None
        data_dir = config_loader.get('app.data_dir', './data')
        
        # æŸ¥æ‰¾æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        json_pattern = f"{data_dir}/*{config.PLATFORM}*.json"
        import glob
        json_files = glob.glob(json_pattern)
        if json_files:
            result_file_path = max(json_files, key=os.path.getmtime)
            utils.logger.info(f"[ä»»åŠ¡ {task_id}] æ‰¾åˆ°ç»“æœæ–‡ä»¶: {result_file_path}")
            
            # è¯»å–å¹¶è§£æç»“æœ
            try:
                with open(result_file_path, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                    data_count = len(result_data) if isinstance(result_data, list) else 1
                    utils.logger.info(f"[ä»»åŠ¡ {task_id}] æˆåŠŸçˆ¬å–æ•°æ® {data_count} æ¡")
                    
                task_status[task_id]["result"] = {
                    "success": True,
                    "data_count": data_count,
                    "file_path": result_file_path,
                    "message": f"æˆåŠŸçˆ¬å– {data_count} æ¡æ•°æ®"
                }
            except Exception as e:
                utils.logger.error(f"[ä»»åŠ¡ {task_id}] è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
                task_status[task_id]["result"] = {
                    "success": False,
                    "data_count": 0,
                    "file_path": result_file_path,
                    "message": f"è¯»å–ç»“æœå¤±è´¥: {e}"
                }
        else:
            utils.logger.warning(f"[ä»»åŠ¡ {task_id}] æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
            task_status[task_id]["result"] = {
                "success": False,
                "data_count": 0,
                "file_path": None,
                "message": "æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶"
            }
        
        # ä»»åŠ¡å®Œæˆ
        task_status[task_id]["status"] = "completed"
        task_status[task_id]["progress"] = 1.0
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        await update_task_progress(task_id, 1.0, "completed", task_status[task_id]["result"]["data_count"])
        await log_task_step(task_id, request.platform, "task_complete", "ä»»åŠ¡æ‰§è¡Œå®Œæˆ", "INFO", 100)
        
        # å…³é—­æ•°æ®åº“è¿æ¥ï¼ˆå¦‚æœä½¿ç”¨äº†æ•°æ®åº“ï¼‰
        if config.SAVE_DATA_OPTION == "db":
            try:
                utils.logger.info(f"[TASK_{task_id}] ğŸ”Œ å…³é—­æ•°æ®åº“è¿æ¥...")
                await db.close()
                utils.logger.info(f"[TASK_{task_id}] âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
            except Exception as e:
                utils.logger.warning(f"[TASK_{task_id}] âš ï¸ å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        utils.logger.info(f"[TASK_{task_id}] ğŸ‰ çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
        utils.logger.info("â–ˆ" * 100)
        
    except Exception as e:
        utils.logger.error("â–ˆ" * 100)
        utils.logger.error(f"[TASK_{task_id}] âŒ çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
        utils.logger.error(f"[TASK_{task_id}] ğŸ› é”™è¯¯è¯¦æƒ…: {str(e)}")
        utils.logger.error(f"[TASK_{task_id}] ğŸ“ é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        utils.logger.error(f"[TASK_{task_id}] ğŸ“Š é”™è¯¯å †æ ˆ:")
        for line in traceback.format_exc().split('\n'):
            if line.strip():
                utils.logger.error(f"[TASK_{task_id}]     {line}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        utils.logger.error(f"[TASK_{task_id}] ğŸ”„ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥...")
        task_status[task_id]["status"] = "failed"
        task_status[task_id]["error"] = str(e)
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        await update_task_progress(task_id, 0.0, "failed")
        await log_task_step(task_id, request.platform, "task_failed", f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR", 0)
        utils.logger.error(f"[TASK_{task_id}] âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°")
        
        # å…³é—­æ•°æ®åº“è¿æ¥ï¼ˆå¦‚æœä½¿ç”¨äº†æ•°æ®åº“ï¼‰
        if config.SAVE_DATA_OPTION == "db":
            try:
                utils.logger.error(f"[TASK_{task_id}] ğŸ”Œ å…³é—­æ•°æ®åº“è¿æ¥...")
                await db.close()
                utils.logger.error(f"[TASK_{task_id}] âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
            except Exception as db_e:
                utils.logger.error(f"[TASK_{task_id}] âš ï¸ å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºç°è­¦å‘Š: {db_e}")
        
        utils.logger.error("â–ˆ" * 100)

async def run_multi_platform_task(task_id: str, request: MultiPlatformCrawlerRequest):
    """åå°è¿è¡Œå¤šå¹³å°æŠ“å–ä»»åŠ¡"""
    if multi_platform_crawler is None:
        task_status[task_id]["status"] = "failed"
        task_status[task_id]["error"] = "å¤šå¹³å°æŠ“å–åŠŸèƒ½æš‚ä¸å¯ç”¨"
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        return
    
    try:
        # åˆ›å»ºå¤šå¹³å°æŠ“å–ä»»åŠ¡
        task_id_created = multi_platform_crawler.create_task(
            platforms=request.platforms,
            keywords=request.keywords,
            max_count_per_platform=request.max_count_per_platform,
            enable_comments=request.enable_comments,
            enable_images=request.enable_images,
            save_format=request.save_format,
            use_proxy=request.use_proxy,
            proxy_strategy=request.proxy_strategy
        )
        
        # å¯åŠ¨ä»»åŠ¡
        success = await multi_platform_crawler.start_task(task_id_created)
        
        if success:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task_status[task_id]["status"] = "completed"
            task_status[task_id]["multi_platform_task_id"] = task_id_created
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
        else:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            task_status[task_id]["status"] = "failed"
            task_status[task_id]["error"] = "å¤šå¹³å°æŠ“å–ä»»åŠ¡å¤±è´¥"
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            
    except Exception as e:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        task_status[task_id]["status"] = "failed"
        task_status[task_id]["error"] = str(e)
        task_status[task_id]["updated_at"] = datetime.now().isoformat()

@app.post("/api/v1/crawler/start", response_model=CrawlerResponse)
async def start_crawler(request: CrawlerRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨çˆ¬è™«ä»»åŠ¡"""
    # è®°å½•è¯¦ç»†çš„è¯·æ±‚ä¿¡æ¯
    utils.logger.info("=" * 80)
    utils.logger.info("[CRAWLER_START] æ”¶åˆ°çˆ¬è™«å¯åŠ¨è¯·æ±‚")
    utils.logger.info(f"[CRAWLER_START] è¯·æ±‚å‚æ•°è¯¦æƒ…:")
    utils.logger.info(f"  - platform: {request.platform}")
    utils.logger.info(f"  - keywords: {request.keywords}")
    utils.logger.info(f"  - max_notes_count: {request.max_notes_count}")
    utils.logger.info(f"  - account_id: {request.account_id}")
    utils.logger.info(f"  - session_id: {request.session_id}")
    utils.logger.info(f"  - login_type: {request.login_type}")
    utils.logger.info(f"  - crawler_type: {request.crawler_type}")
    utils.logger.info(f"  - get_comments: {request.get_comments}")
    utils.logger.info(f"  - save_data_option: {request.save_data_option}")
    utils.logger.info(f"  - use_proxy: {request.use_proxy}")
    utils.logger.info(f"  - proxy_strategy: {request.proxy_strategy}")
    
    try:
        # éªŒè¯å¹³å°å‚æ•° - æ£€æŸ¥å³å°†æ”¯æŒçš„å¹³å°
        try:
            CrawlerFactory.create_crawler(request.platform)
        except PlatformComingSoonException as e:
            error_msg = str(e)
            utils.logger.warning(f"[CRAWLER_START] {error_msg}")
            raise HTTPException(status_code=422, detail=error_msg)
        except ValueError as e:
            error_msg = f"ä¸æ”¯æŒçš„å¹³å°: {request.platform}ï¼Œå½“å‰æ”¯æŒçš„å¹³å°: xhs, dy, ks, bili"
            utils.logger.error(f"[CRAWLER_START] {error_msg}")
            raise HTTPException(status_code=422, detail=error_msg)
        
        # éªŒè¯å…³é”®è¯
        if not request.keywords or not request.keywords.strip():
            error_msg = "å…³é”®è¯ä¸èƒ½ä¸ºç©º"
            utils.logger.error(f"[CRAWLER_START] {error_msg}")
            raise HTTPException(status_code=422, detail=error_msg)
        
        # éªŒè¯æœ€å¤§æ•°é‡
        if request.max_notes_count <= 0:
            error_msg = f"æœ€å¤§çˆ¬å–æ•°é‡å¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {request.max_notes_count}"
            utils.logger.error(f"[CRAWLER_START] {error_msg}")
            raise HTTPException(status_code=422, detail=error_msg)
        
        # éªŒè¯è´¦å·IDï¼ˆå¦‚æœæä¾›ï¼‰
        if request.account_id is not None:
            if request.account_id <= 0:
                error_msg = f"è´¦å·IDå¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {request.account_id}"
                utils.logger.error(f"[CRAWLER_START] {error_msg}")
                raise HTTPException(status_code=422, detail=error_msg)
            utils.logger.info(f"[CRAWLER_START] æŒ‡å®šè´¦å·ID: {request.account_id}")
        
        # éªŒè¯å…¶ä»–å‚æ•°
        if request.login_type not in ["qrcode", "phone", "email", "password"]:
            error_msg = f"ä¸æ”¯æŒçš„ç™»å½•ç±»å‹: {request.login_type}ï¼Œæ”¯æŒçš„ç±»å‹: qrcode, phone, email, password"
            utils.logger.error(f"[CRAWLER_START] {error_msg}")
            raise HTTPException(status_code=422, detail=error_msg)
        
        if request.crawler_type not in ["search", "detail", "creator"]:
            error_msg = f"ä¸æ”¯æŒçš„çˆ¬è™«ç±»å‹: {request.crawler_type}ï¼Œæ”¯æŒçš„ç±»å‹: search, detail, creator"
            utils.logger.error(f"[CRAWLER_START] {error_msg}")
            raise HTTPException(status_code=422, detail=error_msg)
        
        if request.save_data_option not in ["db", "json", "csv"]:
            error_msg = f"ä¸æ”¯æŒçš„æ•°æ®ä¿å­˜é€‰é¡¹: {request.save_data_option}ï¼Œæ”¯æŒçš„é€‰é¡¹: db, json, csv"
            utils.logger.error(f"[CRAWLER_START] {error_msg}")
            raise HTTPException(status_code=422, detail=error_msg)
        
        utils.logger.info(f"[CRAWLER_START] å‚æ•°éªŒè¯é€šè¿‡")
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        utils.logger.info(f"[CRAWLER_START] ç”Ÿæˆä»»åŠ¡ID: {task_id}")
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        task_status[task_id] = {
            "task_id": task_id,
            "status": "pending",
            "progress": 0.0,
            "result": None,
            "error": None,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "request_params": {
                "platform": request.platform,
                "keywords": request.keywords,
                "max_notes_count": request.max_notes_count,
                "account_id": request.account_id,
                "session_id": request.session_id,
                "login_type": request.login_type,
                "crawler_type": request.crawler_type,
                "get_comments": request.get_comments,
                "save_data_option": request.save_data_option,
                "use_proxy": request.use_proxy,
                "proxy_strategy": request.proxy_strategy
            }
        }
        
        utils.logger.info(f"[CRAWLER_START] ä»»åŠ¡çŠ¶æ€å·²åˆå§‹åŒ–")
        
        # åœ¨åå°è¿è¡Œçˆ¬è™«ä»»åŠ¡
        background_tasks.add_task(run_crawler_task, task_id, request)
        utils.logger.info(f"[CRAWLER_START] åå°ä»»åŠ¡å·²æ·»åŠ ")
        
        response = CrawlerResponse(
            task_id=task_id,
            status="pending",
            message="çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨æ£€æŸ¥ç™»å½•çŠ¶æ€..."
        )
        
        utils.logger.info(f"[CRAWLER_START] å“åº”æ•°æ®: {response.dict()}")
        utils.logger.info("=" * 80)
        
        return response
        
    except HTTPException as he:
        utils.logger.error(f"[CRAWLER_START] HTTPå¼‚å¸¸: çŠ¶æ€ç ={he.status_code}, è¯¦æƒ…={he.detail}")
        utils.logger.info("=" * 80)
        raise he
    except Exception as e:
        error_msg = f"å¯åŠ¨çˆ¬è™«å¤±è´¥: {str(e)}"
        utils.logger.error(f"[CRAWLER_START] ç³»ç»Ÿå¼‚å¸¸: {error_msg}")
        utils.logger.error(f"[CRAWLER_START] å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        import traceback
        utils.logger.error(f"[CRAWLER_START] å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        utils.logger.info("=" * 80)
        raise HTTPException(status_code=500, detail=error_msg)

# æ–°å¢å¤šå¹³å°æŠ“å–API
@app.post("/api/v1/multi-platform/start", response_model=CrawlerResponse)
async def start_multi_platform_crawler(request: MultiPlatformCrawlerRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨å¤šå¹³å°æŠ“å–ä»»åŠ¡"""
    if multi_platform_crawler is None:
        raise HTTPException(status_code=503, detail="å¤šå¹³å°æŠ“å–åŠŸèƒ½æš‚ä¸å¯ç”¨")
    
    try:
        # éªŒè¯å¹³å°
        invalid_platforms = [p for p in request.platforms if p not in multi_platform_crawler.platform_mapping]
        if invalid_platforms:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„å¹³å°: {invalid_platforms}")
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        task_status[task_id] = {
            "task_id": task_id,
            "status": "pending",
            "platforms": request.platforms,
            "keywords": request.keywords,
            "progress": {"total": len(request.platforms), "completed": 0, "failed": 0, "pending": len(request.platforms)},
            "result": None,
            "error": None,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat()
        }
        
        # åœ¨åå°è¿è¡Œå¤šå¹³å°æŠ“å–ä»»åŠ¡
        background_tasks.add_task(run_multi_platform_task, task_id, request)
        
        return CrawlerResponse(
            task_id=task_id,
            status="pending",
            message=f"å¤šå¹³å°æŠ“å–ä»»åŠ¡å·²å¯åŠ¨ï¼Œå¹³å°: {', '.join(request.platforms)}ï¼Œå…³é”®è¯: {request.keywords}"
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨å¤šå¹³å°æŠ“å–å¤±è´¥: {str(e)}")

@app.get("/api/v1/crawler/status/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return TaskStatusResponse(**task_status[task_id])

# æ–°å¢å¤šå¹³å°ä»»åŠ¡çŠ¶æ€API
@app.get("/api/v1/multi-platform/status/{task_id}", response_model=MultiPlatformTaskStatusResponse)
async def get_multi_platform_task_status(task_id: str):
    """è·å–å¤šå¹³å°ä»»åŠ¡çŠ¶æ€"""
    if multi_platform_crawler is None:
        raise HTTPException(status_code=503, detail="å¤šå¹³å°æŠ“å–åŠŸèƒ½æš‚ä¸å¯ç”¨")
    
    if task_id not in task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = task_status[task_id]
    
    # å¦‚æœæ˜¯å¤šå¹³å°ä»»åŠ¡ï¼Œè·å–è¯¦ç»†çŠ¶æ€
    if "multi_platform_task_id" in task:
        multi_task_id = task["multi_platform_task_id"]
        multi_status = multi_platform_crawler.get_task_status(multi_task_id)
        
        if multi_status:
            return MultiPlatformTaskStatusResponse(
                task_id=task_id,
                status=multi_status["status"],
                platforms=multi_status["platforms"],
                keywords=multi_status["keywords"],
                progress=multi_status["progress"],
                results=multi_status.get("results"),
                errors=multi_status.get("errors"),
                created_at=task["created_at"],
                started_at=task.get("started_at"),
                completed_at=task.get("completed_at")
            )
    
    # è¿”å›åŸºæœ¬ä»»åŠ¡çŠ¶æ€
    return MultiPlatformTaskStatusResponse(
        task_id=task_id,
        status=task["status"],
        platforms=task.get("platforms", []),
        keywords=task.get("keywords", ""),
        progress=task.get("progress", {}),
        results=task.get("results"),
        errors=task.get("errors"),
        created_at=task["created_at"],
        started_at=task.get("started_at"),
        completed_at=task.get("completed_at")
    )

# æ–°å¢è·å–å¤šå¹³å°ä»»åŠ¡ç»“æœAPI
@app.get("/api/v1/multi-platform/results/{task_id}")
async def get_multi_platform_results(task_id: str, format_type: str = "table"):
    """è·å–å¤šå¹³å°ä»»åŠ¡ç»“æœ"""
    if multi_platform_crawler is None:
        raise HTTPException(status_code=503, detail="å¤šå¹³å°æŠ“å–åŠŸèƒ½æš‚ä¸å¯ç”¨")
    
    if task_id not in task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = task_status[task_id]
    
    # å¦‚æœæ˜¯å¤šå¹³å°ä»»åŠ¡ï¼Œè·å–è¯¦ç»†ç»“æœ
    if "multi_platform_task_id" in task:
        multi_task_id = task["multi_platform_task_id"]
        results = multi_platform_crawler.get_task_results(multi_task_id)
        
        if results:
            if format_type == "json":
                # JSONæ ¼å¼è¿”å›
                json_results = []
                for result in results:
                    json_results.append({
                        "platform": result.platform,
                        "platform_name": multi_platform_crawler.platform_mapping.get(result.platform, result.platform),
                        "content_id": result.content_id,
                        "title": result.title,
                        "author": result.author,
                        "publish_time": result.publish_time,
                        "content": result.content,
                        "likes": result.likes,
                        "comments": result.comments,
                        "shares": result.shares,
                        "views": result.views,
                        "download_links": result.download_links,
                        "tags": result.tags,
                        "url": result.url
                    })
                
                return {
                    "task_id": task_id,
                    "total_count": len(results),
                    "format": "json",
                    "results": json_results
                }
            else:
                # è¡¨æ ¼æ ¼å¼è¿”å›
                table_results = []
                for result in results:
                    platform_name = multi_platform_crawler.platform_mapping.get(result.platform, result.platform)
                    table_results.append({
                        "platform": platform_name,
                        "title": result.title,
                        "author": result.author,
                        "likes": result.likes,
                        "comments": result.comments,
                        "download_links_count": len(result.download_links),
                        "url": result.url
                    })
                
                return {
                    "task_id": task_id,
                    "total_count": len(results),
                    "format": "table",
                    "results": table_results
                }
    
    raise HTTPException(status_code=404, detail="ä»»åŠ¡ç»“æœä¸å­˜åœ¨")

@app.get("/api/v1/crawler/tasks")
async def list_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
    return {
        "tasks": list(task_status.values()),
        "total": len(task_status)
    }

# æ–°å¢å¤šå¹³å°ä»»åŠ¡åˆ—è¡¨API
@app.get("/api/v1/multi-platform/tasks")
async def list_multi_platform_tasks():
    """è·å–æ‰€æœ‰å¤šå¹³å°ä»»åŠ¡åˆ—è¡¨"""
    if multi_platform_crawler is None:
        return {"tasks": [], "total": 0, "message": "å¤šå¹³å°æŠ“å–åŠŸèƒ½æš‚ä¸å¯ç”¨"}
    
    multi_tasks = multi_platform_crawler.list_tasks()
    return {
        "tasks": multi_tasks,
        "total": len(multi_tasks)
    }

@app.delete("/api/v1/crawler/tasks/{task_id}")
async def delete_task(task_id: str):
    """åˆ é™¤ä»»åŠ¡"""
    if task_id not in task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    del task_status[task_id]
    return {"message": "ä»»åŠ¡å·²åˆ é™¤", "task_id": task_id}

# æ–°å¢å–æ¶ˆå¤šå¹³å°ä»»åŠ¡API
@app.post("/api/v1/multi-platform/cancel/{task_id}")
async def cancel_multi_platform_task(task_id: str):
    """å–æ¶ˆå¤šå¹³å°ä»»åŠ¡"""
    if multi_platform_crawler is None:
        raise HTTPException(status_code=503, detail="å¤šå¹³å°æŠ“å–åŠŸèƒ½æš‚ä¸å¯ç”¨")
    
    success = multi_platform_crawler.cancel_task(task_id)
    if success:
        return {"message": "ä»»åŠ¡å·²å–æ¶ˆ", "task_id": task_id}
    else:
        raise HTTPException(status_code=400, detail="å–æ¶ˆä»»åŠ¡å¤±è´¥")

async def register_api_routes():
    """åœ¨æ•°æ®åº“åˆå§‹åŒ–å®Œæˆåæ³¨å†ŒAPIè·¯ç”±"""
    try:
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨æ¨¡å—åŠ è½½æ—¶è®¿é—®æ•°æ®åº“
        utils.logger.info("å¼€å§‹å¯¼å…¥å’Œæ³¨å†Œè·¯ç”±æ¨¡å—...")
        
        # æ³¨å†Œè´¦å·ç®¡ç†è·¯ç”±
        try:
            from api.account_management import account_router
            app.include_router(account_router, prefix="/api/v1", tags=["è´¦å·ç®¡ç†"])
            utils.logger.info("è´¦å·ç®¡ç†è·¯ç”±æ³¨å†ŒæˆåŠŸ")
        except Exception as e:
            utils.logger.error(f"è´¦å·ç®¡ç†è·¯ç”±æ³¨å†Œå¤±è´¥: {e}")
        
        # æ³¨å†Œç™»å½•ç®¡ç†è·¯ç”±
        try:
            from api.login_management import login_router
            app.include_router(login_router, prefix="/api/v1", tags=["ç™»å½•ç®¡ç†"])
            utils.logger.info("ç™»å½•ç®¡ç†è·¯ç”±æ³¨å†ŒæˆåŠŸ")
        except Exception as e:
            utils.logger.error(f"ç™»å½•ç®¡ç†è·¯ç”±æ³¨å†Œå¤±è´¥: {e}")
        
        # æ³¨å†Œä»£ç†ç®¡ç†è·¯ç”±
        try:
            from proxy import proxy_router
            app.include_router(proxy_router, prefix="/api/v1", tags=["ä»£ç†ç®¡ç†"])
            utils.logger.info("ä»£ç†ç®¡ç†è·¯ç”±æ³¨å†ŒæˆåŠŸ")
        except Exception as e:
            utils.logger.error(f"ä»£ç†ç®¡ç†è·¯ç”±æ³¨å†Œå¤±è´¥ï¼ˆä¸å½±å“åŸºæœ¬åŠŸèƒ½ï¼‰: {e}")
        
        # æ³¨å†Œè§†é¢‘æ–‡ä»¶ç®¡ç†è·¯ç”±
        try:
            from api_video_files import router as video_files_router, init_video_files_api
            from db_video_files import VideoFileManager
            
            # åˆå§‹åŒ–è§†é¢‘æ–‡ä»¶ç®¡ç†ç³»ç»Ÿ
            video_file_manager = VideoFileManager()
            await video_file_manager.init_video_files_tables()
            
            # åˆå§‹åŒ–MinIOé…ç½®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            minio_config = None
            try:
                from config.base_config import MINIO_CONFIG
                minio_config = MINIO_CONFIG
            except ImportError:
                pass
            
            init_video_files_api(minio_config)
            app.include_router(video_files_router, tags=["è§†é¢‘æ–‡ä»¶ç®¡ç†"])
            utils.logger.info("è§†é¢‘æ–‡ä»¶ç®¡ç†è·¯ç”±æ³¨å†ŒæˆåŠŸ")
        except Exception as e:
            utils.logger.error(f"è§†é¢‘æ–‡ä»¶ç®¡ç†è·¯ç”±æ³¨å†Œå¤±è´¥ï¼ˆä¸å½±å“åŸºæœ¬åŠŸèƒ½ï¼‰: {e}")
        
        # æ³¨å†Œæ–°çš„ä»»åŠ¡ç»“æœç®¡ç†APIè·¯ç”±
        try:
            app.include_router(api_router, prefix="/api")
            utils.logger.info("ä»»åŠ¡ç»“æœç®¡ç†APIè·¯ç”±æ³¨å†ŒæˆåŠŸ")
        except Exception as e:
            utils.logger.error(f"ä»»åŠ¡ç»“æœç®¡ç†APIè·¯ç”±æ³¨å†Œå¤±è´¥: {e}")
        
        utils.logger.info("æ‰€æœ‰è·¯ç”±æ³¨å†Œå®Œæˆ")
        
    except Exception as e:
        utils.logger.error(f"è·¯ç”±æ³¨å†Œå¤±è´¥: {e}")
        raise


@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œ"""
    utils.logger.info("=== MediaCrawler API Server å¯åŠ¨ ===")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    try:
        utils.logger.info("åˆå§‹åŒ–æ•°æ®åº“...")
        db_initializer = DatabaseInitializer()
        await db_initializer.initialize_database()
        utils.logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–ä¸»æ•°æ®åº“è¿æ¥
        utils.logger.info("åˆå§‹åŒ–ä¸»æ•°æ®åº“è¿æ¥...")
        await db.init_db()
        utils.logger.info("ä¸»æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å®Œæˆ")
        
        # å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        utils.logger.info("å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨...")
        await start_scheduler()
        utils.logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å¯åŠ¨å®Œæˆ")
        
        # æ³¨å†ŒAPIè·¯ç”±ï¼ˆåœ¨æ•°æ®åº“åˆå§‹åŒ–å®Œæˆåï¼‰
        utils.logger.info("æ³¨å†ŒAPIè·¯ç”±...")
        await register_api_routes()
        utils.logger.info("APIè·¯ç”±æ³¨å†Œå®Œæˆ")
        
        # è®¾ç½®æ•°æ®åº“åˆå§‹åŒ–æ ‡å¿—
        global db_initialized
        db_initialized = True
        
        # æ£€æŸ¥Redisè¿æ¥
        try:
            pong = redis_manager.redis_client.ping()
            if pong:
                print("âœ… Redis è¿æ¥æˆåŠŸ (PING å“åº”)")
            else:
                print("âŒ Redis è¿æ¥å¤±è´¥ (PING æ— å“åº”)")
        except Exception as e:
            print(f"âŒ Redis è¿æ¥å¼‚å¸¸: {e}")
        
    except Exception as e:
        utils.logger.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        utils.logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ‰§è¡Œ"""
    utils.logger.info("=== MediaCrawler API Server å…³é—­ ===")
    
    try:
        # åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        utils.logger.info("åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨...")
        await stop_scheduler()
        utils.logger.info("å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")
        
        # å…³é—­æ•°æ®åº“è¿æ¥
        utils.logger.info("å…³é—­æ•°æ®åº“è¿æ¥...")
        try:
            await db.close()
            utils.logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")
        except Exception as db_error:
            utils.logger.warning(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºç°è­¦å‘Š: {db_error}")
        
    except Exception as e:
        utils.logger.error(f"åº”ç”¨å…³é—­æ—¶å‡ºé”™: {e}")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„ï¼Œè¿”å›æµ‹è¯•é¡µé¢"""
    return FileResponse("static/index.html")

@app.get("/api/v1/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "database_initialized": db_initialized
    }

@app.post("/api/v1/database/init")
async def init_database():
    """æ‰‹åŠ¨åˆå§‹åŒ–æ•°æ®åº“"""
    global db_initialized
    try:
        initializer = DatabaseInitializer()
        await initializer.initialize_database()
        db_initialized = True
        return {
            "status": "success",
            "message": "æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

@app.get("/api/v1/database/status")
async def get_database_status():
    """è·å–æ•°æ®åº“çŠ¶æ€"""
    return {
        "initialized": db_initialized,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/platforms")
async def get_supported_platforms():
    """è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨"""
    return {
        "platforms": SUPPORTED_PLATFORMS
    }

# æ–°å¢å¤šå¹³å°ä¿¡æ¯API
@app.get("/api/v1/multi-platform/info")
async def get_multi_platform_info():
    """è·å–å¤šå¹³å°æŠ“å–åŠŸèƒ½ä¿¡æ¯"""
    if multi_platform_crawler is None:
        return {
            "feature": "å¤šå¹³å°åŒæ—¶æŠ“å–",
            "description": "æ”¯æŒå¤šä¸ªå¹³å°åŒæ—¶æŠ“å–ç›¸åŒå…³é”®è¯ï¼Œç»Ÿä¸€ç»“æœæ ¼å¼",
            "supported_platforms": {},
            "capabilities": [
                "å¹¶å‘æŠ“å–å¤šä¸ªå¹³å°",
                "ç»Ÿä¸€ç»“æœæ ¼å¼è¾“å‡º",
                "ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª",
                "è¿›åº¦ç›‘æ§",
                "é”™è¯¯å¤„ç†"
            ],
            "output_formats": ["json", "csv"],
            "max_platforms": 7,
            "status": "unavailable"
        }
    
    return {
        "feature": "å¤šå¹³å°åŒæ—¶æŠ“å–",
        "description": "æ”¯æŒå¤šä¸ªå¹³å°åŒæ—¶æŠ“å–ç›¸åŒå…³é”®è¯ï¼Œç»Ÿä¸€ç»“æœæ ¼å¼",
        "supported_platforms": multi_platform_crawler.platform_mapping,
        "capabilities": [
            "å¹¶å‘æŠ“å–å¤šä¸ªå¹³å°",
            "ç»Ÿä¸€ç»“æœæ ¼å¼è¾“å‡º",
            "ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª",
            "è¿›åº¦ç›‘æ§",
            "é”™è¯¯å¤„ç†"
        ],
        "output_formats": ["json", "csv"],
        "max_platforms": 7,
        "status": "available"
    }

@app.get("/api/v1/proxy/quick-get")
async def quick_get_proxy(
    strategy_type: str = "round_robin",
    platform: str = None,
    check_availability: bool = True
):
    """å¿«é€Ÿè·å–ä»£ç†"""
    global proxy_manager
    
    if proxy_manager is None:
        try:
            from proxy import ProxyManager
            proxy_manager = ProxyManager()
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"ä»£ç†ç®¡ç†å™¨ä¸å¯ç”¨: {str(e)}")
    
    try:
        proxy = await proxy_manager.get_proxy(strategy_type, platform=platform)
        
        if not proxy:
            return {"message": "æ²¡æœ‰å¯ç”¨çš„ä»£ç†", "proxy": None}
        
        if check_availability:
            is_available = await proxy_manager.check_proxy(proxy)
            if not is_available:
                return {"message": "ä»£ç†ä¸å¯ç”¨", "proxy": None}
        
        return {
            "message": "è·å–ä»£ç†æˆåŠŸ",
            "proxy": {
                "id": proxy.id,
                "ip": proxy.ip,
                "port": proxy.port,
                "proxy_type": proxy.proxy_type,
                "country": proxy.country,
                "speed": proxy.speed,
                "anonymity": proxy.anonymity,
                "success_rate": proxy.success_rate
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»£ç†å¤±è´¥: {str(e)}")

@app.get("/api/v1/proxy/quick-stats")
async def quick_proxy_stats():
    """å¿«é€Ÿè·å–ä»£ç†ç»Ÿè®¡"""
    global proxy_manager
    
    if proxy_manager is None:
        try:
            from proxy import ProxyManager
            proxy_manager = ProxyManager()
        except Exception as e:
            raise HTTPException(status_code=503, detail=f"ä»£ç†ç®¡ç†å™¨ä¸å¯ç”¨: {str(e)}")
    
    try:
        stats = await proxy_manager.get_proxy_stats()
        return stats
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ä»£ç†ç»Ÿè®¡å¤±è´¥: {str(e)}")

@app.get("/accounts/{platform}")
async def get_platform_accounts(platform: str):
    """è·å–æŒ‡å®šå¹³å°çš„è´¦å·åˆ—è¡¨"""
    try:
        accounts = await get_account_list_by_platform(platform)
        return {
            "success": True,
            "platform": platform,
            "accounts": accounts,
            "count": len(accounts)
        }
    except Exception as e:
        utils.logger.error(f"è·å–å¹³å°è´¦å·åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–è´¦å·åˆ—è¡¨å¤±è´¥: {str(e)}")


@app.get("/accounts/{platform}/validity")
async def check_platform_token_validity(platform: str, account_id: Optional[str] = None):
    """æ£€æŸ¥æŒ‡å®šå¹³å°å’Œè´¦å·çš„å‡­è¯æœ‰æ•ˆæ€§"""
    try:
        validity = await check_token_validity(platform, account_id)
        return {
            "success": True,
            "platform": platform,
            "account_id": account_id,
            "validity": validity
        }
    except Exception as e:
        utils.logger.error(f"æ£€æŸ¥å‡­è¯æœ‰æ•ˆæ€§å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ£€æŸ¥å‡­è¯æœ‰æ•ˆæ€§å¤±è´¥: {str(e)}")


@app.post("/tokens/cleanup")
async def cleanup_expired_tokens_api():
    """æ¸…ç†è¿‡æœŸçš„å‡­è¯"""
    try:
        count = await cleanup_expired_tokens()
        return {
            "success": True,
            "message": f"å·²æ¸…ç† {count} ä¸ªè¿‡æœŸå‡­è¯",
            "count": count
        }
    except Exception as e:
        utils.logger.error(f"æ¸…ç†è¿‡æœŸå‡­è¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†è¿‡æœŸå‡­è¯å¤±è´¥: {str(e)}")


@app.get("/scheduler/status")
async def get_scheduler_status_api():
    """è·å–å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨çŠ¶æ€"""
    try:
        status = await get_scheduler_status()
        return {
            "success": True,
            "scheduler": status
        }
    except Exception as e:
        utils.logger.error(f"è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {str(e)}")


@app.post("/scheduler/start")
async def start_scheduler_api():
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    try:
        await start_scheduler()
        return {
            "success": True,
            "message": "å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨"
        }
    except Exception as e:
        utils.logger.error(f"å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥: {str(e)}")


@app.post("/scheduler/stop")
async def stop_scheduler_api():
    """åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    try:
        await stop_scheduler()
        return {
            "success": True,
            "message": "å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢"
        }
    except Exception as e:
        utils.logger.error(f"åœæ­¢è°ƒåº¦å™¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢è°ƒåº¦å™¨å¤±è´¥: {str(e)}")



# å†…å®¹æŸ¥è¯¢å·¥å…·å‡½æ•°
async def get_unified_content_from_db(request: ContentListRequest) -> ContentListResponse:
    """ä»æ•°æ®åº“æŸ¥è¯¢ç»Ÿä¸€æ ¼å¼çš„å†…å®¹åˆ—è¡¨ - æ”¯æŒçŸ­è§†é¢‘ä¼˜å…ˆç­›é€‰"""
    if not db_initialized:
        raise HTTPException(status_code=503, detail="æ•°æ®åº“æœªåˆå§‹åŒ–")
    
    from models.content_models import (
        PLATFORM_MAPPING, 
        VIDEO_PRIORITY_PLATFORMS, 
        TODO_PLATFORMS,
        is_video_priority_platform,
        is_todo_platform
    )
    
    all_contents = []
    platforms_summary = {}
    
    # ç¡®å®šè¦æŸ¥è¯¢çš„å¹³å° - æ ¹æ®çŸ­è§†é¢‘ä¼˜å…ˆè®¾ç½®
    if request.platform:
        platforms_to_query = [request.platform]
    else:
        # æ ¹æ®ç­›é€‰æ¡ä»¶ç¡®å®šå¹³å°åˆ—è¡¨
        platforms_to_query = list(PLATFORM_MAPPING.keys())
        
        if request.video_platforms_only:
            # ä»…è§†é¢‘ä¸»å¯¼å¹³å°
            platforms_to_query = VIDEO_PRIORITY_PLATFORMS
            utils.logger.info(f"[CONTENT_QUERY] ä»…æŸ¥è¯¢è§†é¢‘ä¸»å¯¼å¹³å°: {platforms_to_query}")
        elif request.exclude_todo_platforms:
            # æ’é™¤TODOå¹³å°
            platforms_to_query = [p for p in platforms_to_query if p not in TODO_PLATFORMS]
            utils.logger.info(f"[CONTENT_QUERY] æ’é™¤TODOå¹³å°ï¼ŒæŸ¥è¯¢å¹³å°: {platforms_to_query}")
        
        # å¦‚æœåªè¦è§†é¢‘å†…å®¹ï¼Œä¼˜å…ˆæ’åºè§†é¢‘å¹³å°
        if request.video_only:
            video_platforms = [p for p in platforms_to_query if is_video_priority_platform(p)]
            other_platforms = [p for p in platforms_to_query if not is_video_priority_platform(p)]
            platforms_to_query = video_platforms + other_platforms
            utils.logger.info(f"[CONTENT_QUERY] è§†é¢‘ä¼˜å…ˆæ’åº: è§†é¢‘å¹³å°{video_platforms}, å…¶ä»–å¹³å°{other_platforms}")
    
    for platform_key in platforms_to_query:
        if platform_key not in PLATFORM_MAPPING:
            continue
            
        platform_info = PLATFORM_MAPPING[platform_key]
        table_name = platform_info["table"]
        id_field = platform_info["id_field"]
        platform_name = platform_info["name"]
        
        try:
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            conditions = []
            params = []
            
            # è§†é¢‘å†…å®¹ç­›é€‰
            if request.video_only and "video_filter" in platform_info:
                video_filter = platform_info["video_filter"]
                if video_filter and video_filter.strip():
                    conditions.append(f"({video_filter})")
                    utils.logger.info(f"[CONTENT_QUERY] {platform_key} æ·»åŠ è§†é¢‘ç­›é€‰: {video_filter}")
            
            # å†…å®¹ç±»å‹ç­›é€‰
            if request.content_type:
                if request.content_type == "video":
                    if "video_filter" in platform_info and platform_info["video_filter"]:
                        conditions.append(f"({platform_info['video_filter']})")
                elif request.content_type == "image" and platform_key == "xhs":
                    conditions.append("type = 'normal' AND image_list IS NOT NULL AND image_list != ''")
                elif request.content_type == "text":
                    if platform_key == "xhs":
                        conditions.append("type = 'normal' AND (image_list IS NULL OR image_list = '')")
                    elif platform_key in ["dy", "ks", "bili"]:
                        # è¿™äº›å¹³å°ä¸»è¦æ˜¯è§†é¢‘ï¼Œæ–‡æœ¬å†…å®¹è¾ƒå°‘
                        conditions.append("1 = 0")  # åŸºæœ¬ä¸è¿”å›ç»“æœ
            
            # å…³é”®è¯ç­›é€‰
            if request.keyword:
                conditions.append("(title LIKE %s OR `desc` LIKE %s OR source_keyword LIKE %s)")
                keyword_param = f"%{request.keyword}%"
                params.extend([keyword_param, keyword_param, keyword_param])
            
            # ä½œè€…ç­›é€‰
            if request.author_name:
                conditions.append("nickname LIKE %s")
                params.append(f"%{request.author_name}%")
            
            # æ—¶é—´ç­›é€‰
            if request.start_time:
                try:
                    start_ts = int(datetime.fromisoformat(request.start_time.replace('Z', '+00:00')).timestamp())
                    if platform_key in ["xhs", "dy", "ks", "bili"]:
                        conditions.append("time >= %s" if platform_key == "xhs" else "create_time >= %s")
                    else:
                        conditions.append("add_ts >= %s")
                    params.append(start_ts)
                except:
                    pass
            
            if request.end_time:
                try:
                    end_ts = int(datetime.fromisoformat(request.end_time.replace('Z', '+00:00')).timestamp())
                    if platform_key in ["xhs", "dy", "ks", "bili"]:
                        conditions.append("time <= %s" if platform_key == "xhs" else "create_time <= %s")
                    else:
                        conditions.append("add_ts <= %s")
                    params.append(end_ts)
                except:
                    pass
            
            # æ„å»ºWHEREå­å¥
            where_clause = ""
            if conditions:
                where_clause = "WHERE " + " AND ".join(conditions)
            
            # æ„å»ºæ’åº
            sort_field_mapping = {
                "crawl_time": "add_ts",
                "publish_time": "time" if platform_key == "xhs" else "create_time",
                "like_count": "liked_count"
            }
            sort_field = sort_field_mapping.get(request.sort_by, "add_ts")
            sort_order = "DESC" if request.sort_order == "desc" else "ASC"
            
            # è®¡ç®—åç§»é‡
            offset = (request.page - 1) * request.page_size
            
            # æŸ¥è¯¢æ€»æ•°
            count_sql = f"SELECT COUNT(*) as total FROM {table_name} {where_clause}"
            count_result = await db.query(count_sql, *params)
            total_count = count_result[0]['total'] if count_result else 0
            platforms_summary[platform_key] = total_count
            
            if total_count == 0:
                continue
            
            # æŸ¥è¯¢æ•°æ®
            data_sql = f"""
            SELECT * FROM {table_name} 
            {where_clause}
            ORDER BY {sort_field} {sort_order}
            LIMIT %s OFFSET %s
            """
            params.extend([request.page_size, offset])
            rows = await db.query(data_sql, *params)
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            for row in rows:
                unified_content = convert_to_unified_content(row, platform_key, platform_name, id_field)
                if unified_content:
                    all_contents.append(unified_content)
                    
        except Exception as e:
            utils.logger.error(f"æŸ¥è¯¢å¹³å° {platform_key} æ•°æ®å¤±è´¥: {e}")
            platforms_summary[platform_key] = 0
            continue
    
    # å¦‚æœæ˜¯è·¨å¹³å°æŸ¥è¯¢ï¼Œéœ€è¦é‡æ–°æ’åºå’Œåˆ†é¡µ
    if not request.platform:
        # æŒ‰æŒ‡å®šå­—æ®µæ’åº
        if request.sort_by == "crawl_time":
            all_contents.sort(key=lambda x: x.crawl_time or 0, reverse=(request.sort_order == "desc"))
        elif request.sort_by == "publish_time":
            all_contents.sort(key=lambda x: x.publish_time or 0, reverse=(request.sort_order == "desc"))
        elif request.sort_by == "like_count":
            all_contents.sort(key=lambda x: int(str(x.like_count or 0).replace(',', '')), reverse=(request.sort_order == "desc"))
        
        # é‡æ–°åˆ†é¡µ
        total = len(all_contents)
        start_idx = (request.page - 1) * request.page_size
        end_idx = start_idx + request.page_size
        all_contents = all_contents[start_idx:end_idx]
    else:
        total = platforms_summary.get(request.platform, 0)
    
    total_pages = (total + request.page_size - 1) // request.page_size
    
    return ContentListResponse(
        total=total,
        page=request.page,
        page_size=request.page_size,
        total_pages=total_pages,
        items=all_contents,
        platforms_summary=platforms_summary
    )

def convert_to_unified_content(row: Dict, platform: str, platform_name: str, id_field: str) -> Optional[UnifiedContent]:
    """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼"""
    try:
        # åŸºç¡€ä¿¡æ¯
        content_id = str(row.get(id_field, ''))
        title = row.get('title', '')
        description = row.get('desc', '') or row.get('content', '') or row.get('content_text', '')
        
        # åˆ¤æ–­å†…å®¹ç±»å‹
        content_type = "text"
        if platform in ["dy", "ks", "bili"]:
            content_type = "video"
        elif platform == "xhs":
            note_type = row.get('type', '')
            if note_type == "video":
                content_type = "video"
            elif row.get('image_list'):
                content_type = "image"
            else:
                content_type = "text"
        elif platform == "wb":
            if row.get('image_list') or row.get('video_url'):
                content_type = "mixed"
        elif platform == "zhihu":
            zhihu_type = row.get('content_type', '')
            if zhihu_type == "zvideo":
                content_type = "video"
            else:
                content_type = "text"
        
        # ä½œè€…ä¿¡æ¯
        author_id = str(row.get('user_id', ''))
        author_name = row.get('nickname', '') or row.get('user_nickname', '')
        author_avatar = row.get('avatar', '') or row.get('user_avatar', '')
        
        # ç»Ÿè®¡æ•°æ®
        like_count = row.get('liked_count') or row.get('voteup_count') or row.get('like_count') or 0
        comment_count = row.get('comment_count') or row.get('comments_count') or 0
        share_count = row.get('share_count') or row.get('shared_count') or 0
        view_count = row.get('video_play_count') or row.get('viewd_count') or row.get('view_count') or 0
        collect_count = row.get('collected_count') or row.get('video_favorite_count') or 0
        
        # æ—¶é—´ä¿¡æ¯
        publish_time = None
        publish_time_str = None
        crawl_time = row.get('add_ts')
        crawl_time_str = None
        
        # æ ¹æ®å¹³å°è·å–å‘å¸ƒæ—¶é—´
        if platform in ["xhs"]:
            publish_time = row.get('time')
        elif platform in ["dy", "ks", "bili"]:
            publish_time = row.get('create_time')
        elif platform == "wb":
            publish_time = row.get('create_time')
            publish_time_str = row.get('create_date_time')
        elif platform == "zhihu":
            created_time_str = row.get('created_time')
            if created_time_str:
                try:
                    publish_time = int(datetime.fromisoformat(created_time_str).timestamp())
                except:
                    pass
                publish_time_str = created_time_str
        elif platform == "tieba":
            publish_time_str = row.get('publish_time')
        
        # æ ¼å¼åŒ–æ—¶é—´å­—ç¬¦ä¸²
        if publish_time and not publish_time_str:
            try:
                publish_time_str = datetime.fromtimestamp(publish_time).strftime('%Y-%m-%d %H:%M:%S')
            except:
                pass
        
        if crawl_time:
            try:
                crawl_time_str = datetime.fromtimestamp(crawl_time).strftime('%Y-%m-%d %H:%M:%S')
            except:
                pass
        
        # å…³è”ä¿¡æ¯
        source_keyword = row.get('source_keyword', '')
        content_url = row.get('note_url') or row.get('aweme_url') or row.get('video_url') or row.get('content_url') or ''
        cover_url = row.get('cover_url') or row.get('video_cover_url') or ''
        video_url = row.get('video_url') or row.get('video_play_url') or row.get('video_download_url') or ''
        
        # æ ‡ç­¾å¤„ç†
        tags = []
        tag_list = row.get('tag_list', '')
        if tag_list:
            try:
                if isinstance(tag_list, str):
                    if tag_list.startswith('['):
                        tags = json.loads(tag_list)
                    else:
                        tags = [tag.strip() for tag in tag_list.split(',') if tag.strip()]
                elif isinstance(tag_list, list):
                    tags = tag_list
            except:
                pass
        
        # IPåœ°ç†ä½ç½®
        ip_location = row.get('ip_location', '')
        
        return UnifiedContent(
            id=row.get('id', 0),
            platform=platform,
            platform_name=platform_name,
            content_id=content_id,
            content_type=content_type,
            title=title,
            description=description[:500] if description else None,  # é™åˆ¶æè¿°é•¿åº¦
            content=description,
            author_id=author_id,
            author_name=author_name,
            author_avatar=author_avatar,
            like_count=like_count,
            comment_count=comment_count,
            share_count=share_count,
            view_count=view_count,
            collect_count=collect_count,
            publish_time=publish_time,
            publish_time_str=publish_time_str,
            crawl_time=crawl_time,
            crawl_time_str=crawl_time_str,
            source_keyword=source_keyword,
            content_url=content_url,
            cover_url=cover_url,
            video_url=video_url,
            tags=tags,
            ip_location=ip_location,
            extra_data=None
        )
        
    except Exception as e:
        utils.logger.error(f"è½¬æ¢å†…å®¹æ•°æ®å¤±è´¥: {e}, row: {row}")
        return None

# å†…å®¹ç›¸å…³APIæ¥å£
@app.post("/api/v1/content/list", response_model=ContentListResponse)
async def get_content_list(request: ContentListRequest):
    """è·å–å†…å®¹åˆ—è¡¨"""
    try:
        utils.logger.info(f"[CONTENT_LIST] æ”¶åˆ°å†…å®¹åˆ—è¡¨æŸ¥è¯¢è¯·æ±‚")
        utils.logger.info(f"[CONTENT_LIST] æŸ¥è¯¢å‚æ•°: platform={request.platform}, keyword={request.keyword}, page={request.page}")
        
        result = await get_unified_content_from_db(request)
        
        utils.logger.info(f"[CONTENT_LIST] æŸ¥è¯¢å®Œæˆ: æ€»æ•°={result.total}, å½“å‰é¡µ={result.page}, è¿”å›{len(result.items)}æ¡æ•°æ®")
        return result
        
    except Exception as e:
        utils.logger.error(f"[CONTENT_LIST] æŸ¥è¯¢å†…å®¹åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å†…å®¹åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.get("/api/v1/content/{platform}/{content_id}", response_model=UnifiedContent)
async def get_content_detail(platform: str, content_id: str):
    """è·å–å†…å®¹è¯¦æƒ…"""
    try:
        utils.logger.info(f"[CONTENT_DETAIL] è·å–å†…å®¹è¯¦æƒ…: platform={platform}, content_id={content_id}")
        
        if not db_initialized:
            raise HTTPException(status_code=503, detail="æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        platform_mapping = PLATFORM_MAPPING
        
        if platform not in platform_mapping:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„å¹³å°: {platform}")
        
        platform_info = platform_mapping[platform]
        table_name = platform_info["table"]
        id_field = platform_info["id_field"]
        platform_name = platform_info["name"]
        
        # æŸ¥è¯¢æ•°æ®
        sql = f"SELECT * FROM {table_name} WHERE {id_field} = %s LIMIT 1"
        rows = await db.query(sql, content_id)
        
        if not rows:
            raise HTTPException(status_code=404, detail="å†…å®¹ä¸å­˜åœ¨")
        
        row = rows[0]
        unified_content = convert_to_unified_content(row, platform, platform_name, id_field)
        
        if not unified_content:
            raise HTTPException(status_code=500, detail="æ•°æ®è½¬æ¢å¤±è´¥")
        
        utils.logger.info(f"[CONTENT_DETAIL] è·å–è¯¦æƒ…æˆåŠŸ: {unified_content.title}")
        return unified_content
        
    except HTTPException:
        raise
    except Exception as e:
        utils.logger.error(f"[CONTENT_DETAIL] è·å–å†…å®¹è¯¦æƒ…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å†…å®¹è¯¦æƒ…å¤±è´¥: {str(e)}")

@app.get("/api/v1/content/platforms")
async def get_platforms_info():
    """è·å–å¹³å°ä¿¡æ¯å’Œç»Ÿè®¡"""
    try:
        if not db_initialized:
            raise HTTPException(status_code=503, detail="æ•°æ®åº“æœªåˆå§‹åŒ–")
        
        from models.content_models import (
            PLATFORM_MAPPING, 
            VIDEO_PRIORITY_PLATFORMS, 
            TODO_PLATFORMS,
            get_platform_description,
            is_video_priority_platform
        )
        
        platforms_info = {}
        
        for platform_key, platform_info in PLATFORM_MAPPING.items():
            try:
                sql = f"SELECT COUNT(*) as total FROM {platform_info['table']}"
                result = await db.query(sql)
                total_count = result[0]['total'] if result else 0
                
                # ç»Ÿè®¡è§†é¢‘å†…å®¹æ•°é‡
                video_count = 0
                if "video_filter" in platform_info and platform_info["video_filter"]:
                    video_sql = f"SELECT COUNT(*) as total FROM {platform_info['table']} WHERE {platform_info['video_filter']}"
                    video_result = await db.query(video_sql)
                    video_count = video_result[0]['total'] if video_result else 0
                
                # è·å–æœ€è¿‘çš„å…³é”®è¯
                recent_keywords_sql = f"""
                SELECT source_keyword, COUNT(*) as count 
                FROM {platform_info['table']} 
                WHERE source_keyword IS NOT NULL AND source_keyword != ''
                GROUP BY source_keyword 
                ORDER BY count DESC, MAX(add_ts) DESC
                LIMIT 5
                """
                keywords_result = await db.query(recent_keywords_sql)
                recent_keywords = [row['source_keyword'] for row in keywords_result]
                
                platforms_info[platform_key] = {
                    "name": platform_info["name"],
                    "description": get_platform_description(platform_key),
                    "total_count": total_count,
                    "video_count": video_count,
                    "video_ratio": round(video_count / total_count * 100, 1) if total_count > 0 else 0,
                    "recent_keywords": recent_keywords,
                    "is_video_priority": is_video_priority_platform(platform_key),
                    "is_todo": platform_key in TODO_PLATFORMS,
                    "primary_content_type": platform_info.get("primary_content_type", "mixed")
                }
                
            except Exception as e:
                utils.logger.error(f"è·å–å¹³å° {platform_key} ç»Ÿè®¡å¤±è´¥: {e}")
                platforms_info[platform_key] = {
                    "name": platform_info["name"],
                    "description": get_platform_description(platform_key),
                    "total_count": 0,
                    "video_count": 0,
                    "video_ratio": 0,
                    "recent_keywords": [],
                    "is_video_priority": is_video_priority_platform(platform_key),
                    "is_todo": platform_key in TODO_PLATFORMS,
                    "primary_content_type": platform_info.get("primary_content_type", "mixed")
                }
        
        return {
            "platforms": platforms_info,
            "total_platforms": len(platforms_info),
            "video_priority_platforms": VIDEO_PRIORITY_PLATFORMS,
            "todo_platforms": TODO_PLATFORMS,
            "total_content": sum(info["total_count"] for info in platforms_info.values()),
            "total_video_content": sum(info["video_count"] for info in platforms_info.values())
        }
        
    except Exception as e:
        utils.logger.error(f"è·å–å¹³å°ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å¹³å°ä¿¡æ¯å¤±è´¥: {str(e)}")


@app.post("/api/v1/content/videos", response_model=ContentListResponse)
async def get_video_content_list(
    keyword: Optional[str] = None,
    platform: Optional[str] = None,
    page: int = 1,
    page_size: int = 20
):
    """è·å–çŸ­è§†é¢‘å†…å®¹åˆ—è¡¨ - ä¸“æ³¨çŸ­è§†é¢‘ä¼˜å…ˆå¹³å°"""
    try:
        utils.logger.info(f"[VIDEO_CONTENT] æ”¶åˆ°çŸ­è§†é¢‘å†…å®¹æŸ¥è¯¢è¯·æ±‚: keyword={keyword}, platform={platform}")
        
        # æ„å»ºä¸“é—¨çš„çŸ­è§†é¢‘æŸ¥è¯¢è¯·æ±‚
        request = ContentListRequest(
            platform=platform,
            keyword=keyword,
            page=page,
            page_size=page_size,
            video_only=True,  # ä»…è§†é¢‘å†…å®¹
            video_platforms_only=True,  # ä»…è§†é¢‘ä¼˜å…ˆå¹³å°
            exclude_todo_platforms=True,  # æ’é™¤TODOå¹³å°
            sort_by="crawl_time",
            sort_order="desc"
        )
        
        result = await get_unified_content_from_db(request)
        
        utils.logger.info(f"[VIDEO_CONTENT] çŸ­è§†é¢‘æŸ¥è¯¢å®Œæˆ: æ€»æ•°={result.total}, è¿”å›{len(result.items)}æ¡è§†é¢‘")
        return result
        
    except Exception as e:
        utils.logger.error(f"[VIDEO_CONTENT] æŸ¥è¯¢çŸ­è§†é¢‘å†…å®¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢çŸ­è§†é¢‘å†…å®¹å¤±è´¥: {str(e)}")


@app.get("/api/v1/content/video-platforms")
async def get_video_platforms_info():
    """è·å–çŸ­è§†é¢‘ä¼˜å…ˆå¹³å°ä¿¡æ¯"""
    try:
        from models.content_models import (
            VIDEO_PRIORITY_PLATFORMS, 
            PLATFORM_MAPPING,
            get_platform_description
        )
        
        video_platforms = []
        for platform_key in VIDEO_PRIORITY_PLATFORMS:
            if platform_key in PLATFORM_MAPPING:
                platform_info = PLATFORM_MAPPING[platform_key]
                
                # è·å–è§†é¢‘æ•°é‡ç»Ÿè®¡
                video_count = 0
                total_count = 0
                if db_initialized:
                    try:
                        # æ€»æ•°é‡
                        total_sql = f"SELECT COUNT(*) as total FROM {platform_info['table']}"
                        total_result = await db.query(total_sql)
                        total_count = total_result[0]['total'] if total_result else 0
                        
                        # è§†é¢‘æ•°é‡
                        if "video_filter" in platform_info and platform_info["video_filter"]:
                            video_sql = f"SELECT COUNT(*) as total FROM {platform_info['table']} WHERE {platform_info['video_filter']}"
                            video_result = await db.query(video_sql)
                            video_count = video_result[0]['total'] if video_result else 0
                        else:
                            video_count = total_count  # å¦‚æœæ²¡æœ‰è§†é¢‘ç­›é€‰ï¼Œå‡è®¾å…¨éƒ¨æ˜¯è§†é¢‘
                    except:
                        pass
                
                video_platforms.append({
                    "code": platform_key,
                    "name": platform_info["name"],
                    "description": get_platform_description(platform_key),
                    "total_count": total_count,
                    "video_count": video_count,
                    "video_ratio": round(video_count / total_count * 100, 1) if total_count > 0 else 100,
                    "primary_content_type": platform_info.get("primary_content_type", "video")
                })
        
        return {
            "video_priority_platforms": video_platforms,
            "total_platforms": len(video_platforms),
            "total_video_content": sum(p["video_count"] for p in video_platforms),
            "message": "æœ¬å¹³å°ä¸“æ³¨äºçŸ­è§†é¢‘å†…å®¹ï¼Œä»¥ä¸Šä¸ºä¸»è¦çŸ­è§†é¢‘å¹³å°"
        }
        
    except Exception as e:
        utils.logger.error(f"è·å–çŸ­è§†é¢‘å¹³å°ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–çŸ­è§†é¢‘å¹³å°ä¿¡æ¯å¤±è´¥: {str(e)}")

# ==================== æ™ºèƒ½å¤šå¹³å°çˆ¬å–APIç«¯ç‚¹ ====================

class SmartMultiPlatformRequest(BaseModel):
    """æ™ºèƒ½å¤šå¹³å°çˆ¬å–è¯·æ±‚"""
    platforms: List[str] = Field(..., description="ç›®æ ‡å¹³å°åˆ—è¡¨")
    keywords: str = Field(..., description="æœç´¢å…³é”®è¯")
    max_count_per_platform: int = Field(default=20, description="æ¯å¹³å°æœ€å¤§çˆ¬å–æ•°é‡")
    
    # è´¦å·ç­–ç•¥
    account_strategy: str = Field(default="random", description="è´¦å·ä½¿ç”¨ç­–ç•¥: random/round_robin/priority/single")
    enable_anti_bot: bool = Field(default=True, description="å¯ç”¨æ™ºèƒ½åçˆ¬è™«")
    enable_human_intervention: bool = Field(default=True, description="å¯ç”¨äººå·¥å¹²é¢„")
    
    # ä»£ç†ç­–ç•¥é…ç½®
    enable_proxy: bool = Field(default=False, description="å¯ç”¨ä»£ç†æ± ")
    proxy_strategy: str = Field(default="round_robin", description="ä»£ç†ç­–ç•¥: round_robin/random/weighted/failover/sticky")
    proxy_quality: str = Field(default="auto", description="ä»£ç†è´¨é‡: auto/premium/datacenter/mobile")
    
    # å†…å®¹åå¥½
    content_preference: str = Field(default="video_priority", description="å†…å®¹ç±»å‹åå¥½: video_priority/all/video_only")
    sort_preference: str = Field(default="hot", description="æ’åºåå¥½: hot/time/comprehensive")
    
    # é™„åŠ åŠŸèƒ½
    enable_comments: bool = Field(default=True, description="çˆ¬å–è¯„è®º")
    enable_creator_info: bool = Field(default=False, description="è·å–åˆ›ä½œè€…ä¿¡æ¯")
    auto_download_videos: bool = Field(default=False, description="è‡ªåŠ¨ä¸‹è½½è§†é¢‘")
    
    # æ•°æ®å­˜å‚¨é…ç½®
    save_format: str = Field(default="db_only", description="æ•°æ®å­˜å‚¨æ–¹å¼: db_only/db_json/db_csv/all")
    video_process_mode: str = Field(default="metadata_only", description="è§†é¢‘æ–‡ä»¶å¤„ç†: metadata_only/download_later/auto_download")
    video_quality_preset: str = Field(default="auto", description="è§†é¢‘è´¨é‡é¢„è®¾: auto/high/medium/fast")
    max_video_size: int = Field(default=100, description="å•ä¸ªè§†é¢‘æœ€å¤§å°ºå¯¸(MB)")
    enable_file_size_check: bool = Field(default=True, description="å¯ç”¨æ–‡ä»¶å¤§å°æ£€æŸ¥")
    file_naming_rule: str = Field(default="platform_id", description="æ–‡ä»¶å‘½åè§„åˆ™: platform_id/title_author/timestamp_id/custom")
    save_video_metadata: bool = Field(default=True, description="ä¿å­˜è§†é¢‘å…ƒæ•°æ®")
    save_thumbnails: bool = Field(default=False, description="ä¿å­˜ç¼©ç•¥å›¾")

class HumanInterventionRequest(BaseModel):
    """äººå·¥å¹²é¢„è¯·æ±‚"""
    type: str = Field(..., description="å¹²é¢„ç±»å‹: captcha/login_required")
    data: str = Field(..., description="å¹²é¢„æ•°æ®: éªŒè¯ç /ç™»å½•çŠ¶æ€")

@app.post("/api/v1/multi-platform/smart-start", response_model=CrawlerResponse)
async def start_smart_multi_platform_crawler(request: SmartMultiPlatformRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨æ™ºèƒ½å¤šå¹³å°çˆ¬å–ä»»åŠ¡"""
    try:
        # éªŒè¯å¹³å°é€‰æ‹©
        from models.content_models import PLATFORM_MAPPING, VIDEO_PRIORITY_PLATFORMS, TODO_PLATFORMS
        
        invalid_platforms = [p for p in request.platforms if p not in PLATFORM_MAPPING]
        if invalid_platforms:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„å¹³å°: {invalid_platforms}")
        
        # æ£€æŸ¥TODOå¹³å°
        todo_platforms = [p for p in request.platforms if p in TODO_PLATFORMS]
        if todo_platforms:
            utils.logger.warning(f"è¯·æ±‚åŒ…å«TODOå¹³å°: {todo_platforms}")
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"smart_multi_{int(time.time())}_{random.randint(1000, 9999)}"
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        creation_time = datetime.now().isoformat()
        task_data = {
            "task_id": task_id,
            "type": "smart_multi_platform",
            "platforms": request.platforms,
            "keywords": request.keywords,
            "status": "pending",
            "config": request.dict(),
            "created_at": creation_time,
            "requires_intervention": False
        }
        
        # å­˜å‚¨åˆ°å…¨å±€ä»»åŠ¡ç®¡ç†å™¨
        if not hasattr(app.state, 'smart_multi_tasks'):
            app.state.smart_multi_tasks = {}
        app.state.smart_multi_tasks[task_id] = task_data
        
        # å¯åŠ¨åå°ä»»åŠ¡
        background_tasks.add_task(run_smart_multi_platform_task, task_id, request)
        
        utils.logger.info(f"[SMART_MULTI] æ™ºèƒ½å¤šå¹³å°ä»»åŠ¡å·²åˆ›å»º: {task_id}, å¹³å°: {request.platforms}")
        
        return CrawlerResponse(
            task_id=task_id,
            status="pending",
            message=f"æ™ºèƒ½å¤šå¹³å°çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨ï¼Œå°†åœ¨ {len(request.platforms)} ä¸ªå¹³å°æœç´¢å…³é”®è¯: {request.keywords}"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        utils.logger.error(f"[SMART_MULTI] å¯åŠ¨æ™ºèƒ½å¤šå¹³å°ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨æ™ºèƒ½å¤šå¹³å°ä»»åŠ¡å¤±è´¥: {str(e)}")

async def run_smart_multi_platform_task(task_id: str, request: SmartMultiPlatformRequest):
    """è¿è¡Œæ™ºèƒ½å¤šå¹³å°çˆ¬å–ä»»åŠ¡"""
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task_data = app.state.smart_multi_tasks[task_id]
        task_data["status"] = "running"
        task_data["started_at"] = datetime.now().isoformat()
        task_data["platform_progress"] = {}
        
        utils.logger.info(f"[SMART_MULTI] å¼€å§‹æ‰§è¡Œæ™ºèƒ½å¤šå¹³å°ä»»åŠ¡: {task_id}")
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        await db.init_db()
        
        # é€‰æ‹©å’ŒéªŒè¯è´¦å·
        selected_accounts = await select_accounts_for_platforms(request.platforms, request.account_strategy)
        
        # æ›´æ–°è¿›åº¦
        task_data["overall_progress"] = 10
        task_data["message"] = "è´¦å·é€‰æ‹©å®Œæˆï¼Œå¼€å§‹çˆ¬å–..."
        
        all_results = []
        platform_count = len(request.platforms)
        
        for idx, platform in enumerate(request.platforms):
            try:
                # æ›´æ–°å¹³å°è¿›åº¦
                task_data["platform_progress"][platform] = {
                    "status": "running",
                    "message": "æ­£åœ¨çˆ¬å–...",
                    "completed": 0,
                    "total": request.max_count_per_platform
                }
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥å¹²é¢„
                if await requires_human_intervention(platform, task_id):
                    task_data["requires_intervention"] = True
                    task_data["intervention_data"] = await get_intervention_data(platform)
                    
                    # ç­‰å¾…äººå·¥å¹²é¢„å®Œæˆ
                    while task_data.get("requires_intervention", False):
                        await asyncio.sleep(5)
                
                # æ‰§è¡Œçˆ¬å–ä»»åŠ¡
                platform_results = await crawl_platform_smart(
                    platform, 
                    request.keywords, 
                    request.max_count_per_platform,
                    selected_accounts.get(platform),
                    request
                )
                
                all_results.extend(platform_results)
                
                # æ›´æ–°å¹³å°å®ŒæˆçŠ¶æ€
                task_data["platform_progress"][platform] = {
                    "status": "completed",
                    "message": f"å®Œæˆï¼Œè·å– {len(platform_results)} æ¡å†…å®¹",
                    "completed": len(platform_results),
                    "total": request.max_count_per_platform
                }
                
                # æ›´æ–°æ€»ä½“è¿›åº¦
                task_data["overall_progress"] = 10 + int((idx + 1) / platform_count * 80)
                
            except Exception as e:
                utils.logger.error(f"[SMART_MULTI] å¹³å° {platform} çˆ¬å–å¤±è´¥: {e}")
                task_data["platform_progress"][platform] = {
                    "status": "error",
                    "message": f"çˆ¬å–å¤±è´¥: {str(e)}",
                    "completed": 0,
                    "total": request.max_count_per_platform
                }
        
        # æŒ‰çƒ­åº¦æ’åºç»“æœ
        if request.sort_preference == "hot":
            all_results.sort(key=lambda x: (x.get('like_count', 0) + x.get('view_count', 0) + x.get('comment_count', 0)), reverse=True)
        elif request.sort_preference == "time":
            all_results.sort(key=lambda x: x.get('publish_time', ''), reverse=True)
        
        # è‡ªåŠ¨ä¸‹è½½è§†é¢‘
        if request.auto_download_videos:
            await download_videos_batch(all_results, request.video_storage_type, request.video_quality)
        
        # æ›´æ–°ä»»åŠ¡å®ŒæˆçŠ¶æ€
        task_data["status"] = "completed"
        task_data["completed_at"] = datetime.now().isoformat()
        task_data["overall_progress"] = 100
        task_data["results"] = all_results
        task_data["message"] = f"ä»»åŠ¡å®Œæˆï¼Œå…±è·å– {len(all_results)} æ¡å†…å®¹"
        
        utils.logger.info(f"[SMART_MULTI] æ™ºèƒ½å¤šå¹³å°ä»»åŠ¡å®Œæˆ: {task_id}, æ€»å…±è·å– {len(all_results)} æ¡å†…å®¹")
        
    except Exception as e:
        utils.logger.error(f"[SMART_MULTI] æ™ºèƒ½å¤šå¹³å°ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task_id}, é”™è¯¯: {e}")
        task_data["status"] = "failed"
        task_data["error"] = str(e)
        task_data["overall_progress"] = 0

async def select_accounts_for_platforms(platforms: List[str], strategy: str) -> Dict[str, Optional[int]]:
    """ä¸ºå„å¹³å°é€‰æ‹©è´¦å·"""
    selected_accounts = {}
    
    for platform in platforms:
        try:
            # è·å–å¹³å°å¯ç”¨è´¦å·
            accounts_response = await get_platform_accounts(platform)
            if not accounts_response or not isinstance(accounts_response, list):
                selected_accounts[platform] = None
                continue
            
            # ç­›é€‰å·²ç™»å½•è´¦å·
            logged_in_accounts = [acc for acc in accounts_response if acc.get('login_status') == 'logged_in']
            
            if not logged_in_accounts:
                utils.logger.warning(f"å¹³å° {platform} æ²¡æœ‰å·²ç™»å½•è´¦å·")
                selected_accounts[platform] = None
                continue
            
            # æ ¹æ®ç­–ç•¥é€‰æ‹©è´¦å·
            if strategy == "random":
                selected_account = random.choice(logged_in_accounts)
            elif strategy == "round_robin":
                # ç®€åŒ–å®ç°ï¼Œé€‰æ‹©ç¬¬ä¸€ä¸ª
                selected_account = logged_in_accounts[0]
            elif strategy == "priority":
                # æŒ‰æ´»è·ƒåº¦æ’åºï¼Œé€‰æ‹©æœ€æ´»è·ƒçš„
                selected_account = max(logged_in_accounts, key=lambda x: x.get('activity_score', 0))
            else:  # single æˆ–å…¶ä»–
                selected_account = logged_in_accounts[0]
            
            selected_accounts[platform] = selected_account.get('id')
            utils.logger.info(f"[ACCOUNT_SELECT] å¹³å° {platform} é€‰æ‹©è´¦å·: {selected_account.get('username', 'unknown')}")
            
        except Exception as e:
            utils.logger.error(f"[ACCOUNT_SELECT] å¹³å° {platform} è´¦å·é€‰æ‹©å¤±è´¥: {e}")
            selected_accounts[platform] = None
    
    return selected_accounts

async def requires_human_intervention(platform: str, task_id: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥å¹²é¢„"""
    # æ¨¡æ‹Ÿæ£€æŸ¥é€»è¾‘
    # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šæ£€æŸ¥å¹³å°çš„åçˆ¬è™«çŠ¶æ€
    return False

async def get_intervention_data(platform: str) -> Dict:
    """è·å–äººå·¥å¹²é¢„æ•°æ®"""
    # æ¨¡æ‹Ÿç”Ÿæˆå¹²é¢„æ•°æ®
    return {
        "platform": platform,
        "type": "captcha",
        "description": "éœ€è¦è¾“å…¥éªŒè¯ç ",
        "captcha_image": "/static/captcha_placeholder.png"
    }

async def crawl_platform_smart(platform: str, keywords: str, max_count: int, account_id: Optional[int], config: SmartMultiPlatformRequest) -> List[Dict]:
    """æ™ºèƒ½çˆ¬å–å•ä¸ªå¹³å°"""
    try:
        # æ„å»ºä¼ ç»Ÿçˆ¬è™«è¯·æ±‚
        crawler_request = CrawlerRequest(
            platform=platform,
            keywords=keywords,
            max_notes_count=max_count,
            account_id=account_id,
            get_comments=config.enable_comments,
            save_data_option=config.save_format,
            use_proxy=config.enable_proxy,
            video_priority=(config.content_preference == "video_priority"),
            video_only=(config.content_preference == "video_only")
        )
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = CrawlerFactory.create_crawler(platform)
        if not crawler:
            raise Exception(f"æ— æ³•åˆ›å»º {platform} çˆ¬è™«")
        
        # æ‰§è¡Œçˆ¬å–
        await crawler.run(crawler_request)
        
        # ä»æ•°æ®åº“è¯»å–ç»“æœ
        from models.content_models import PLATFORM_MAPPING
        platform_info = PLATFORM_MAPPING.get(platform, {})
        table_name = platform_info.get("table", "")
        
        if table_name:
            # æŸ¥è¯¢æœ€è¿‘çˆ¬å–çš„æ•°æ®
            sql = f"""
            SELECT * FROM {table_name} 
            WHERE source_keyword = %s 
            ORDER BY add_ts DESC 
            LIMIT %s
            """
            results = await db.query(sql, keywords, max_count)
            results_list = [dict(row) for row in results]
            
            # ä¿å­˜è§†é¢‘å…ƒæ•°æ®åˆ°video_filesè¡¨
            if config.save_video_metadata and results_list:
                await save_video_metadata_to_files_table(results_list, platform, task_id="smart_" + keywords, config=config)
            
            return results_list
        
        return []
        
    except Exception as e:
        utils.logger.error(f"[CRAWL_SMART] å¹³å° {platform} æ™ºèƒ½çˆ¬å–å¤±è´¥: {e}")
        return []

async def save_video_metadata_to_files_table(results: List[Dict], platform: str, task_id: str, config: SmartMultiPlatformRequest):
    """å°†è§†é¢‘å…ƒæ•°æ®ä¿å­˜åˆ°video_filesè¡¨"""
    try:
        from db_video_files import VideoFileManager
        
        video_file_manager = VideoFileManager()
        saved_count = 0
        
        for result in results:
            # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘URL
            video_url = result.get('video_url') or result.get('video_download_url')
            if not video_url:
                continue
            
            # æ„å»ºè§†é¢‘æ–‡ä»¶ä¿¡æ¯
            video_info = {
                'platform': platform,
                'content_id': str(result.get('note_id') or result.get('aweme_id') or result.get('id', '')),
                'task_id': task_id,
                'original_url': video_url,
                'title': result.get('title') or result.get('desc', ''),
                'author_name': result.get('nickname') or result.get('author_name', ''),
                'duration': result.get('video_duration'),
                'video_format': 'mp4',  # é»˜è®¤æ ¼å¼
                'storage_type': 'url_only' if config.video_process_mode == 'metadata_only' else 'temp',
                'thumbnail_url': result.get('thumbnail_url') or result.get('cover_url'),
                'metadata': {
                    'like_count': result.get('like_count', 0),
                    'comment_count': result.get('comment_count', 0),
                    'view_count': result.get('view_count', 0),
                    'share_count': result.get('share_count', 0),
                    'publish_time': result.get('publish_time'),
                    'platform_specific': result  # ä¿å­˜åŸå§‹æ•°æ®
                }
            }
            
            # å°è¯•ä»è§†é¢‘ä¿¡æ¯ä¸­æå–æ›´å¤šæŠ€æœ¯å‚æ•°
            if 'video_info' in result:
                video_details = result['video_info']
                if isinstance(video_details, dict):
                    video_info.update({
                        'resolution': video_details.get('resolution'),
                        'bitrate': video_details.get('bitrate'),
                        'fps': video_details.get('fps'),
                        'file_size': video_details.get('file_size')
                    })
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            file_id = await video_file_manager.save_video_metadata(video_info)
            if file_id:
                saved_count += 1
        
        utils.logger.info(f"[VIDEO_METADATA] æˆåŠŸä¿å­˜ {saved_count} ä¸ªè§†é¢‘å…ƒæ•°æ®åˆ°filesè¡¨")
        
    except Exception as e:
        utils.logger.error(f"[VIDEO_METADATA] ä¿å­˜è§†é¢‘å…ƒæ•°æ®å¤±è´¥: {e}")

async def download_videos_batch(results: List[Dict], storage_type: str, quality: str):
    """æ‰¹é‡ä¸‹è½½è§†é¢‘"""
    try:
        utils.logger.info(f"[VIDEO_DOWNLOAD] å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(results)} ä¸ªè§†é¢‘")
        
        # è¿™é‡Œå®ç°è§†é¢‘ä¸‹è½½é€»è¾‘
        # æ ¹æ®storage_typeå†³å®šå­˜å‚¨æ–¹å¼
        # æ ¹æ®qualityå†³å®šä¸‹è½½è´¨é‡
        
        for result in results:
            video_url = result.get('video_url')
            if video_url:
                # å®é™…ä¸‹è½½é€»è¾‘
                pass
        
        utils.logger.info(f"[VIDEO_DOWNLOAD] æ‰¹é‡ä¸‹è½½å®Œæˆ")
        
    except Exception as e:
        utils.logger.error(f"[VIDEO_DOWNLOAD] æ‰¹é‡ä¸‹è½½å¤±è´¥: {e}")

@app.get("/api/v1/multi-platform/status/{task_id}")
async def get_smart_multi_platform_status(task_id: str):
    """è·å–æ™ºèƒ½å¤šå¹³å°ä»»åŠ¡çŠ¶æ€"""
    try:
        if not hasattr(app.state, 'smart_multi_tasks') or task_id not in app.state.smart_multi_tasks:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        task_data = app.state.smart_multi_tasks[task_id]
        return task_data
        
    except HTTPException:
        raise
    except Exception as e:
        utils.logger.error(f"[SMART_MULTI_STATUS] è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")

@app.post("/api/v1/multi-platform/intervention/{task_id}")
async def handle_human_intervention(task_id: str, request: HumanInterventionRequest):
    """å¤„ç†äººå·¥å¹²é¢„"""
    try:
        if not hasattr(app.state, 'smart_multi_tasks') or task_id not in app.state.smart_multi_tasks:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        task_data = app.state.smart_multi_tasks[task_id]
        
        # å¤„ç†ä¸åŒç±»å‹çš„å¹²é¢„
        if request.type == "captcha":
            # å¤„ç†éªŒè¯ç 
            utils.logger.info(f"[INTERVENTION] æ”¶åˆ°éªŒè¯ç : {request.data}")
            # è¿™é‡Œå®ç°éªŒè¯ç æäº¤é€»è¾‘
            
        elif request.type == "login_required":
            # å¤„ç†ç™»å½•è¦æ±‚
            utils.logger.info(f"[INTERVENTION] å¤„ç†ç™»å½•è¦æ±‚")
            # è¿™é‡Œå®ç°ç™»å½•å¤„ç†é€»è¾‘
        
        # æ¸…é™¤å¹²é¢„æ ‡å¿—
        task_data["requires_intervention"] = False
        task_data["intervention_data"] = None
        
        return {"status": "success", "message": "äººå·¥å¹²é¢„å¤„ç†å®Œæˆ"}
        
    except HTTPException:
        raise
    except Exception as e:
        utils.logger.error(f"[INTERVENTION] å¤„ç†äººå·¥å¹²é¢„å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†äººå·¥å¹²é¢„å¤±è´¥: {str(e)}")

@app.post("/api/v1/download/video/{platform}/{content_id}")
async def download_single_video(platform: str, content_id: str):
    """ä¸‹è½½å•ä¸ªè§†é¢‘"""
    try:
        # ä»æ•°æ®åº“è·å–è§†é¢‘ä¿¡æ¯
        from models.content_models import PLATFORM_MAPPING
        platform_info = PLATFORM_MAPPING.get(platform)
        if not platform_info:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„å¹³å°")
        
        table_name = platform_info["table"]
        id_field = platform_info["id_field"]
        
        sql = f"SELECT * FROM {table_name} WHERE {id_field} = %s"
        rows = await db.query(sql, content_id)
        
        if not rows:
            raise HTTPException(status_code=404, detail="å†…å®¹ä¸å­˜åœ¨")
        
        row = rows[0]
        video_url = row.get('video_url') or row.get('video_download_url')
        
        if not video_url:
            raise HTTPException(status_code=404, detail="è¯¥å†…å®¹æ²¡æœ‰è§†é¢‘æ–‡ä»¶")
        
        # è¿™é‡Œå®ç°è§†é¢‘ä¸‹è½½é€»è¾‘
        # è¿”å›è§†é¢‘æ–‡ä»¶æµæˆ–ä¸‹è½½é“¾æ¥
        
        return {"status": "success", "message": "è§†é¢‘ä¸‹è½½å·²å¼€å§‹", "download_url": video_url}
        
    except HTTPException:
        raise
    except Exception as e:
        utils.logger.error(f"[VIDEO_DOWNLOAD] ä¸‹è½½å•ä¸ªè§†é¢‘å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½è§†é¢‘å¤±è´¥: {str(e)}")

@app.post("/api/v1/download/batch/{task_id}")
async def download_batch_videos(task_id: str):
    """æ‰¹é‡ä¸‹è½½ä»»åŠ¡ç›¸å…³çš„æ‰€æœ‰è§†é¢‘"""
    try:
        if not hasattr(app.state, 'smart_multi_tasks') or task_id not in app.state.smart_multi_tasks:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        task_data = app.state.smart_multi_tasks[task_id]
        results = task_data.get("results", [])
        
        if not results:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡æ²¡æœ‰ç»“æœæ•°æ®")
        
        # å¯åŠ¨æ‰¹é‡ä¸‹è½½
        video_count = len([r for r in results if r.get('video_url')])
        
        # è¿™é‡Œå®ç°æ‰¹é‡ä¸‹è½½é€»è¾‘
        utils.logger.info(f"[BATCH_DOWNLOAD] å¼€å§‹æ‰¹é‡ä¸‹è½½ {video_count} ä¸ªè§†é¢‘")
        
        return {
            "status": "success", 
            "message": f"æ‰¹é‡ä¸‹è½½å·²å¼€å§‹ï¼Œå…± {video_count} ä¸ªè§†é¢‘",
            "video_count": video_count
        }
        
    except HTTPException:
        raise
    except Exception as e:
        utils.logger.error(f"[BATCH_DOWNLOAD] æ‰¹é‡ä¸‹è½½å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡ä¸‹è½½å¤±è´¥: {str(e)}")

@app.get("/api/v1/export/results/{task_id}")
async def export_task_results(task_id: str, format: str = "json"):
    """å¯¼å‡ºä»»åŠ¡ç»“æœ"""
    try:
        if not hasattr(app.state, 'smart_multi_tasks') or task_id not in app.state.smart_multi_tasks:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        task_data = app.state.smart_multi_tasks[task_id]
        results = task_data.get("results", [])
        
        if not results:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡æ²¡æœ‰ç»“æœæ•°æ®")
        
        if format == "json":
            return {"task_id": task_id, "results": results, "total": len(results)}
        elif format == "csv":
            # å®ç°CSVå¯¼å‡º
            import io
            import csv
            
            output = io.StringIO()
            if results:
                writer = csv.DictWriter(output, fieldnames=results[0].keys())
                writer.writeheader()
                writer.writerows(results)
            
            return {"status": "success", "data": output.getvalue(), "format": "csv"}
        elif format == "excel":
            # å®ç°Excelå¯¼å‡º
            return {"status": "success", "message": "Excelå¯¼å‡ºåŠŸèƒ½å¼€å‘ä¸­", "format": "excel"}
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼")
        
    except HTTPException:
        raise
    except Exception as e:
        utils.logger.error(f"[EXPORT] å¯¼å‡ºç»“æœå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯¼å‡ºç»“æœå¤±è´¥: {str(e)}")

# ==================== æ™ºèƒ½å¤šå¹³å°çˆ¬å–APIç«¯ç‚¹ç»“æŸ ====================

if __name__ == "__main__":
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        access_log=True
    ) 