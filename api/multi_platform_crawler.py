"""
å¤šå¹³å°çˆ¬å–æ ¸å¿ƒæ¨¡å—
æ”¯æŒåŒæ—¶çˆ¬å–å¤šä¸ªå¹³å°ï¼Œç»Ÿä¸€æ•°æ®æ ¼å¼å­˜å‚¨
"""

import asyncio
import uuid
import json
from datetime import datetime
from typing import Dict, Any, Optional, List
from enum import Enum
from fastapi import APIRouter, BackgroundTasks, HTTPException
from pydantic import BaseModel, Field
from tools import utils
from var import media_crawler_db_var

# å¯¼å…¥æ•°æ®æ¨¡å‹
from models.content_models import (
    MultiPlatformCrawlerRequest, MultiPlatformTaskStatusResponse,
    UnifiedResultResponse
)

router = APIRouter()

# å…¨å±€å¤šå¹³å°ä»»åŠ¡çŠ¶æ€å­˜å‚¨
multi_platform_task_status = {}

# ğŸ†• å¤šå¹³å°ä»»åŠ¡æ¸…ç†é…ç½®
MULTI_TASK_CLEANUP_INTERVAL = 3600  # 1å°æ—¶æ¸…ç†ä¸€æ¬¡
MULTI_TASK_MAX_AGE = 86400  # 24å°æ—¶åæ¸…ç†ä»»åŠ¡çŠ¶æ€

async def cleanup_old_multi_platform_tasks():
    """æ¸…ç†è¿‡æœŸçš„å¤šå¹³å°ä»»åŠ¡çŠ¶æ€"""
    try:
        from datetime import datetime, timedelta
        current_time = datetime.now()
        
        tasks_to_remove = []
        for task_id, task_info in multi_platform_task_status.items():
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¶…è¿‡24å°æ—¶
            if 'created_at' in task_info:
                created_time = datetime.fromisoformat(task_info['created_at'])
                if current_time - created_time > timedelta(seconds=MULTI_TASK_MAX_AGE):
                    tasks_to_remove.append(task_id)
        
        # ç§»é™¤è¿‡æœŸä»»åŠ¡
        for task_id in tasks_to_remove:
            del multi_platform_task_status[task_id]
            utils.logger.info(f"æ¸…ç†è¿‡æœŸå¤šå¹³å°ä»»åŠ¡çŠ¶æ€: {task_id}")
        
        if tasks_to_remove:
            utils.logger.info(f"æ¸…ç†äº† {len(tasks_to_remove)} ä¸ªè¿‡æœŸå¤šå¹³å°ä»»åŠ¡çŠ¶æ€")
            
    except Exception as e:
        utils.logger.error(f"æ¸…ç†è¿‡æœŸå¤šå¹³å°ä»»åŠ¡å¤±è´¥: {e}")

# ğŸ†• å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
import asyncio
async def start_multi_platform_task_cleanup():
    """å¯åŠ¨å®šæœŸå¤šå¹³å°ä»»åŠ¡æ¸…ç†"""
    while True:
        try:
            await asyncio.sleep(MULTI_TASK_CLEANUP_INTERVAL)
            await cleanup_old_multi_platform_tasks()
        except Exception as e:
            utils.logger.error(f"å¤šå¹³å°ä»»åŠ¡æ¸…ç†å¾ªç¯å¤±è´¥: {e}")
            await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†é‡è¯•

class AccountStrategy(str, Enum):
    """è´¦å·ç­–ç•¥æšä¸¾"""
    RANDOM = "random"           # éšæœºé€‰æ‹©
    ROUND_ROBIN = "round_robin" # è½®è¯¢é€‰æ‹©
    PRIORITY = "priority"        # ä¼˜å…ˆçº§é€‰æ‹©
    SMART = "smart"             # æ™ºèƒ½é€‰æ‹©ï¼ˆæ ¹æ®ç™»å½•çŠ¶æ€ã€æˆåŠŸç‡ç­‰ï¼‰
    SINGLE = "single"           # å•è´¦å·ä½¿ç”¨

class MultiPlatformCrawlerFactory:
    """å¤šå¹³å°çˆ¬è™«å·¥å‚ç±»"""
    
    # æ”¯æŒçš„å¹³å°
    SUPPORTED_PLATFORMS = ["xhs", "dy", "ks", "bili"]
    
    @staticmethod
    def _get_crawler_class(platform: str):
        """å»¶è¿Ÿå¯¼å…¥çˆ¬è™«ç±»"""
        if platform == "xhs":
            from media_platform.xhs import XiaoHongShuCrawler
            return XiaoHongShuCrawler
        elif platform == "dy":
            from media_platform.douyin import DouYinCrawler
            return DouYinCrawler
        elif platform == "ks":
            from media_platform.kuaishou import KuaishouCrawler
            return KuaishouCrawler
        elif platform == "bili":
            from media_platform.bilibili import BilibiliCrawler
            return BilibiliCrawler
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")

    @staticmethod
    def create_crawler(platform: str, task_id: str = None):
        """åˆ›å»ºçˆ¬è™«å®ä¾‹"""
        crawler_class = MultiPlatformCrawlerFactory._get_crawler_class(platform)
        return crawler_class(task_id=task_id)

# ğŸ†• å¤ç”¨å•å¹³å°çˆ¬è™«çš„æ•°æ®åº“è¿æ¥ç®¡ç†
from api.crawler_core import get_db_connection, close_db_connection

async def create_multi_platform_task_record(task_id: str, request: MultiPlatformCrawlerRequest) -> None:
    """åˆ›å»ºå¤šå¹³å°ä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“"""
    try:
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[MULTI_TASK_RECORD] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºä»»åŠ¡å‚æ•°JSON
        task_params = {
            "platforms": request.platforms,
            "keywords": request.keywords,
            "max_count_per_platform": request.max_count_per_platform,
            "enable_comments": request.enable_comments,
            "enable_images": request.enable_images,
            "save_format": request.save_format,
            "use_proxy": request.use_proxy,
            "proxy_ip": request.proxy_ip,  # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨proxy_ipè€Œä¸æ˜¯proxy_strategy
            "account_strategy": request.account_strategy if hasattr(request, 'account_strategy') else "smart",
            "execution_mode": request.execution_mode if hasattr(request, 'execution_mode') else "parallel"
        }
        
        # ä½¿ç”¨å­—å…¸æ–¹å¼æ„å»ºæ•°æ®
        task_data = {
            'id': task_id,
            'platform': ','.join(request.platforms),  # å¤šå¹³å°ç”¨é€—å·åˆ†éš”
            'task_type': 'multi_platform',
            'crawler_type': 'search',  # å¤šå¹³å°é»˜è®¤ä¸ºæœç´¢æ¨¡å¼
            'creator_ref_ids': None,
            'keywords': request.keywords,
            'status': 'pending',
            'progress': 0.0,
            'result_count': 0,
            'error_message': None,
            'user_id': None,
            'params': json.dumps(task_params),
            'priority': 0,
            'is_favorite': False,
            'deleted': False,
            'is_pinned': False,
            'ip_address': None,
            'user_security_id': None,
            'user_signature': None,
            'created_at': datetime.now(),
            'updated_at': datetime.now(),
            'started_at': None,
            'completed_at': None
        }
        
        await async_db_obj.item_to_table('crawler_tasks', task_data)
        utils.logger.info(f"[MULTI_TASK_RECORD] å¤šå¹³å°ä»»åŠ¡è®°å½•åˆ›å»ºæˆåŠŸ: {task_id}")
        
    except Exception as e:
        utils.logger.error(f"[MULTI_TASK_RECORD] åˆ›å»ºå¤šå¹³å°ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
        raise

async def update_multi_platform_task_progress(task_id: str, progress: float, status: str = None, 
                                           platform_results: Dict[str, int] = None):
    """æ›´æ–°å¤šå¹³å°ä»»åŠ¡è¿›åº¦"""
    try:
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[MULTI_TASK_PROGRESS] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºæ›´æ–°æ•°æ®å­—å…¸
        update_data = {
            'progress': progress,
            'updated_at': datetime.now()
        }
        
        if status:
            update_data['status'] = status
        
        if platform_results:
            total_results = sum(platform_results.values())
            update_data['result_count'] = total_results
        
        await async_db_obj.update_table('crawler_tasks', update_data, 'id', task_id)
        
        utils.logger.info(f"[MULTI_TASK_PROGRESS] å¤šå¹³å°ä»»åŠ¡è¿›åº¦æ›´æ–°: {task_id}, è¿›åº¦: {progress}, çŠ¶æ€: {status}")
        
    except Exception as e:
        utils.logger.error(f"[MULTI_TASK_PROGRESS] æ›´æ–°å¤šå¹³å°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

async def log_multi_platform_task_step(task_id: str, platform: str, step: str, message: str, 
                                     log_level: str = "INFO", progress: int = None):
    """è®°å½•å¤šå¹³å°ä»»åŠ¡æ­¥éª¤æ—¥å¿—"""
    try:
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[MULTI_TASK_LOG] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºæ—¥å¿—æ•°æ®å­—å…¸
        log_data = {
            'task_id': task_id,
            'platform': platform,
            'account_id': None,
            'log_level': log_level,
            'message': message,
            'step': step,
            'progress': progress or 0,
            'created_at': datetime.now()
        }
        
        await async_db_obj.item_to_table('crawler_task_logs', log_data)
        utils.logger.info(f"[MULTI_TASK_LOG] {task_id} - {platform} - {step}: {message}")
        
    except Exception as e:
        utils.logger.error(f"[MULTI_TASK_LOG] è®°å½•å¤šå¹³å°ä»»åŠ¡æ—¥å¿—å¤±è´¥: {e}")

async def get_platform_accounts(platform: str, account_strategy: str = "smart") -> List[Dict]:
    """æ ¹æ®ç­–ç•¥è·å–å¹³å°è´¦å·åˆ—è¡¨"""
    try:
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            return []
        
        # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„è¡¨ç»“æ„ï¼Œå‚è€ƒå•å¹³å°çˆ¬å–çš„è´¦å·ç®¡ç†é€»è¾‘
        # è·å–è¯¥å¹³å°çš„æ‰€æœ‰å¯ç”¨è´¦å·ï¼ˆä»social_accountsè¡¨ï¼‰
        query = """
            SELECT sa.id, sa.account_name, sa.username, sa.platform, sa.login_method,
                   lt.is_valid, lt.expires_at, lt.last_used_at, lt.created_at as token_created_at
            FROM social_accounts sa
            LEFT JOIN login_tokens lt ON sa.id = lt.account_id AND sa.platform = lt.platform
            WHERE sa.platform = %s
            ORDER BY 
                CASE 
                    WHEN lt.is_valid = 1 AND lt.expires_at > NOW() THEN 1
                    ELSE 2
                END,
                lt.created_at DESC,
                sa.created_at DESC
        """
        
        accounts = await async_db_obj.query(query, platform)
        
        if not accounts:
            utils.logger.warning(f"å¹³å° {platform} æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è´¦å·")
            return []
        
        # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„è´¦å·ï¼ˆæœ‰æœ‰æ•ˆtokençš„è´¦å·ï¼‰
        valid_accounts = []
        for account in accounts:
            if account.get('is_valid') == 1 and account.get('expires_at'):
                # æ£€æŸ¥tokenæ˜¯å¦è¿‡æœŸ
                from datetime import datetime
                expires_at = account['expires_at']
                if isinstance(expires_at, str):
                    from datetime import datetime
                    expires_at = datetime.fromisoformat(expires_at.replace('Z', '+00:00'))
                
                if expires_at > datetime.now():
                    valid_accounts.append(account)
        
        if not valid_accounts:
            utils.logger.warning(f"å¹³å° {platform} æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç™»å½•è´¦å·")
            return []
        
        utils.logger.info(f"å¹³å° {platform} æ‰¾åˆ° {len(valid_accounts)} ä¸ªæœ‰æ•ˆè´¦å·")
        
        # æ ¹æ®ç­–ç•¥é€‰æ‹©è´¦å·
        if account_strategy == "random":
            import random
            selected = random.sample(valid_accounts, min(len(valid_accounts), 3))
            utils.logger.info(f"éšæœºé€‰æ‹© {len(selected)} ä¸ªè´¦å·")
            return selected
        elif account_strategy == "round_robin":
            # è½®è¯¢é€‰æ‹©ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œé€‰æ‹©å‰å‡ ä¸ª
            selected = valid_accounts[:min(len(valid_accounts), 2)]
            utils.logger.info(f"è½®è¯¢é€‰æ‹© {len(selected)} ä¸ªè´¦å·")
            return selected
        elif account_strategy == "priority":
            # ä¼˜å…ˆçº§é€‰æ‹©ï¼šæœ‰æ•ˆtoken > æœ€è¿‘ä½¿ç”¨
            selected = valid_accounts[:min(len(valid_accounts), 2)]
            utils.logger.info(f"ä¼˜å…ˆçº§é€‰æ‹© {len(selected)} ä¸ªè´¦å·")
            return selected
        elif account_strategy == "smart":
            # æ™ºèƒ½é€‰æ‹©ï¼šç»¼åˆè€ƒè™‘tokenæœ‰æ•ˆæ€§ã€ä½¿ç”¨é¢‘ç‡
            # æŒ‰tokenåˆ›å»ºæ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
            smart_accounts = sorted(valid_accounts, 
                                 key=lambda x: x.get('token_created_at', datetime.min), 
                                 reverse=True)
            selected = smart_accounts[:min(len(smart_accounts), 2)]
            utils.logger.info(f"æ™ºèƒ½é€‰æ‹© {len(selected)} ä¸ªè´¦å·")
            return selected
        elif account_strategy == "single":
            # å•è´¦å·ä½¿ç”¨
            selected = valid_accounts[:1]
            utils.logger.info(f"å•è´¦å·é€‰æ‹© {len(selected)} ä¸ªè´¦å·")
            return selected
        else:
            # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª
            selected = valid_accounts[:1]
            utils.logger.info(f"é»˜è®¤é€‰æ‹© {len(selected)} ä¸ªè´¦å·")
            return selected
            
    except Exception as e:
        utils.logger.error(f"è·å–å¹³å° {platform} è´¦å·å¤±è´¥: {e}")
        return []

async def run_single_platform_crawler(task_id: str, platform: str, request: MultiPlatformCrawlerRequest, 
                                    account_strategy: str = "smart", execution_mode: str = "parallel", proxy_info=None):
    """è¿è¡Œå•ä¸ªå¹³å°çš„çˆ¬è™«ä»»åŠ¡"""
    # ğŸ†• å¯¼å…¥é”™è¯¯å¤„ç†æ¨¡å—
    from utils.crawler_error_handler import create_error_handler, RetryConfig, ErrorType, RetryableCrawlerOperation
    
    try:
        utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸš€ å¼€å§‹æ‰§è¡Œå¹³å° {platform} çˆ¬å–ä»»åŠ¡")
        
        # ğŸ†• åˆ›å»ºé”™è¯¯å¤„ç†å™¨
        retry_config = RetryConfig(
            max_retries=3,
            base_delay=2.0,
            max_delay=30.0,
            account_switch_enabled=True,
            max_account_switches=3
        )
        error_handler = await create_error_handler(platform, task_id, retry_config)
        utils.logger.info(f"[MULTI_TASK_{task_id}] âœ… å¹³å° {platform} é”™è¯¯å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # è·å–å¹³å°è´¦å·
        accounts = await get_platform_accounts(platform, account_strategy)
        if not accounts:
            raise Exception(f"å¹³å° {platform} æ²¡æœ‰å¯ç”¨è´¦å·")
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªè´¦å·
        selected_account = accounts[0]
        account_id = selected_account['id']
        
        await log_multi_platform_task_step(task_id, platform, "account_selected", 
                                         f"é€‰æ‹©è´¦å·: {selected_account.get('account_name', 'æœªçŸ¥')} (ID: {account_id})")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = MultiPlatformCrawlerFactory.create_crawler(platform, task_id=task_id)
        # ğŸ†• æ ‡è®°æµè§ˆå™¨ç”±å¤–éƒ¨ç®¡ç†ï¼Œé¿å…é‡å¤å…³é—­
        crawler._externally_managed = True
        
        # ğŸ†• è®¾ç½®ä»£ç†ä¿¡æ¯
        if proxy_info and request.use_proxy:
            crawler.proxy_info = proxy_info
            utils.logger.info(f"[MULTI_TASK_{task_id}] è®¾ç½®ä»£ç†: {proxy_info.ip}:{proxy_info.port}")
            # ğŸ†• æ‰“å°ä»£ç†ä½¿ç”¨ä¿¡æ¯
            utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸŒ å¹³å° {platform} ä»£ç†ä½¿ç”¨ä¿¡æ¯:")
            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ä»£ç†åœ°å€: {proxy_info.ip}:{proxy_info.port}")
            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ä»£ç†ç±»å‹: {proxy_info.proxy_type}")
            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ è®¤è¯ä¿¡æ¯: {proxy_info.username}:{proxy_info.password}")
            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ åŒºåŸŸ: {proxy_info.area}")
            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ æè¿°: {proxy_info.description}")
            utils.logger.info(f"[MULTI_TASK_{task_id}]   â””â”€ ä½¿ç”¨æ–¹å¼: curl -x {proxy_info.proxy_type}://{proxy_info.username}:{proxy_info.password}@{proxy_info.ip}:{proxy_info.port} https://httpbin.org/ip")
            await log_multi_platform_task_step(task_id, platform, "proxy_setup", f"è®¾ç½®ä»£ç†: {proxy_info.ip}:{proxy_info.port}", "INFO")
        
        # ğŸ†• æ¸…ç©ºä¹‹å‰æ”¶é›†çš„æ•°æ®ï¼Œç¡®ä¿æ–°ä»»åŠ¡çš„æ•°æ®æ­£ç¡®
        try:
            if platform == "xhs":
                from store.xhs import _clear_collected_data
                _clear_collected_data()
            elif platform == "dy":
                if hasattr(crawler, 'douyin_store') and hasattr(crawler.douyin_store, 'clear_collected_data'):
                    crawler.douyin_store.clear_collected_data()
            elif platform == "ks":
                from store.kuaishou import _clear_collected_data
                _clear_collected_data()
            elif platform == "bili":
                from store.bilibili import _clear_collected_data
                _clear_collected_data()
            elif platform == "wb":
                from store.weibo import _clear_collected_data
                _clear_collected_data()
            elif platform == "zhihu":
                from store.zhihu import _clear_collected_data
                _clear_collected_data()
            elif platform == "tieba":
                from store.tieba import _clear_collected_data
                _clear_collected_data()
        except Exception as e:
            utils.logger.warning(f"[MULTI_TASK_{task_id}] æ¸…ç©ºæ•°æ®å¤±è´¥: {e}")
        
        # è®¾ç½®çˆ¬è™«é…ç½®
        import config
        config.PLATFORM = platform
        config.ENABLE_GET_COMMENTS = request.enable_comments
        config.SAVE_DATA_OPTION = "db"  # å¤šå¹³å°å›ºå®šä½¿ç”¨æ•°æ®åº“å­˜å‚¨
        
        # å¼€å§‹çˆ¬å–
        await log_multi_platform_task_step(task_id, platform, "crawling_start", "å¼€å§‹æ‰§è¡Œçˆ¬å–")
        
        # ğŸ†• ä½¿ç”¨é”™è¯¯å¤„ç†å™¨åŒ…è£…çˆ¬å–æ“ä½œ
        async def execute_platform_crawling():
            """æ‰§è¡Œå¹³å°çˆ¬å–æ“ä½œ"""
            return await crawler.search_by_keywords(
                keywords=request.keywords,
                max_count=request.max_count_per_platform,
                account_id=account_id,
                session_id=None,
                login_type="qrcode",
                get_comments=request.enable_comments,
                save_data_option="db",
                use_proxy=request.use_proxy,
                proxy_ip=request.proxy_ip,  # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨proxy_ipè€Œä¸æ˜¯proxy_strategy
                start_page=1  # å¤šå¹³å°çˆ¬å–é»˜è®¤ä»ç¬¬1é¡µå¼€å§‹
            )
        
        # ğŸ†• ä½¿ç”¨é”™è¯¯å¤„ç†å™¨æ‰§è¡Œçˆ¬å–
        retry_op = RetryableCrawlerOperation(error_handler)
        results = await retry_op.execute(execute_platform_crawling)
        
        result_count = len(results) if results else 0
        await log_multi_platform_task_step(task_id, platform, "crawling_completed", 
                                         f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {result_count} æ¡æ•°æ®")
        
        # ğŸ†• è®°å½•é”™è¯¯æ‘˜è¦
        error_summary = error_handler.get_error_summary()
        if error_summary["total_errors"] > 0:
            utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸ“Š å¹³å° {platform} é”™è¯¯å¤„ç†æ‘˜è¦: {error_summary}")
            await log_multi_platform_task_step(task_id, platform, "error_summary", f"é”™è¯¯å¤„ç†æ‘˜è¦: {error_summary}", "INFO")
        
        # ğŸ†• å®‰å…¨å…³é—­çˆ¬è™«èµ„æº
        try:
            if hasattr(crawler, 'close'):
                await crawler.close()
                utils.logger.info(f"[MULTI_TASK_{task_id}] çˆ¬è™«èµ„æºå·²å…³é—­")
        except Exception as e:
            utils.logger.warning(f"[MULTI_TASK_{task_id}] å…³é—­çˆ¬è™«èµ„æºæ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # ğŸ†• ç¡®ä¿æµè§ˆå™¨å®ä¾‹è¢«æ­£ç¡®å…³é—­
        try:
            if hasattr(crawler, 'browser') and crawler.browser:
                await crawler.browser.close()
                utils.logger.info(f"[MULTI_TASK_{task_id}] æµè§ˆå™¨å®ä¾‹å·²å…³é—­")
        except Exception as e:
            utils.logger.warning(f"[MULTI_TASK_{task_id}] å…³é—­æµè§ˆå™¨å®ä¾‹æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        # ğŸ†• æ¸…ç†Playwrightä¸Šä¸‹æ–‡
        try:
            if hasattr(crawler, 'context') and crawler.context:
                await crawler.context.close()
                utils.logger.info(f"[MULTI_TASK_{task_id}] Playwrightä¸Šä¸‹æ–‡å·²å…³é—­")
        except Exception as e:
            utils.logger.warning(f"[MULTI_TASK_{task_id}] å…³é—­Playwrightä¸Šä¸‹æ–‡æ—¶å‡ºç°è­¦å‘Š: {e}")
        
        return result_count
        
    except Exception as e:
        utils.logger.error(f"[MULTI_TASK_{task_id}] âŒ å¹³å° {platform} çˆ¬å–å¤±è´¥: {e}")
        
        # ğŸ†• è®°å½•é”™è¯¯å¤„ç†æ‘˜è¦
        try:
            error_summary = error_handler.get_error_summary()
            utils.logger.error(f"[MULTI_TASK_{task_id}] ğŸ“Š å¹³å° {platform} æœ€ç»ˆé”™è¯¯å¤„ç†æ‘˜è¦: {error_summary}")
        except:
            pass
        
        await log_multi_platform_task_step(task_id, platform, "crawling_failed", f"çˆ¬å–å¤±è´¥: {str(e)}", "ERROR")
        raise

async def run_multi_platform_crawler_task(task_id: str, request: MultiPlatformCrawlerRequest):
    """åå°è¿è¡Œå¤šå¹³å°çˆ¬è™«ä»»åŠ¡"""
    # ğŸ†• è®¾ç½®ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆ45åˆ†é’Ÿï¼Œå› ä¸ºå¤šå¹³å°éœ€è¦æ›´å¤šæ—¶é—´ï¼‰
    import asyncio
    from concurrent.futures import TimeoutError
    
    try:
        # ğŸ†• ä½¿ç”¨asyncio.wait_foræ·»åŠ è¶…æ—¶æœºåˆ¶
        await asyncio.wait_for(
            _run_multi_platform_crawler_task_internal(task_id, request),
            timeout=2700  # 45åˆ†é’Ÿè¶…æ—¶
        )
    except TimeoutError:
        utils.logger.error(f"[MULTI_TASK_{task_id}] âŒ å¤šå¹³å°ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ï¼ˆ45åˆ†é’Ÿï¼‰")
        multi_platform_task_status[task_id]["status"] = "timeout"
        multi_platform_task_status[task_id]["error"] = "å¤šå¹³å°ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é‡è¯•"
        multi_platform_task_status[task_id]["completed_at"] = datetime.now().isoformat()
        await update_multi_platform_task_progress(task_id, 0.0, "timeout")
        await log_multi_platform_task_step(task_id, "multi", "task_timeout", "å¤šå¹³å°ä»»åŠ¡æ‰§è¡Œè¶…æ—¶", "ERROR", 0)
    except Exception as e:
        utils.logger.error(f"[MULTI_TASK_{task_id}] âŒ å¤šå¹³å°ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        multi_platform_task_status[task_id]["status"] = "failed"
        multi_platform_task_status[task_id]["error"] = str(e)
        multi_platform_task_status[task_id]["completed_at"] = datetime.now().isoformat()
        await update_multi_platform_task_progress(task_id, 0.0, "failed")
        await log_multi_platform_task_step(task_id, "multi", "task_failed", f"å¤šå¹³å°ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR", 0)

async def _run_multi_platform_crawler_task_internal(task_id: str, request: MultiPlatformCrawlerRequest):
    """å†…éƒ¨å¤šå¹³å°çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå‡½æ•°"""
    try:
        utils.logger.info("â–ˆ" * 100)
        utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸš€ å¼€å§‹æ‰§è¡Œå¤šå¹³å°çˆ¬è™«ä»»åŠ¡")
        utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸ“ è¯·æ±‚å‚æ•°è¯¦æƒ…:")
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ platforms: {request.platforms}")
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ keywords: {request.keywords}")
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ max_count_per_platform: {request.max_count_per_platform}")
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ enable_comments: {request.enable_comments}")
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ enable_images: {request.enable_images}")
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ save_format: {request.save_format}")
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ use_proxy: {request.use_proxy}")
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ proxy_ip: {request.proxy_ip}")  # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨proxy_ipè€Œä¸æ˜¯proxy_strategy
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ account_strategy: {getattr(request, 'account_strategy', 'smart')}")
        utils.logger.info(f"[MULTI_TASK_{task_id}]   â””â”€ execution_mode: {getattr(request, 'execution_mode', 'parallel')}")
        
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸ“Š åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
        try:
            from db import init_mediacrawler_db
            await init_mediacrawler_db()
            utils.logger.info(f"[MULTI_TASK_{task_id}] âœ… æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            utils.logger.error(f"[MULTI_TASK_{task_id}] âŒ æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸ“ åˆ›å»ºå¤šå¹³å°ä»»åŠ¡è®°å½•...")
        await create_multi_platform_task_record(task_id, request)
        utils.logger.info(f"[MULTI_TASK_{task_id}] âœ… å¤šå¹³å°ä»»åŠ¡è®°å½•åˆ›å»ºæˆåŠŸ")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        multi_platform_task_status[task_id]["status"] = "running"
        multi_platform_task_status[task_id]["started_at"] = datetime.now().isoformat()
        await update_multi_platform_task_progress(task_id, 0.0, "running")
        await log_multi_platform_task_step(task_id, "multi", "task_start", "å¤šå¹³å°ä»»åŠ¡å¼€å§‹æ‰§è¡Œ")
        
        # è·å–æ‰§è¡Œæ¨¡å¼å’Œè´¦å·ç­–ç•¥
        execution_mode = getattr(request, 'execution_mode', 'parallel')
        account_strategy = getattr(request, 'account_strategy', 'smart')
        
        # ğŸ†• è·å–ä»£ç†ä¿¡æ¯
        proxy_info = None
        if request.use_proxy:
            if hasattr(request, 'proxy_ip') and request.proxy_ip:
                # æ‰‹åŠ¨æŒ‡å®šä»£ç†IP
                try:
                    from proxy.qingguo_long_term_proxy import get_qingguo_proxy_manager
                    proxy_manager = await get_qingguo_proxy_manager()
                    
                    # ä»æ•°æ®åº“è·å–æŒ‡å®šIPçš„ä»£ç†ä¿¡æ¯
                    db = await get_db_connection()
                    if db:
                        query = "SELECT * FROM proxy_pool WHERE ip = %s AND status = 'active' AND enabled = 1"
                        proxy_data = await db.get_first(query, request.proxy_ip)
                        
                        if proxy_data:
                            from proxy.qingguo_long_term_proxy import ProxyInfo, ProxyStatus
                            proxy_info = ProxyInfo(
                                id=str(proxy_data['id']),
                                ip=proxy_data['ip'],
                                port=proxy_data['port'],
                                username=proxy_data.get('username', ''),
                                password=proxy_data.get('password', ''),
                                proxy_type=proxy_data['proxy_type'],
                                expire_ts=proxy_data.get('expire_ts', 0),
                                created_at=proxy_data['created_at'],
                                status=ProxyStatus(proxy_data.get('status', 'active')),
                                enabled=proxy_data.get('enabled', True),
                                area=proxy_data.get('area'),
                                description=proxy_data.get('description')
                            )
                            utils.logger.info(f"[MULTI_TASK_{task_id}] ä½¿ç”¨æŒ‡å®šä»£ç†: {proxy_info.ip}:{proxy_info.port}")
                            # ğŸ†• æ‰“å°ä»£ç†è¯¦ç»†ä¿¡æ¯
                            utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸ“‹ ä»£ç†è¯¦ç»†ä¿¡æ¯:")
                            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ä»£ç†ID: {proxy_info.id}")
                            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ä»£ç†åœ°å€: {proxy_info.ip}:{proxy_info.port}")
                            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ä»£ç†ç±»å‹: {proxy_info.proxy_type}")
                            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ç”¨æˆ·å: {proxy_info.username}")
                            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ åŒºåŸŸ: {proxy_info.area}")
                            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ æè¿°: {proxy_info.description}")
                            utils.logger.info(f"[MULTI_TASK_{task_id}]   â””â”€ è¿‡æœŸæ—¶é—´: {proxy_info.expire_ts}")
                        else:
                            utils.logger.warning(f"[MULTI_TASK_{task_id}] æŒ‡å®šçš„ä»£ç†IP {request.proxy_ip} ä¸å¯ç”¨")
                except Exception as e:
                    utils.logger.warning(f"[MULTI_TASK_{task_id}] è·å–æŒ‡å®šä»£ç†å¤±è´¥: {e}")
            else:
                # è‡ªåŠ¨è·å–ä»£ç†
                try:
                    from proxy.qingguo_long_term_proxy import get_qingguo_proxy_manager
                    proxy_manager = await get_qingguo_proxy_manager()
                    proxy_info = await proxy_manager.get_available_proxy()
                    if proxy_info:
                        utils.logger.info(f"[MULTI_TASK_{task_id}] è‡ªåŠ¨è·å–ä»£ç†: {proxy_info.ip}:{proxy_info.port}")
                        # ğŸ†• æ‰“å°ä»£ç†è¯¦ç»†ä¿¡æ¯
                        utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸ“‹ è‡ªåŠ¨ä»£ç†è¯¦ç»†ä¿¡æ¯:")
                        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ä»£ç†ID: {proxy_info.id}")
                        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ä»£ç†åœ°å€: {proxy_info.ip}:{proxy_info.port}")
                        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ä»£ç†ç±»å‹: {proxy_info.proxy_type}")
                        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ ç”¨æˆ·å: {proxy_info.username}")
                        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ åŒºåŸŸ: {proxy_info.area}")
                        utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ æè¿°: {proxy_info.description}")
                        utils.logger.info(f"[MULTI_TASK_{task_id}]   â””â”€ è¿‡æœŸæ—¶é—´: {proxy_info.expire_ts}")
                except Exception as e:
                    utils.logger.warning(f"[MULTI_TASK_{task_id}] è‡ªåŠ¨è·å–ä»£ç†å¤±è´¥: {e}")
        else:
            utils.logger.info(f"[MULTI_TASK_{task_id}] æœªå¯ç”¨ä»£ç†åŠŸèƒ½")
        
        # æ‰§è¡Œçˆ¬å–ä»»åŠ¡
        platform_results = {}
        platform_errors = {}
        
        if execution_mode == "parallel":
            # å¹¶è¡Œæ‰§è¡Œ
            utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸ”„ å¹¶è¡Œæ‰§è¡Œæ¨¡å¼")
            await log_multi_platform_task_step(task_id, "multi", "execution_mode", "å¹¶è¡Œæ‰§è¡Œæ¨¡å¼")
            
            # åˆ›å»ºæ‰€æœ‰å¹³å°çš„çˆ¬å–ä»»åŠ¡
            tasks = []
            for platform in request.platforms:
                task = run_single_platform_crawler(task_id, platform, request, account_strategy, execution_mode, proxy_info)
                tasks.append((platform, task))
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for i, (platform, _) in enumerate(tasks):
                if isinstance(results[i], Exception):
                    platform_errors[platform] = str(results[i])
                    platform_results[platform] = 0
                else:
                    platform_results[platform] = results[i]
                    
        else:
            # é¡ºåºæ‰§è¡Œ
            utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸ”„ é¡ºåºæ‰§è¡Œæ¨¡å¼")
            await log_multi_platform_task_step(task_id, "multi", "execution_mode", "é¡ºåºæ‰§è¡Œæ¨¡å¼")
            
            for i, platform in enumerate(request.platforms):
                try:
                    progress = (i / len(request.platforms)) * 100
                    await update_multi_platform_task_progress(task_id, progress)
                    await log_multi_platform_task_step(task_id, "multi", "platform_start", f"å¼€å§‹æ‰§è¡Œå¹³å° {platform}")
                    
                    result_count = await run_single_platform_crawler(task_id, platform, request, account_strategy, execution_mode, proxy_info)
                    platform_results[platform] = result_count
                    
                    await log_multi_platform_task_step(task_id, "multi", "platform_completed", f"å¹³å° {platform} æ‰§è¡Œå®Œæˆï¼Œè·å– {result_count} æ¡æ•°æ®")
                    
                except Exception as e:
                    platform_errors[platform] = str(e)
                    platform_results[platform] = 0
                    await log_multi_platform_task_step(task_id, "multi", "platform_failed", f"å¹³å° {platform} æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR")
        
        # æ›´æ–°æœ€ç»ˆçŠ¶æ€
        total_results = sum(platform_results.values())
        success_platforms = len([p for p in request.platforms if p not in platform_errors])
        
        if platform_errors:
            # ğŸ†• ä¿®å¤ï¼šç¼©çŸ­çŠ¶æ€å€¼ä»¥ç¬¦åˆæ•°æ®åº“å­—æ®µé•¿åº¦é™åˆ¶
            status = "completed_with_errors" if len("completed_with_errors") <= 20 else "completed_errors"
            message = f"éƒ¨åˆ†å¹³å°æ‰§è¡Œå¤±è´¥ï¼ŒæˆåŠŸ: {success_platforms}/{len(request.platforms)} ä¸ªå¹³å°"
        else:
            status = "completed"
            message = f"æ‰€æœ‰å¹³å°æ‰§è¡ŒæˆåŠŸï¼Œå…±è·å– {total_results} æ¡æ•°æ®"
        
        multi_platform_task_status[task_id].update({
            "status": status,
            "results": platform_results,
            "errors": platform_errors,
            "completed_at": datetime.now().isoformat()
        })
        
        await update_multi_platform_task_progress(task_id, 100.0, status, platform_results)
        await log_multi_platform_task_step(task_id, "multi", "task_completed", message)
        
        utils.logger.info(f"[MULTI_TASK_{task_id}] âœ… å¤šå¹³å°çˆ¬å–ä»»åŠ¡å®Œæˆ")
        utils.logger.info(f"[MULTI_TASK_{task_id}] ğŸ“Š ç»“æœç»Ÿè®¡:")
        for platform, count in platform_results.items():
            utils.logger.info(f"[MULTI_TASK_{task_id}]   â”œâ”€ {platform}: {count} æ¡")
        if platform_errors:
            utils.logger.info(f"[MULTI_TASK_{task_id}]   â””â”€ é”™è¯¯: {platform_errors}")
        utils.logger.info("â–ˆ" * 100)
        
    except Exception as e:
        utils.logger.error("â–ˆ" * 100)
        utils.logger.error(f"[MULTI_TASK_{task_id}] âŒ å¤šå¹³å°çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
        utils.logger.error(f"[MULTI_TASK_{task_id}] ğŸ› é”™è¯¯è¯¦æƒ…: {e}")
        utils.logger.error(f"[MULTI_TASK_{task_id}] ğŸ“ é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        utils.logger.error(f"[MULTI_TASK_{task_id}] ğŸ“Š é”™è¯¯å †æ ˆ:")
        utils.logger.error(f"[MULTI_TASK_{task_id}] {traceback.format_exc()}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        multi_platform_task_status[task_id]["status"] = "failed"
        multi_platform_task_status[task_id]["error"] = str(e)
        multi_platform_task_status[task_id]["completed_at"] = datetime.now().isoformat()
        await update_multi_platform_task_progress(task_id, 0.0, "failed")
        await log_multi_platform_task_step(task_id, "multi", "task_failed", f"å¤šå¹³å°ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR")
        utils.logger.error("â–ˆ" * 100)

@router.post("/multi-platform/start", response_model=MultiPlatformTaskStatusResponse)
async def start_multi_platform_crawler(request: MultiPlatformCrawlerRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨å¤šå¹³å°çˆ¬è™«ä»»åŠ¡"""
    try:
        utils.logger.info("=" * 100)
        utils.logger.info("[MULTI_CRAWLER_START] æ”¶åˆ°å¤šå¹³å°çˆ¬è™«ä»»åŠ¡å¯åŠ¨è¯·æ±‚")
        utils.logger.info(f"[MULTI_CRAWLER_START] å¹³å°: {request.platforms}")
        utils.logger.info(f"[MULTI_CRAWLER_START] å…³é”®è¯: {request.keywords}")
        utils.logger.info(f"[MULTI_CRAWLER_START] æ¯å¹³å°æœ€å¤§æ•°é‡: {request.max_count_per_platform}")
        
        # å‚æ•°éªŒè¯
        if not request.platforms:
            raise HTTPException(status_code=400, detail="è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªå¹³å°")
        
        if not request.keywords.strip():
            raise HTTPException(status_code=400, detail="è¯·è¾“å…¥æœç´¢å…³é”®è¯")
        
        # æ£€æŸ¥å¹³å°æ”¯æŒæƒ…å†µ
        unsupported_platforms = [p for p in request.platforms if p not in MultiPlatformCrawlerFactory.SUPPORTED_PLATFORMS]
        if unsupported_platforms:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„å¹³å°: {', '.join(unsupported_platforms)}")
        
        # æ£€æŸ¥å„å¹³å°ç™»å½•çŠ¶æ€
        utils.logger.info("[MULTI_CRAWLER_START] æ£€æŸ¥å„å¹³å°ç™»å½•çŠ¶æ€...")
        login_issues = []
        
        for platform in request.platforms:
            try:
                import httpx
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        "http://localhost:8100/api/v1/login/check",
                        json={"platform": platform},
                        timeout=10.0
                    )
                    login_result = response.json()
                    
                    if login_result["code"] != 200:
                        login_issues.append(f"{platform}: {login_result.get('message', 'unknown')}")
                        
            except Exception as e:
                login_issues.append(f"{platform}: æ£€æŸ¥å¤±è´¥ - {str(e)}")
        
        if login_issues:
            utils.logger.warning(f"[MULTI_CRAWLER_START] éƒ¨åˆ†å¹³å°ç™»å½•çŠ¶æ€å¼‚å¸¸: {login_issues}")
            # ç»§ç»­æ‰§è¡Œï¼Œä½†è®°å½•è­¦å‘Š
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        utils.logger.info(f"[MULTI_CRAWLER_START] ç”Ÿæˆä»»åŠ¡ID: {task_id}")
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        multi_platform_task_status[task_id] = {
            "task_id": task_id,
            "status": "pending",
            "platforms": request.platforms,
            "keywords": request.keywords,
            "progress": {
                "total": len(request.platforms),
                "completed": 0,
                "failed": 0,
                "pending": len(request.platforms)
            },
            "results": {},
            "errors": {},
            "created_at": datetime.now().isoformat(),
            "started_at": None,
            "completed_at": None
        }
        utils.logger.info("[MULTI_CRAWLER_START] å¤šå¹³å°ä»»åŠ¡çŠ¶æ€å·²åˆå§‹åŒ–")
        
        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_multi_platform_crawler_task, task_id, request)
        utils.logger.info("[MULTI_CRAWLER_START] åå°ä»»åŠ¡å·²æ·»åŠ ")
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "task_id": task_id,
            "status": "pending",
            "platforms": request.platforms,
            "keywords": request.keywords,
            "progress": {
                "total": len(request.platforms),
                "completed": 0,
                "failed": 0,
                "pending": len(request.platforms)
            },
            "results": {},
            "errors": {},
            "created_at": datetime.now().isoformat(),
            "started_at": None,
            "completed_at": None
        }
        utils.logger.info(f"[MULTI_CRAWLER_START] å“åº”æ•°æ®: {response_data}")
        utils.logger.info("=" * 100)
        
        return MultiPlatformTaskStatusResponse(**response_data)
        
    except Exception as e:
        utils.logger.error(f"[MULTI_CRAWLER_START] å¯åŠ¨å¤šå¹³å°çˆ¬è™«ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨å¤šå¹³å°çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}")

@router.get("/multi-platform/status/{task_id}", response_model=MultiPlatformTaskStatusResponse)
async def get_multi_platform_task_status(task_id: str):
    """è·å–å¤šå¹³å°ä»»åŠ¡çŠ¶æ€"""
    if task_id not in multi_platform_task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return MultiPlatformTaskStatusResponse(**multi_platform_task_status[task_id])

@router.get("/multi-platform/tasks")
async def list_multi_platform_tasks():
    """è·å–æ‰€æœ‰å¤šå¹³å°ä»»åŠ¡åˆ—è¡¨"""
    return {
        "tasks": list(multi_platform_task_status.values()),
        "total": len(multi_platform_task_status)
    }

@router.delete("/multi-platform/tasks/{task_id}")
async def delete_multi_platform_task(task_id: str):
    """åˆ é™¤å¤šå¹³å°ä»»åŠ¡"""
    if task_id not in multi_platform_task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    del multi_platform_task_status[task_id]
    return {"message": "å¤šå¹³å°ä»»åŠ¡å·²åˆ é™¤"}

@router.get("/multi-platform/health")
async def get_multi_platform_health():
    """è·å–å¤šå¹³å°çˆ¬è™«ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        db_status = "unknown"
        try:
            async_db_obj = await get_db_connection()
            if async_db_obj:
                db_status = "healthy"
            else:
                db_status = "unhealthy"
        except Exception as e:
            db_status = f"error: {str(e)}"
        
        # ç»Ÿè®¡ä»»åŠ¡çŠ¶æ€
        total_tasks = len(multi_platform_task_status)
        running_tasks = len([t for t in multi_platform_task_status.values() if t.get('status') == 'running'])
        completed_tasks = len([t for t in multi_platform_task_status.values() if t.get('status') == 'completed'])
        failed_tasks = len([t for t in multi_platform_task_status.values() if t.get('status') == 'failed'])
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "database": db_status,
            "tasks": {
                "total": total_tasks,
                "running": running_tasks,
                "completed": completed_tasks,
                "failed": failed_tasks
            },
            "memory_usage": {
                "task_status_size": len(str(multi_platform_task_status)),
                "estimated_memory_mb": len(str(multi_platform_task_status)) / (1024 * 1024)
            }
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@router.get("/multi-platform/info")
async def get_multi_platform_crawler_info():
    """è·å–å¤šå¹³å°åŠŸèƒ½ä¿¡æ¯"""
    return {
        "supported_platforms": MultiPlatformCrawlerFactory.SUPPORTED_PLATFORMS,
        "account_strategies": [strategy.value for strategy in AccountStrategy],
        "execution_modes": ["parallel", "sequential"],
        "save_formats": ["db"],  # å¤šå¹³å°å›ºå®šä½¿ç”¨æ•°æ®åº“å­˜å‚¨
        "features": {
            "concurrent_crawling": True,
            "unified_data_format": True,
            "account_management": True,
            "progress_tracking": True,
            "error_handling": True
        }
    } 