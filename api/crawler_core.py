"""
çˆ¬è™«æ ¸å¿ƒè·¯ç”±æ¨¡å—
åŒ…å«çˆ¬è™«ä»»åŠ¡å¯åŠ¨ã€çŠ¶æ€æŸ¥è¯¢ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import uuid
import json
from datetime import datetime
from typing import Dict, Any, Optional
from fastapi import APIRouter, BackgroundTasks, HTTPException
from pydantic import BaseModel, Field
from tools import utils
from var import media_crawler_db_var

# æ·»åŠ ç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥å‡½æ•°
async def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    try:
        from config.env_config_loader import config_loader
        from async_db import AsyncMysqlDB
        import aiomysql
        
        db_config = config_loader.get_database_config()
        
        pool = await aiomysql.create_pool(
            host=db_config['host'],
            port=db_config['port'],
            user=db_config['username'],
            password=db_config['password'],
            db=db_config['database'],
            autocommit=True,
            minsize=1,
            maxsize=10,
        )
        
        async_db_obj = AsyncMysqlDB(pool)
        return async_db_obj
        
    except Exception as e:
        utils.logger.error(f"è·å–æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return None

from models.content_models import (
    CrawlerRequest, CrawlerResponse, TaskStatusResponse,
    MultiPlatformCrawlerRequest, MultiPlatformTaskStatusResponse
)

router = APIRouter()

# å…¨å±€ä»»åŠ¡çŠ¶æ€å­˜å‚¨
task_status = {}

class PlatformComingSoonException(Exception):
    """å¹³å°å³å°†æ”¯æŒå¼‚å¸¸"""
    pass

class CrawlerFactory:
    # è§†é¢‘ä¼˜å…ˆå¹³å° - å½“å‰æ”¯æŒ
    VIDEO_PLATFORMS = ["xhs", "dy", "ks", "bili"]
    
    # æ–‡å­—å¹³å° - å³å°†æ”¯æŒ
    COMING_SOON_PLATFORMS = {
        "wb": "å¾®åš",
        "tieba": "è´´å§", 
        "zhihu": "çŸ¥ä¹"
    }

    @staticmethod
    def _get_crawler_class(platform: str):
        """å»¶è¿Ÿå¯¼å…¥çˆ¬è™«ç±» - ä»…æ”¯æŒè§†é¢‘å¹³å°"""
        if platform == "xhs":
            from media_platform.xhs import XiaoHongShuCrawler
            return XiaoHongShuCrawler
        elif platform == "dy":
            from media_platform.douyin import DouYinCrawler
            return DouYinCrawler
        elif platform == "ks":
            from media_platform.kuaishou import KuaishouCrawler
            return KuaishouCrawler
        elif platform == "bili":
            from media_platform.bilibili import BilibiliCrawler
            return BilibiliCrawler
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")

    @staticmethod
    def create_crawler(platform: str, task_id: str = None):
        # æ£€æŸ¥æ˜¯å¦ä¸ºå³å°†æ”¯æŒçš„å¹³å°
        if platform in CrawlerFactory.COMING_SOON_PLATFORMS:
            platform_name = CrawlerFactory.COMING_SOON_PLATFORMS[platform]
            raise PlatformComingSoonException(f"{platform_name}å¹³å°å³å°†æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ï¼å½“å‰ä¸“æ³¨äºçŸ­è§†é¢‘å¹³å°ä¼˜åŒ–ã€‚")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è§†é¢‘å¹³å°
        crawler_class = CrawlerFactory._get_crawler_class(platform)
        return crawler_class(task_id=task_id)

async def create_task_record(task_id: str, request: CrawlerRequest) -> None:
    """åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“"""
    try:
        # è·å–æ•°æ®åº“è¿æ¥
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[TASK_RECORD] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºä»»åŠ¡å‚æ•°JSON
        task_params = {
            "platform": request.platform,
            "keywords": request.keywords,
            "max_count": request.max_notes_count,
            "account_id": request.account_id,
            "session_id": request.session_id,
            "login_type": request.login_type,
            "crawler_type": request.crawler_type,
            "get_comments": request.get_comments,
            "save_data_option": request.save_data_option,
            "use_proxy": request.use_proxy,
            "proxy_strategy": request.proxy_strategy
        }
        
        # ä½¿ç”¨å­—å…¸æ–¹å¼æ„å»ºæ•°æ®
        task_data = {
            'id': task_id,
            'platform': request.platform,
            'task_type': 'single_platform',
            'keywords': request.keywords,
            'status': 'pending',
            'progress': 0.0,
            'result_count': 0,
            'error_message': None,
            'user_id': None,
            'params': json.dumps(task_params),
            'priority': 0,
            'is_favorite': False,
            'deleted': False,
            'is_pinned': False,
            'ip_address': None,
            'user_security_id': None,
            'user_signature': None,
            'created_at': datetime.now(),
            'updated_at': datetime.now(),
            'started_at': None,
            'completed_at': None
        }
        
        # ä½¿ç”¨item_to_tableæ–¹æ³•ï¼Œæ›´å®‰å…¨
        await async_db_obj.item_to_table('crawler_tasks', task_data)
        
        utils.logger.info(f"[TASK_RECORD] ä»»åŠ¡è®°å½•åˆ›å»ºæˆåŠŸ: {task_id}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_RECORD] åˆ›å»ºä»»åŠ¡è®°å½•å¤±è´¥: {e}")
        raise

async def update_task_progress(task_id: str, progress: float, status: str = None, result_count: int = None):
    """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
    try:
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[TASK_PROGRESS] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºæ›´æ–°æ•°æ®å­—å…¸
        update_data = {
            'progress': progress,
            'updated_at': datetime.now()
        }
        
        if status:
            update_data['status'] = status
        
        if result_count is not None:
            update_data['result_count'] = result_count
        
        # ä½¿ç”¨update_tableæ–¹æ³•
        await async_db_obj.update_table('crawler_tasks', update_data, 'id', task_id)
        
        utils.logger.info(f"[TASK_PROGRESS] ä»»åŠ¡è¿›åº¦æ›´æ–°: {task_id}, è¿›åº¦: {progress}, çŠ¶æ€: {status}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_PROGRESS] æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

async def log_task_step(task_id: str, platform: str, step: str, message: str, log_level: str = "INFO", progress: int = None):
    """è®°å½•ä»»åŠ¡æ­¥éª¤æ—¥å¿—"""
    try:
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[TASK_LOG] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºæ—¥å¿—æ•°æ®å­—å…¸
        log_data = {
            'task_id': task_id,
            'platform': platform,
            'account_id': None,
            'log_level': log_level,
            'message': message,
            'step': step,
            'progress': progress or 0,
            'created_at': datetime.now()
        }
        
        # ä½¿ç”¨item_to_tableæ–¹æ³•
        await async_db_obj.item_to_table('crawler_task_logs', log_data)
        
        utils.logger.info(f"[TASK_LOG] {task_id} - {step}: {message}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_LOG] è®°å½•ä»»åŠ¡æ—¥å¿—å¤±è´¥: {e}")

async def run_crawler_task(task_id: str, request: CrawlerRequest):
    """åå°è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    try:
        utils.logger.info("â–ˆ" * 100)
        utils.logger.info(f"[TASK_{task_id}] ğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡")
        utils.logger.info(f"[TASK_{task_id}] ğŸ“ è¯·æ±‚å‚æ•°è¯¦æƒ…:")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ platform: {request.platform}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ keywords: {request.keywords}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ max_count: {request.max_notes_count}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ account_id: {request.account_id}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ session_id: {request.session_id}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ login_type: {request.login_type}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ crawler_type: {request.crawler_type}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ get_comments: {request.get_comments}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ save_data_option: {request.save_data_option}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ use_proxy: {request.use_proxy}")
        utils.logger.info(f"[TASK_{task_id}]   â””â”€ proxy_strategy: {request.proxy_strategy}")
        
        # ğŸ†• åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ï¼ˆç¡®ä¿ä¸Šä¸‹æ–‡å˜é‡å¯ç”¨ï¼‰
        utils.logger.info(f"[TASK_{task_id}] ğŸ“Š åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
        try:
            from db import init_mediacrawler_db
            await init_mediacrawler_db()
            utils.logger.info(f"[TASK_{task_id}] âœ… æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            utils.logger.error(f"[TASK_{task_id}] âŒ æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            # ç»§ç»­æ‰§è¡Œï¼Œå› ä¸ºæœ‰äº›å­˜å‚¨æ–¹å¼å¯èƒ½ä¸éœ€è¦æ•°æ®åº“
        
        # ğŸ†• åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“
        utils.logger.info(f"[TASK_{task_id}] ğŸ“ åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“...")
        await create_task_record(task_id, request)
        utils.logger.info(f"[TASK_{task_id}] âœ… ä»»åŠ¡è®°å½•åˆ›å»ºæˆåŠŸ")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        utils.logger.info(f"[TASK_{task_id}] ğŸ”„ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­...")
        task_status[task_id]["status"] = "running"
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        utils.logger.info(f"[TASK_{task_id}] âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°")
        
        # ğŸ†• æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
        await update_task_progress(task_id, 0.0, "running")
        await log_task_step(task_id, request.platform, "task_start", "ä»»åŠ¡å¼€å§‹æ‰§è¡Œ", "INFO", 0)
        
        # è®¾ç½®çˆ¬è™«é…ç½®
        utils.logger.info(f"[TASK_{task_id}] âš™ï¸ è®¾ç½®çˆ¬è™«é…ç½®...")
        await log_task_step(task_id, request.platform, "config_setup", "è®¾ç½®çˆ¬è™«é…ç½®", "INFO", 35)
        
        import config
        config.PLATFORM = request.platform
        config.ENABLE_GET_COMMENTS = request.get_comments
        config.SAVE_DATA_OPTION = request.save_data_option
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        utils.logger.info(f"[TASK_{task_id}] ğŸ”§ åˆ›å»ºçˆ¬è™«å®ä¾‹...")
        await log_task_step(task_id, request.platform, "crawler_init", "åˆ›å»ºçˆ¬è™«å®ä¾‹", "INFO", 40)
        
        try:
            crawler = CrawlerFactory.create_crawler(request.platform, task_id=task_id)
            utils.logger.info(f"[TASK_{task_id}] âœ… çˆ¬è™«å®ä¾‹åˆ›å»ºæˆåŠŸ")
            await log_task_step(task_id, request.platform, "crawler_ready", "çˆ¬è™«å®ä¾‹å°±ç»ª", "INFO", 45)
        except PlatformComingSoonException as e:
            utils.logger.warning(f"[TASK_{task_id}] âš ï¸ å¹³å°å³å°†æ”¯æŒ: {e}")
            task_status[task_id]["status"] = "failed"
            task_status[task_id]["error"] = str(e)
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            await update_task_progress(task_id, 0.0, "failed")
            await log_task_step(task_id, request.platform, "platform_coming_soon", str(e), "WARN", 0)
            return
        except Exception as e:
            utils.logger.error(f"[TASK_{task_id}] âŒ åˆ›å»ºçˆ¬è™«å®ä¾‹å¤±è´¥: {e}")
            task_status[task_id]["status"] = "failed"
            task_status[task_id]["error"] = f"åˆ›å»ºçˆ¬è™«å®ä¾‹å¤±è´¥: {str(e)}"
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            await update_task_progress(task_id, 0.0, "failed")
            await log_task_step(task_id, request.platform, "crawler_init_failed", f"åˆ›å»ºçˆ¬è™«å®ä¾‹å¤±è´¥: {str(e)}", "ERROR", 0)
            return
        
        # å¼€å§‹çˆ¬å–
        utils.logger.info(f"[TASK_{task_id}] ğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬å–...")
        await log_task_step(task_id, request.platform, "crawling_start", "å¼€å§‹æ‰§è¡Œçˆ¬å–", "INFO", 50)
        
        try:
            # æ ¹æ®çˆ¬è™«ç±»å‹æ‰§è¡Œä¸åŒçš„çˆ¬å–é€»è¾‘
            if request.crawler_type == "search":
                results = await crawler.search_by_keywords(
                    keywords=request.keywords,
                    max_count=request.max_notes_count,
                    account_id=request.account_id,
                    session_id=request.session_id,
                    login_type=request.login_type,
                    get_comments=request.get_comments,
                    save_data_option=request.save_data_option,
                    use_proxy=request.use_proxy,
                    proxy_strategy=request.proxy_strategy
                )
            elif request.crawler_type == "user":
                results = await crawler.get_user_notes(
                    user_id=request.keywords,  # è¿™é‡Œkeywordså®é™…ä¸Šæ˜¯user_id
                    max_count=request.max_notes_count,
                    account_id=request.account_id,
                    session_id=request.session_id,
                    login_type=request.login_type,
                    get_comments=request.get_comments,
                    save_data_option=request.save_data_option,
                    use_proxy=request.use_proxy,
                    proxy_strategy=request.proxy_strategy
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„çˆ¬è™«ç±»å‹: {request.crawler_type}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task_status[task_id]["status"] = "completed"
            task_status[task_id]["result_count"] = len(results) if results else 0
            task_status[task_id]["results"] = results
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            
            await update_task_progress(task_id, 100.0, "completed", len(results) if results else 0)
            await log_task_step(task_id, request.platform, "crawling_completed", f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(results) if results else 0} æ¡æ•°æ®", "INFO", 100)
            
            utils.logger.info(f"[TASK_{task_id}] âœ… çˆ¬å–ä»»åŠ¡å®Œæˆï¼Œå…±è·å– {len(results) if results else 0} æ¡æ•°æ®")
            
        except Exception as e:
            utils.logger.error(f"[TASK_{task_id}] âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            task_status[task_id]["status"] = "failed"
            task_status[task_id]["error"] = f"çˆ¬å–å¤±è´¥: {str(e)}"
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            await update_task_progress(task_id, 0.0, "failed")
            await log_task_step(task_id, request.platform, "crawling_failed", f"çˆ¬å–å¤±è´¥: {str(e)}", "ERROR", 0)
            raise
        
    except Exception as e:
        utils.logger.error("â–ˆ" * 100)
        utils.logger.error(f"[TASK_{task_id}] âŒ çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
        utils.logger.error(f"[TASK_{task_id}] ğŸ› é”™è¯¯è¯¦æƒ…: {e}")
        utils.logger.error(f"[TASK_{task_id}] ğŸ“ é”™è¯¯ç±»å‹: {type(e).__name__}")
        utils.logger.error(f"[TASK_{task_id}] ğŸ“Š é”™è¯¯å †æ ˆ:")
        import traceback
        utils.logger.error(f"[TASK_{task_id}] {traceback.format_exc()}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        utils.logger.error(f"[TASK_{task_id}] ğŸ”„ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥...")
        task_status[task_id]["status"] = "failed"
        task_status[task_id]["error"] = str(e)
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        await update_task_progress(task_id, 0.0, "failed")
        await log_task_step(task_id, request.platform, "task_failed", f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR", 0)
        utils.logger.error(f"[TASK_{task_id}] âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°")
        utils.logger.error("â–ˆ" * 100)

@router.post("/crawler/start", response_model=CrawlerResponse)
async def start_crawler(request: CrawlerRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨å•å¹³å°çˆ¬è™«ä»»åŠ¡"""
    try:
        utils.logger.info("=" * 100)
        utils.logger.info("[CRAWLER_START] æ”¶åˆ°çˆ¬è™«ä»»åŠ¡å¯åŠ¨è¯·æ±‚")
        utils.logger.info(f"[CRAWLER_START] å¹³å°: {request.platform}")
        utils.logger.info(f"[CRAWLER_START] å…³é”®è¯: {request.keywords}")
        utils.logger.info(f"[CRAWLER_START] æœ€å¤§æ•°é‡: {request.max_notes_count}")
        
        # å‚æ•°éªŒè¯
        utils.logger.info("[CRAWLER_START] å‚æ•°éªŒè¯é€šè¿‡")
        
        # ğŸ†• æ£€æŸ¥ç™»å½•çŠ¶æ€ - åœ¨ä»»åŠ¡å¯åŠ¨å‰æ£€æŸ¥
        utils.logger.info("[CRAWLER_START] æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        
        # ç›´æ¥è°ƒç”¨ç™»å½•æ£€æŸ¥API
        import httpx
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8100/api/v1/login/check",
                    json={"platform": request.platform},
                    timeout=10.0
                )
                login_result = response.json()
        except Exception as e:
            utils.logger.error(f"[CRAWLER_START] ç™»å½•æ£€æŸ¥APIè°ƒç”¨å¤±è´¥: {e}")
            login_result = {"code": 500, "message": f"ç™»å½•æ£€æŸ¥å¤±è´¥: {str(e)}"}
        
        if login_result["code"] != 200:
            utils.logger.warning(f"[CRAWLER_START] å¹³å° {request.platform} æœªç™»å½•ï¼ŒçŠ¶æ€: {login_result.get('message', 'unknown')}")
            
            # è¿”å›éœ€è¦ç™»å½•çš„é”™è¯¯ä¿¡æ¯
            error_message = f"å¹³å° {request.platform} éœ€è¦ç™»å½•ï¼Œè¯·å…ˆè¿›è¡Œè¿œç¨‹ç™»å½•"
            
            return CrawlerResponse(
                task_id="",
                status="need_login",
                message=error_message,
                data={
                    "platform": request.platform,
                    "login_status": "not_logged_in",
                    "redirect_url": "/static/account_management.html"
                }
            )
        
        utils.logger.info(f"[CRAWLER_START] å¹³å° {request.platform} ç™»å½•çŠ¶æ€æ­£å¸¸")
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        utils.logger.info(f"[CRAWLER_START] ç”Ÿæˆä»»åŠ¡ID: {task_id}")
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        task_status[task_id] = {
            "task_id": task_id,
            "status": "pending",
            "platform": request.platform,
            "keywords": request.keywords,
            "max_count": request.max_notes_count,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "progress": 0.0,
            "result_count": 0,
            "results": None,
            "error": None
        }
        utils.logger.info("[CRAWLER_START] ä»»åŠ¡çŠ¶æ€å·²åˆå§‹åŒ–")
        
        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawler_task, task_id, request)
        utils.logger.info("[CRAWLER_START] åå°ä»»åŠ¡å·²æ·»åŠ ")
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "task_id": task_id,
            "status": "pending",
            "message": "çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨æ‰§è¡Œ...",
            "data": None
        }
        utils.logger.info(f"[CRAWLER_START] å“åº”æ•°æ®: {response_data}")
        utils.logger.info("=" * 100)
        
        return CrawlerResponse(**response_data)
        
    except Exception as e:
        utils.logger.error(f"[CRAWLER_START] å¯åŠ¨çˆ¬è™«ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}")

@router.get("/crawler/status/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return TaskStatusResponse(**task_status[task_id])

@router.get("/crawler/tasks")
async def list_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
    return {
        "tasks": list(task_status.values()),
        "total": len(task_status)
    }

@router.delete("/crawler/tasks/{task_id}")
async def delete_task(task_id: str):
    """åˆ é™¤ä»»åŠ¡"""
    if task_id not in task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    del task_status[task_id]
    return {"message": "ä»»åŠ¡å·²åˆ é™¤"} 