"""
çˆ¬è™«æ ¸å¿ƒè·¯ç”±æ¨¡å—
åŒ…å«çˆ¬è™«ä»»åŠ¡å¯åŠ¨ã€çŠ¶æ€æŸ¥è¯¢ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import uuid
import json
from datetime import datetime
from typing import Dict, Any, Optional, List
from fastapi import APIRouter, BackgroundTasks, HTTPException
from pydantic import BaseModel, Field
from tools import utils
from var import media_crawler_db_var

# ğŸ†• å…¨å±€æ•°æ®åº“è¿æ¥æ± ç®¡ç†
_db_pool = None
_db_async_obj = None

async def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥ - ä½¿ç”¨å…¨å±€è¿æ¥æ± """
    global _db_pool, _db_async_obj
    
    try:
        # å¦‚æœè¿æ¥æ± å·²å­˜åœ¨ä¸”æœ‰æ•ˆï¼Œç›´æ¥è¿”å›
        if _db_pool and not _db_pool.closed:
            return _db_async_obj
        
        # åˆ›å»ºæ–°çš„è¿æ¥æ± 
        from config.env_config_loader import config_loader
        from async_db import AsyncMysqlDB
        import aiomysql
        
        db_config = config_loader.get_database_config()
        
        _db_pool = await aiomysql.create_pool(
            host=db_config['host'],
            port=db_config['port'],
            user=db_config['username'],
            password=db_config['password'],
            db=db_config['database'],
            autocommit=True,
            minsize=1,
            maxsize=20,  # ğŸ†• å¢åŠ è¿æ¥æ± å¤§å°
            echo=False,   # ğŸ†• å…³é—­SQLæ—¥å¿—ï¼Œå‡å°‘æ€§èƒ½å¼€é”€
        )
        
        _db_async_obj = AsyncMysqlDB(_db_pool)
        utils.logger.info("æ•°æ®åº“è¿æ¥æ± åˆ›å»ºæˆåŠŸ")
        return _db_async_obj
        
    except Exception as e:
        utils.logger.error(f"è·å–æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return None

async def close_db_connection():
    """å…³é—­æ•°æ®åº“è¿æ¥æ± """
    global _db_pool, _db_async_obj
    
    try:
        if _db_pool and not _db_pool.closed:
            _db_pool.close()
            await _db_pool.wait_closed()
            _db_pool = None
            _db_async_obj = None
            utils.logger.info("æ•°æ®åº“è¿æ¥æ± å·²å…³é—­")
    except Exception as e:
        utils.logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥æ± å¤±è´¥: {e}")

from models.content_models import (
    CrawlerRequest, CrawlerResponse, TaskStatusResponse,
    MultiPlatformCrawlerRequest, MultiPlatformTaskStatusResponse
)

router = APIRouter()

# å…¨å±€ä»»åŠ¡çŠ¶æ€å­˜å‚¨
task_status = {}

# ğŸ†• ä»»åŠ¡æ¸…ç†é…ç½®
TASK_CLEANUP_INTERVAL = 3600  # 1å°æ—¶æ¸…ç†ä¸€æ¬¡
TASK_MAX_AGE = 86400  # 24å°æ—¶åæ¸…ç†ä»»åŠ¡çŠ¶æ€

async def cleanup_old_tasks():
    """æ¸…ç†è¿‡æœŸçš„ä»»åŠ¡çŠ¶æ€"""
    try:
        from datetime import datetime, timedelta
        current_time = datetime.now()
        
        tasks_to_remove = []
        for task_id, task_info in task_status.items():
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¶…è¿‡24å°æ—¶
            if 'created_at' in task_info:
                created_time = datetime.fromisoformat(task_info['created_at'])
                if current_time - created_time > timedelta(seconds=TASK_MAX_AGE):
                    tasks_to_remove.append(task_id)
        
        # ç§»é™¤è¿‡æœŸä»»åŠ¡
        for task_id in tasks_to_remove:
            del task_status[task_id]
            utils.logger.info(f"æ¸…ç†è¿‡æœŸä»»åŠ¡çŠ¶æ€: {task_id}")
        
        if tasks_to_remove:
            utils.logger.info(f"æ¸…ç†äº† {len(tasks_to_remove)} ä¸ªè¿‡æœŸä»»åŠ¡çŠ¶æ€")
            
    except Exception as e:
        utils.logger.error(f"æ¸…ç†è¿‡æœŸä»»åŠ¡å¤±è´¥: {e}")

# ğŸ†• å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
import asyncio
async def start_task_cleanup():
    """å¯åŠ¨å®šæœŸä»»åŠ¡æ¸…ç†"""
    while True:
        try:
            await asyncio.sleep(TASK_CLEANUP_INTERVAL)
            await cleanup_old_tasks()
        except Exception as e:
            utils.logger.error(f"ä»»åŠ¡æ¸…ç†å¾ªç¯å¤±è´¥: {e}")
            await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†é‡è¯•

class PlatformComingSoonException(Exception):
    """å¹³å°å³å°†æ”¯æŒå¼‚å¸¸"""
    pass

class CrawlerFactory:
    # è§†é¢‘ä¼˜å…ˆå¹³å° - å½“å‰æ”¯æŒ
    VIDEO_PLATFORMS = ["xhs", "dy", "ks", "bili"]
    
    # æ–‡å­—å¹³å° - å³å°†æ”¯æŒ
    COMING_SOON_PLATFORMS = {
        "wb": "å¾®åš",
        "tieba": "è´´å§", 
        "zhihu": "çŸ¥ä¹"
    }

    @staticmethod
    def _get_crawler_class(platform: str):
        """å»¶è¿Ÿå¯¼å…¥çˆ¬è™«ç±» - ä»…æ”¯æŒè§†é¢‘å¹³å°"""
        if platform == "xhs":
            from media_platform.xhs import XiaoHongShuCrawler
            return XiaoHongShuCrawler
        elif platform == "dy":
            from media_platform.douyin import DouYinCrawler
            return DouYinCrawler
        elif platform == "ks":
            from media_platform.kuaishou import KuaishouCrawler
            return KuaishouCrawler
        elif platform == "bili":
            from media_platform.bilibili import BilibiliCrawler
            return BilibiliCrawler
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")

    @staticmethod
    def create_crawler(platform: str, task_id: str = None):
        # æ£€æŸ¥æ˜¯å¦ä¸ºå³å°†æ”¯æŒçš„å¹³å°
        if platform in CrawlerFactory.COMING_SOON_PLATFORMS:
            platform_name = CrawlerFactory.COMING_SOON_PLATFORMS[platform]
            raise PlatformComingSoonException(f"{platform_name}å¹³å°å³å°†æ”¯æŒï¼Œæ•¬è¯·æœŸå¾…ï¼å½“å‰ä¸“æ³¨äºçŸ­è§†é¢‘å¹³å°ä¼˜åŒ–ã€‚")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„è§†é¢‘å¹³å°
        crawler_class = CrawlerFactory._get_crawler_class(platform)
        return crawler_class(task_id=task_id)

async def create_task_record(task_id: str, request: CrawlerRequest) -> None:
    """åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“"""
    try:
        # è·å–æ•°æ®åº“è¿æ¥
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[TASK_RECORD] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºä»»åŠ¡å‚æ•°JSON
        task_params = {
            "platform": request.platform,
            "keywords": request.keywords,
            "max_count": request.max_notes_count,
            "account_id": request.account_id,
            "session_id": request.session_id,
            "login_type": request.login_type,
            "crawler_type": request.crawler_type,
            "get_comments": request.get_comments,
            "save_data_option": request.save_data_option,
            "use_proxy": request.use_proxy,
            "proxy_strategy": request.proxy_strategy
        }
        
        # å¤„ç†åˆ›ä½œè€…IDåˆ—è¡¨
        creator_ref_ids = None
        if request.crawler_type == "creator":
            if hasattr(request, 'selected_creators') and request.selected_creators:
                creator_ref_ids = request.selected_creators
            elif hasattr(request, 'creator_ref_ids') and request.creator_ref_ids:
                creator_ref_ids = request.creator_ref_ids
            elif hasattr(request, 'creator_ref_id') and request.creator_ref_id:
                creator_ref_ids = [request.creator_ref_id]
        
        # ä½¿ç”¨å­—å…¸æ–¹å¼æ„å»ºæ•°æ®
        task_data = {
            'id': task_id,
            'platform': request.platform,
            'task_type': 'single_platform',
            'crawler_type': request.crawler_type,  # æ·»åŠ çˆ¬å–ç±»å‹
            'creator_ref_ids': json.dumps(creator_ref_ids) if creator_ref_ids else None,  # æ·»åŠ åˆ›ä½œè€…å¼•ç”¨IDåˆ—è¡¨
            'keywords': request.keywords,
            'status': 'pending',
            'progress': 0.0,
            'result_count': 0,
            'error_message': None,
            'user_id': None,
            'params': json.dumps(task_params),
            'priority': 0,
            'is_favorite': False,
            'deleted': False,
            'is_pinned': False,
            'ip_address': None,
            'user_security_id': None,
            'user_signature': None,
            'created_at': datetime.now(),
            'updated_at': datetime.now(),
            'started_at': None,
            'completed_at': None
        }
        
        # ä½¿ç”¨item_to_tableæ–¹æ³•ï¼Œæ›´å®‰å…¨
        await async_db_obj.item_to_table('crawler_tasks', task_data)
        
        utils.logger.info(f"[TASK_RECORD] ä»»åŠ¡è®°å½•åˆ›å»ºæˆåŠŸ: {task_id}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_RECORD] åˆ›å»ºä»»åŠ¡è®°å½•å¤±è´¥: {e}")
        raise

async def update_task_progress(task_id: str, progress: float, status: str = None, result_count: int = None):
    """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
    try:
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[TASK_PROGRESS] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºæ›´æ–°æ•°æ®å­—å…¸
        update_data = {
            'progress': progress,
            'updated_at': datetime.now()
        }
        
        if status:
            update_data['status'] = status
        
        if result_count is not None:
            update_data['result_count'] = result_count
        
        # ä½¿ç”¨update_tableæ–¹æ³•
        await async_db_obj.update_table('crawler_tasks', update_data, 'id', task_id)
        
        utils.logger.info(f"[TASK_PROGRESS] ä»»åŠ¡è¿›åº¦æ›´æ–°: {task_id}, è¿›åº¦: {progress}, çŠ¶æ€: {status}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_PROGRESS] æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

async def update_task_creator_ref_ids(task_id: str, creator_ref_ids: List[str]):
    """æ›´æ–°ä»»åŠ¡çš„creator_ref_idså­—æ®µ"""
    try:
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[TASK_CREATOR_REF] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºæ›´æ–°æ•°æ®å­—å…¸
        update_data = {
            'creator_ref_ids': json.dumps(creator_ref_ids),
            'updated_at': datetime.now()
        }
        
        # ä½¿ç”¨update_tableæ–¹æ³•
        await async_db_obj.update_table('crawler_tasks', update_data, 'id', task_id)
        
        utils.logger.info(f"[TASK_CREATOR_REF] ä»»åŠ¡creator_ref_idsæ›´æ–°: {task_id}, creator_ref_ids: {creator_ref_ids}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_CREATOR_REF] æ›´æ–°ä»»åŠ¡creator_ref_idså¤±è´¥: {e}")

async def log_task_step(task_id: str, platform: str, step: str, message: str, log_level: str = "INFO", progress: int = None):
    """è®°å½•ä»»åŠ¡æ­¥éª¤æ—¥å¿—"""
    try:
        async_db_obj = await get_db_connection()
        if not async_db_obj:
            utils.logger.error("[TASK_LOG] æ— æ³•è·å–æ•°æ®åº“è¿æ¥")
            return
        
        # æ„å»ºæ—¥å¿—æ•°æ®å­—å…¸
        log_data = {
            'task_id': task_id,
            'platform': platform,
            'account_id': None,
            'log_level': log_level,
            'message': message,
            'step': step,
            'progress': progress or 0,
            'created_at': datetime.now()
        }
        
        # ä½¿ç”¨item_to_tableæ–¹æ³•
        await async_db_obj.item_to_table('crawler_task_logs', log_data)
        
        utils.logger.info(f"[TASK_LOG] {task_id} - {step}: {message}")
        
    except Exception as e:
        utils.logger.error(f"[TASK_LOG] è®°å½•ä»»åŠ¡æ—¥å¿—å¤±è´¥: {e}")

async def run_crawler_task(task_id: str, request: CrawlerRequest):
    """åå°è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    # ğŸ†• è®¾ç½®ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆ30åˆ†é’Ÿï¼‰
    import asyncio
    from concurrent.futures import TimeoutError
    
    try:
        # ğŸ†• ä½¿ç”¨asyncio.wait_foræ·»åŠ è¶…æ—¶æœºåˆ¶
        await asyncio.wait_for(
            _run_crawler_task_internal(task_id, request),
            timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
        )
    except TimeoutError:
        utils.logger.error(f"[TASK_{task_id}] âŒ ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ï¼ˆ30åˆ†é’Ÿï¼‰")
        task_status[task_id]["status"] = "timeout"
        task_status[task_id]["error"] = "ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–é‡è¯•"
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        await update_task_progress(task_id, 0.0, "timeout")
        await log_task_step(task_id, request.platform, "task_timeout", "ä»»åŠ¡æ‰§è¡Œè¶…æ—¶", "ERROR", 0)
    except Exception as e:
        utils.logger.error(f"[TASK_{task_id}] âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        task_status[task_id]["status"] = "failed"
        task_status[task_id]["error"] = str(e)
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        await update_task_progress(task_id, 0.0, "failed")
        await log_task_step(task_id, request.platform, "task_failed", f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR", 0)

async def _run_crawler_task_internal(task_id: str, request: CrawlerRequest):
    """å†…éƒ¨çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå‡½æ•°"""
    try:
        utils.logger.info("â–ˆ" * 100)
        utils.logger.info(f"[TASK_{task_id}] ğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡")
        utils.logger.info(f"[TASK_{task_id}] ğŸ“ è¯·æ±‚å‚æ•°è¯¦æƒ…:")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ platform: {request.platform}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ keywords: {request.keywords}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ max_count: {request.max_notes_count}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ account_id: {request.account_id}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ session_id: {request.session_id}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ login_type: {request.login_type}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ crawler_type: {request.crawler_type}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ get_comments: {request.get_comments}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ save_data_option: {request.save_data_option}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ use_proxy: {request.use_proxy}")
        utils.logger.info(f"[TASK_{task_id}]   â”œâ”€ proxy_strategy: {request.proxy_strategy}")
        utils.logger.info(f"[TASK_{task_id}]   â””â”€ selected_creators: {getattr(request, 'selected_creators', None)}")
        
        # ğŸ†• åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ï¼ˆç¡®ä¿ä¸Šä¸‹æ–‡å˜é‡å¯ç”¨ï¼‰
        utils.logger.info(f"[TASK_{task_id}] ğŸ“Š åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
        try:
            from db import init_mediacrawler_db
            await init_mediacrawler_db()
            utils.logger.info(f"[TASK_{task_id}] âœ… æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            utils.logger.error(f"[TASK_{task_id}] âŒ æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            # ç»§ç»­æ‰§è¡Œï¼Œå› ä¸ºæœ‰äº›å­˜å‚¨æ–¹å¼å¯èƒ½ä¸éœ€è¦æ•°æ®åº“
        
        # ğŸ†• åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“
        utils.logger.info(f"[TASK_{task_id}] ğŸ“ åˆ›å»ºä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“...")
        await create_task_record(task_id, request)
        utils.logger.info(f"[TASK_{task_id}] âœ… ä»»åŠ¡è®°å½•åˆ›å»ºæˆåŠŸ")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        utils.logger.info(f"[TASK_{task_id}] ğŸ”„ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­...")
        task_status[task_id]["status"] = "running"
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        utils.logger.info(f"[TASK_{task_id}] âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°")
        
        # ğŸ†• æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€
        await update_task_progress(task_id, 0.0, "running")
        await log_task_step(task_id, request.platform, "task_start", "ä»»åŠ¡å¼€å§‹æ‰§è¡Œ", "INFO", 0)
        
        # è®¾ç½®çˆ¬è™«é…ç½®
        utils.logger.info(f"[TASK_{task_id}] âš™ï¸ è®¾ç½®çˆ¬è™«é…ç½®...")
        await log_task_step(task_id, request.platform, "config_setup", "è®¾ç½®çˆ¬è™«é…ç½®", "INFO", 35)
        
        import config
        config.PLATFORM = request.platform
        config.ENABLE_GET_COMMENTS = request.get_comments
        config.SAVE_DATA_OPTION = request.save_data_option
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        utils.logger.info(f"[TASK_{task_id}] ğŸ”§ åˆ›å»ºçˆ¬è™«å®ä¾‹...")
        await log_task_step(task_id, request.platform, "crawler_init", "åˆ›å»ºçˆ¬è™«å®ä¾‹", "INFO", 40)
        
        try:
            crawler = CrawlerFactory.create_crawler(request.platform, task_id=task_id)
            utils.logger.info(f"[TASK_{task_id}] âœ… çˆ¬è™«å®ä¾‹åˆ›å»ºæˆåŠŸ")
            await log_task_step(task_id, request.platform, "crawler_ready", "çˆ¬è™«å®ä¾‹å°±ç»ª", "INFO", 45)
        except PlatformComingSoonException as e:
            utils.logger.warning(f"[TASK_{task_id}] âš ï¸ å¹³å°å³å°†æ”¯æŒ: {e}")
            task_status[task_id]["status"] = "failed"
            task_status[task_id]["error"] = str(e)
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            await update_task_progress(task_id, 0.0, "failed")
            await log_task_step(task_id, request.platform, "platform_coming_soon", str(e), "WARN", 0)
            return
        except Exception as e:
            utils.logger.error(f"[TASK_{task_id}] âŒ åˆ›å»ºçˆ¬è™«å®ä¾‹å¤±è´¥: {e}")
            task_status[task_id]["status"] = "failed"
            task_status[task_id]["error"] = f"åˆ›å»ºçˆ¬è™«å®ä¾‹å¤±è´¥: {str(e)}"
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            await update_task_progress(task_id, 0.0, "failed")
            await log_task_step(task_id, request.platform, "crawler_init_failed", f"åˆ›å»ºçˆ¬è™«å®ä¾‹å¤±è´¥: {str(e)}", "ERROR", 0)
            return
        
        # å¼€å§‹çˆ¬å–
        utils.logger.info(f"[TASK_{task_id}] ğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬å–...")
        await log_task_step(task_id, request.platform, "crawling_start", "å¼€å§‹æ‰§è¡Œçˆ¬å–", "INFO", 50)
        
        try:
            # æ ¹æ®çˆ¬è™«ç±»å‹æ‰§è¡Œä¸åŒçš„çˆ¬å–é€»è¾‘
            if request.crawler_type == "search":
                results = await crawler.search_by_keywords(
                    keywords=request.keywords,
                    max_count=request.max_notes_count,
                    account_id=request.account_id,
                    session_id=request.session_id,
                    login_type=request.login_type,
                    get_comments=request.get_comments,
                    save_data_option=request.save_data_option,
                    use_proxy=request.use_proxy,
                    proxy_strategy=request.proxy_strategy
                )
            elif request.crawler_type == "creator":
                # ä»æ•°æ®åº“è·å–åˆ›ä½œè€…åˆ—è¡¨
                db = await get_db_connection()
                if not db:
                    raise Exception("æ•°æ®åº“è¿æ¥å¤±è´¥")
                
                # è·å–æŒ‡å®šå¹³å°çš„åˆ›ä½œè€…åˆ—è¡¨
                utils.logger.info(f"[TASK_{task_id}] æ£€æŸ¥ç”¨æˆ·é€‰æ‹©çš„åˆ›ä½œè€…...")
                utils.logger.info(f"[TASK_{task_id}] selected_creators å±æ€§å­˜åœ¨: {hasattr(request, 'selected_creators')}")
                utils.logger.info(f"[TASK_{task_id}] selected_creators å€¼: {getattr(request, 'selected_creators', None)}")
                
                if hasattr(request, 'selected_creators') and request.selected_creators:
                    # ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„åˆ›ä½œè€…
                    utils.logger.info(f"[TASK_{task_id}] ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„åˆ›ä½œè€…ï¼Œæ•°é‡: {len(request.selected_creators)}")
                    creators_query = """
                        SELECT creator_id, platform, name, nickname 
                        FROM unified_creator 
                        WHERE platform = %s AND creator_id IN ({})
                        ORDER BY last_modify_ts DESC
                    """.format(','.join(['%s'] * len(request.selected_creators)))
                    creators = await db.query(creators_query, request.platform, *request.selected_creators)
                    utils.logger.info(f"[TASK_{task_id}] ç”¨æˆ·é€‰æ‹©äº† {len(creators)} ä¸ªåˆ›ä½œè€…")
                    utils.logger.info(f"[TASK_{task_id}] åˆ›ä½œè€…åˆ—è¡¨: {[c.get('name', c.get('nickname', 'æœªçŸ¥')) for c in creators]}")
                else:
                    # è·å–æ‰€æœ‰åˆ›ä½œè€…ï¼ˆæŒ‰æœ€å¤§æ•°é‡é™åˆ¶ï¼‰
                    utils.logger.info(f"[TASK_{task_id}] æœªé€‰æ‹©ç‰¹å®šåˆ›ä½œè€…ï¼Œè·å–æ‰€æœ‰åˆ›ä½œè€…")
                    creators_query = """
                        SELECT creator_id, platform, name, nickname 
                        FROM unified_creator 
                        WHERE platform = %s AND is_deleted = 0
                        ORDER BY last_modify_ts DESC
                        LIMIT %s
                    """
                    creators = await db.query(creators_query, request.platform, request.max_notes_count)
                    utils.logger.info(f"[TASK_{task_id}] æ‰¾åˆ° {len(creators)} ä¸ªåˆ›ä½œè€…ï¼ˆè‡ªåŠ¨é€‰æ‹©ï¼‰")
                    utils.logger.info(f"[TASK_{task_id}] åˆ›ä½œè€…åˆ—è¡¨: {[c.get('name', c.get('nickname', 'æœªçŸ¥')) for c in creators]}")
                
                if not creators:
                    raise Exception(f"å¹³å° {request.platform} æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„åˆ›ä½œè€…")
                
                # å…ˆåˆå§‹åŒ–çˆ¬è™«ï¼ˆåˆ›å»ºå®¢æˆ·ç«¯ç­‰ï¼‰
                await crawler.start()
                
                # ğŸ†• æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œç¡®ä¿å…³é”®å­—æ­£ç¡®ä¼ é€’
                utils.logger.debug(f"[TASK_{task_id}] ä¼ é€’ç»™åˆ›ä½œè€…çˆ¬å–æ–¹æ³•çš„å…³é”®å­—: '{request.keywords}'")
                utils.logger.debug(f"[TASK_{task_id}] å…³é”®å­—ç±»å‹: {type(request.keywords)}")
                utils.logger.debug(f"[TASK_{task_id}] å…³é”®å­—æ˜¯å¦ä¸ºç©º: {not request.keywords or not request.keywords.strip()}")
                
                # è°ƒç”¨åˆ›ä½œè€…çˆ¬å–æ–¹æ³•
                results = await crawler.get_creators_and_notes_from_db(
                    creators=creators,
                    max_count=request.max_notes_count,
                    keywords=request.keywords,  # æ·»åŠ å…³é”®è¯å‚æ•°
                    account_id=request.account_id,
                    session_id=request.session_id,
                    login_type=request.login_type,
                    get_comments=request.get_comments,
                    save_data_option=request.save_data_option,
                    use_proxy=request.use_proxy,
                    proxy_strategy=request.proxy_strategy
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„çˆ¬è™«ç±»å‹: {request.crawler_type}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            task_status[task_id]["status"] = "completed"
            task_status[task_id]["result_count"] = len(results) if results else 0
            task_status[task_id]["results"] = results
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            
            await update_task_progress(task_id, 100.0, "completed", len(results) if results else 0)
            await log_task_step(task_id, request.platform, "crawling_completed", f"çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(results) if results else 0} æ¡æ•°æ®", "INFO", 100)
            
            utils.logger.info(f"[TASK_{task_id}] âœ… çˆ¬å–ä»»åŠ¡å®Œæˆï¼Œå…±è·å– {len(results) if results else 0} æ¡æ•°æ®")
            
        except Exception as e:
            utils.logger.error(f"[TASK_{task_id}] âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            task_status[task_id]["status"] = "failed"
            task_status[task_id]["error"] = f"çˆ¬å–å¤±è´¥: {str(e)}"
            task_status[task_id]["updated_at"] = datetime.now().isoformat()
            await update_task_progress(task_id, 0.0, "failed")
            await log_task_step(task_id, request.platform, "crawling_failed", f"çˆ¬å–å¤±è´¥: {str(e)}", "ERROR", 0)
            raise
        finally:
            # ğŸ†• å®‰å…¨å…³é—­çˆ¬è™«èµ„æº
            try:
                if hasattr(crawler, 'close'):
                    await crawler.close()
                    utils.logger.info(f"[TASK_{task_id}] çˆ¬è™«èµ„æºå·²å…³é—­")
            except Exception as e:
                utils.logger.warning(f"[TASK_{task_id}] å…³é—­çˆ¬è™«èµ„æºæ—¶å‡ºç°è­¦å‘Š: {e}")
            
            # ğŸ†• ç¡®ä¿æµè§ˆå™¨å®ä¾‹è¢«æ­£ç¡®å…³é—­
            try:
                if hasattr(crawler, 'browser') and crawler.browser:
                    await crawler.browser.close()
                    utils.logger.info(f"[TASK_{task_id}] æµè§ˆå™¨å®ä¾‹å·²å…³é—­")
            except Exception as e:
                utils.logger.warning(f"[TASK_{task_id}] å…³é—­æµè§ˆå™¨å®ä¾‹æ—¶å‡ºç°è­¦å‘Š: {e}")
            
            # ğŸ†• æ¸…ç†Playwrightä¸Šä¸‹æ–‡
            try:
                if hasattr(crawler, 'context') and crawler.context:
                    await crawler.context.close()
                    utils.logger.info(f"[TASK_{task_id}] Playwrightä¸Šä¸‹æ–‡å·²å…³é—­")
            except Exception as e:
                utils.logger.warning(f"[TASK_{task_id}] å…³é—­Playwrightä¸Šä¸‹æ–‡æ—¶å‡ºç°è­¦å‘Š: {e}")
        
    except Exception as e:
        utils.logger.error("â–ˆ" * 100)
        utils.logger.error(f"[TASK_{task_id}] âŒ çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
        utils.logger.error(f"[TASK_{task_id}] ğŸ› é”™è¯¯è¯¦æƒ…: {e}")
        utils.logger.error(f"[TASK_{task_id}] ğŸ“ é”™è¯¯ç±»å‹: {type(e).__name__}")
        utils.logger.error(f"[TASK_{task_id}] ğŸ“Š é”™è¯¯å †æ ˆ:")
        import traceback
        utils.logger.error(f"[TASK_{task_id}] {traceback.format_exc()}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        utils.logger.error(f"[TASK_{task_id}] ğŸ”„ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥...")
        task_status[task_id]["status"] = "failed"
        task_status[task_id]["error"] = str(e)
        task_status[task_id]["updated_at"] = datetime.now().isoformat()
        await update_task_progress(task_id, 0.0, "failed")
        await log_task_step(task_id, request.platform, "task_failed", f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR", 0)
        utils.logger.error(f"[TASK_{task_id}] âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°")
        utils.logger.error("â–ˆ" * 100)

@router.post("/crawler/start", response_model=CrawlerResponse)
async def start_crawler(request: CrawlerRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨å•å¹³å°çˆ¬è™«ä»»åŠ¡"""
    try:
        utils.logger.info("=" * 100)
        utils.logger.info("[CRAWLER_START] æ”¶åˆ°çˆ¬è™«ä»»åŠ¡å¯åŠ¨è¯·æ±‚")
        utils.logger.info(f"[CRAWLER_START] å¹³å°: {request.platform}")
        utils.logger.info(f"[CRAWLER_START] å…³é”®è¯: {request.keywords}")
        utils.logger.info(f"[CRAWLER_START] æœ€å¤§æ•°é‡: {request.max_notes_count}")
        
        # å‚æ•°éªŒè¯
        utils.logger.info("[CRAWLER_START] å‚æ•°éªŒè¯é€šè¿‡")
        
        # ğŸ†• æ£€æŸ¥ç™»å½•çŠ¶æ€ - åœ¨ä»»åŠ¡å¯åŠ¨å‰æ£€æŸ¥
        utils.logger.info("[CRAWLER_START] æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        
        # ç›´æ¥è°ƒç”¨ç™»å½•æ£€æŸ¥API
        import httpx
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    "http://localhost:8100/api/v1/login/check",
                    json={"platform": request.platform},
                    timeout=10.0
                )
                login_result = response.json()
        except Exception as e:
            utils.logger.error(f"[CRAWLER_START] ç™»å½•æ£€æŸ¥APIè°ƒç”¨å¤±è´¥: {e}")
            login_result = {"code": 500, "message": f"ç™»å½•æ£€æŸ¥å¤±è´¥: {str(e)}"}
        
        if login_result["code"] != 200:
            utils.logger.warning(f"[CRAWLER_START] å¹³å° {request.platform} æœªç™»å½•ï¼ŒçŠ¶æ€: {login_result.get('message', 'unknown')}")
            
            # è¿”å›éœ€è¦ç™»å½•çš„é”™è¯¯ä¿¡æ¯
            error_message = f"å¹³å° {request.platform} éœ€è¦ç™»å½•ï¼Œè¯·å…ˆè¿›è¡Œè¿œç¨‹ç™»å½•"
            
            return CrawlerResponse(
                task_id="",
                status="need_login",
                message=error_message,
                data={
                    "platform": request.platform,
                    "login_status": "not_logged_in",
                    "redirect_url": "/static/account_management.html"
                }
            )
        
        utils.logger.info(f"[CRAWLER_START] å¹³å° {request.platform} ç™»å½•çŠ¶æ€æ­£å¸¸")
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        utils.logger.info(f"[CRAWLER_START] ç”Ÿæˆä»»åŠ¡ID: {task_id}")
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        task_status[task_id] = {
            "task_id": task_id,
            "status": "pending",
            "platform": request.platform,
            "keywords": request.keywords,
            "max_count": request.max_notes_count,
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "progress": 0.0,
            "result_count": 0,
            "results": None,
            "error": None
        }
        utils.logger.info("[CRAWLER_START] ä»»åŠ¡çŠ¶æ€å·²åˆå§‹åŒ–")
        
        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(run_crawler_task, task_id, request)
        utils.logger.info("[CRAWLER_START] åå°ä»»åŠ¡å·²æ·»åŠ ")
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "task_id": task_id,
            "status": "pending",
            "message": "çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨æ‰§è¡Œ...",
            "data": None
        }
        utils.logger.info(f"[CRAWLER_START] å“åº”æ•°æ®: {response_data}")
        utils.logger.info("=" * 100)
        
        return CrawlerResponse(**response_data)
        
    except Exception as e:
        utils.logger.error(f"[CRAWLER_START] å¯åŠ¨çˆ¬è™«ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨çˆ¬è™«ä»»åŠ¡å¤±è´¥: {str(e)}")

@router.get("/crawler/status/{task_id}", response_model=TaskStatusResponse)
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    return TaskStatusResponse(**task_status[task_id])

@router.get("/crawler/tasks")
async def list_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
    return {
        "tasks": list(task_status.values()),
        "total": len(task_status)
    }

@router.delete("/crawler/tasks/{task_id}")
async def delete_task(task_id: str):
    """åˆ é™¤ä»»åŠ¡"""
    if task_id not in task_status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    del task_status[task_id]
    return {"message": "ä»»åŠ¡å·²åˆ é™¤"}

@router.get("/crawler/health")
async def get_crawler_health():
    """è·å–çˆ¬è™«ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        db_status = "unknown"
        try:
            async_db_obj = await get_db_connection()
            if async_db_obj:
                db_status = "healthy"
            else:
                db_status = "unhealthy"
        except Exception as e:
            db_status = f"error: {str(e)}"
        
        # ç»Ÿè®¡ä»»åŠ¡çŠ¶æ€
        total_tasks = len(task_status)
        running_tasks = len([t for t in task_status.values() if t.get('status') == 'running'])
        completed_tasks = len([t for t in task_status.values() if t.get('status') == 'completed'])
        failed_tasks = len([t for t in task_status.values() if t.get('status') == 'failed'])
        
        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "database": db_status,
            "tasks": {
                "total": total_tasks,
                "running": running_tasks,
                "completed": completed_tasks,
                "failed": failed_tasks
            },
            "memory_usage": {
                "task_status_size": len(str(task_status)),
                "estimated_memory_mb": len(str(task_status)) / (1024 * 1024)
            }
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        } 